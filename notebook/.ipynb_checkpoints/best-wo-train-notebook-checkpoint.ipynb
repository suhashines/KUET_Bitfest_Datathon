{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"},{"sourceId":10314104,"sourceType":"datasetVersion","datasetId":6385201}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport re\nfrom ast import literal_eval\nfrom datetime import datetime\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:28:50.153636Z","iopub.execute_input":"2024-12-28T06:28:50.154104Z","iopub.status.idle":"2024-12-28T06:28:55.532360Z","shell.execute_reply.started":"2024-12-28T06:28:50.153878Z","shell.execute_reply":"2024-12-28T06:28:55.531500Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"###############################################################################\n# TextProcessor\n###############################################################################","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self, max_features=300):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.nmf_models = {}\n        self.max_features = max_features\n\n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', 'NUM', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n\n    def process_list(self, text):\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 3),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features // 2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50, random_state=42)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        self.nmf_models[feature_name] = NMF(n_components=30, random_state=42)\n        nmf_matrix = self.nmf_models[feature_name].fit_transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        nmf_matrix = self.nmf_models[feature_name].transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:28:59.958024Z","iopub.execute_input":"2024-12-28T06:28:59.958303Z","iopub.status.idle":"2024-12-28T06:28:59.967980Z","shell.execute_reply.started":"2024-12-28T06:28:59.958283Z","shell.execute_reply":"2024-12-28T06:28:59.966979Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"FEATURE_WEIGHTS = {\n    'skills_required': 0.12,\n    'locations': 0.10,\n    'experience_requirement': 0.09,\n    'job_position_name': 0.08,\n    'educational_requirements': 0.07,\n    'major_field_of_studies': 0.06,\n    'responsibilities.1': 0.05,\n    'passing_years': 0.05,\n    'career_objective': 0.04,\n    'skills': 0.03,\n    'age_requirement': 0.03,\n    'start_dates': 0.02,\n    'responsibilities': 0.02\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:32:38.150667Z","iopub.execute_input":"2024-12-28T06:32:38.151015Z","iopub.status.idle":"2024-12-28T06:32:38.155632Z","shell.execute_reply.started":"2024-12-28T06:32:38.150993Z","shell.execute_reply":"2024-12-28T06:32:38.154697Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.scaler = StandardScaler()\n        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n        self.categorical_encoders = {}\n\n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]\n            experiences = [e - s for s, e in zip(start_years, end_years)]\n            return {\n                'total_experience': sum(experiences),\n                'max_experience': max(experiences) if experiences else 0,\n                'num_positions': len(experiences)\n            }\n        except:\n            return {'total_experience': 0, 'max_experience': 0, 'num_positions': 0}\n\n    def clean_education_result(self, result_str):\n        if pd.isna(result_str):\n            return 0\n        try:\n            if result_str.startswith('['):\n                result_str = literal_eval(result_str)[0]\n            result_str = str(result_str).upper()\n            if result_str in ['N/A', 'NONE', 'NAN', '']:\n                return 0\n            result_str = result_str.replace('%', '')\n            return float(result_str)\n        except:\n            return 0\n\n    def extract_education_features(self, row):\n        try:\n            degree = str(row['degree_names']).lower() if not pd.isna(row['degree_names']) else ''\n            result = self.clean_education_result(row['educational_results'])\n            edu_score = 0\n            if 'phd' in degree or 'doctorate' in degree:\n                edu_score = 4\n            elif 'master' in degree:\n                edu_score = 3\n            elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n                edu_score = 2\n            elif 'diploma' in degree or 'certificate' in degree:\n                edu_score = 1\n            return {\n                'education_score': edu_score,\n                'education_result': result,\n                'education_weight': edu_score * (result / 100 if result > 0 else 1)\n            }\n        except:\n            return {\n                'education_score': 0,\n                'education_result': 0,\n                'education_weight': 0\n            }\n\n    def calculate_weighted_feature(self, df):\n        \"\"\"Calculates a composite feature using FEATURE_WEIGHTS.\"\"\"\n        df['composite_feature'] = 0  # Initialize composite feature\n        for feature, weight in FEATURE_WEIGHTS.items():\n            if feature in df.columns:\n                numeric_feature = pd.to_numeric(df[feature], errors='coerce').fillna(0)\n                df['composite_feature'] += numeric_feature * weight\n        return df\n\n    def encode_categorical_features(self, df, categorical_columns, is_train=True):\n        \"\"\"\n        Encodes categorical columns using One-Hot or Label Encoding.\n        \"\"\"\n        encoded_dfs = []\n        for col in categorical_columns:\n            if is_train:\n                if df[col].nunique() <= 50:  # Threshold for OneHotEncoding\n                    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n                    encoded = encoder.fit_transform(df[[col]].fillna('Unknown'))\n                    self.categorical_encoders[col] = encoder\n                    df_encoded = pd.DataFrame(\n                        encoded,\n                        columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n                        index=df.index\n                    )\n                else:\n                    encoder = LabelEncoder()\n                    encoded = encoder.fit_transform(df[col].fillna('Unknown'))\n                    self.categorical_encoders[col] = encoder\n                    df_encoded = pd.DataFrame(\n                        {f\"{col}_label\": encoded}, index=df.index\n                    )\n            else:\n                encoder = self.categorical_encoders.get(col)\n                if encoder:\n                    if isinstance(encoder, OneHotEncoder):\n                        encoded = encoder.transform(df[[col]].fillna('Unknown'))\n                        df_encoded = pd.DataFrame(\n                            encoded,\n                            columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n                            index=df.index\n                        )\n                    elif isinstance(encoder, LabelEncoder):\n                        encoded = encoder.transform(df[col].fillna('Unknown'))\n                        df_encoded = pd.DataFrame(\n                            {f\"{col}_label\": encoded}, index=df.index\n                        )\n                    else:\n                        raise ValueError(f\"Unsupported encoder type for column '{col}'\")\n                else:\n                    # Handle cases where the encoder is not available\n                    print(f\"Warning: No encoder found for column '{col}'. Using default encoding.\")\n                    df_encoded = pd.DataFrame(\n                        {f\"{col}_label\": [0] * len(df)}, index=df.index\n                    )\n    \n            # Append the encoded DataFrame to the list\n            encoded_dfs.append(df_encoded)\n        \n        # Concatenate all encoded columns\n        if encoded_dfs:\n            return pd.concat(encoded_dfs, axis=1)\n        else:\n            # If no valid encoding was applied, return an empty DataFrame\n            return pd.DataFrame(index=df.index)\n\n\n\n    def sanitize_feature_names(self, df):\n        \"\"\"Clean feature names to remove special characters unsupported by LightGBM.\"\"\"\n        df.columns = [\n            re.sub(r'[^\\w\\.-]', '_', col).replace('__', '_') for col in df.columns\n        ]\n        return df\n    def transform(self, df, is_train=True):\n        # Numeric features from experience and education\n        exp_features = df.apply(self.extract_years_experience, axis=1)\n        edu_features = df.apply(self.extract_education_features, axis=1)\n    \n        feature_dict = {}\n        for feat in ['total_experience', 'max_experience', 'num_positions']:\n            feature_dict[feat] = [x[feat] for x in exp_features]\n        for feat in ['education_score', 'education_result', 'education_weight']:\n            feature_dict[feat] = [x[feat] for x in edu_features]\n    \n        # Basic numeric count features\n        feature_dict['num_skills'] = df['skills'].fillna('').str.count(',') + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        feature_dict['num_languages'] = df['languages'].fillna('').str.count(',') + 1\n    \n        # Additional *interaction* features\n        feature_dict['experience_per_position'] = np.array(feature_dict['total_experience']) / (\n            np.array(feature_dict['num_positions']) + 0.1\n        )\n        feature_dict['result_x_edu_score'] = (\n            np.array(feature_dict['education_result']) * np.array(feature_dict['education_score'])\n        )\n    \n        numeric_df = pd.DataFrame(feature_dict, index=df.index)\n    \n        # Drop constant columns\n        constant_cols = numeric_df.columns[numeric_df.nunique() <= 1]\n        if len(constant_cols) > 0:\n            print(f\"Dropping constant columns: {list(constant_cols)}\")\n            numeric_df = numeric_df.drop(columns=constant_cols)\n    \n        # Power transform or scale numeric features\n        if is_train:\n            try:\n                numeric_arr = self.power_transformer.fit_transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.fit_transform(numeric_df)\n        else:\n            try:\n                numeric_arr = self.power_transformer.transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.transform(numeric_df)\n    \n        numeric_df = pd.DataFrame(numeric_arr, columns=numeric_df.columns, index=numeric_df.index)\n    \n        # Text features\n        text_features = [\n            'skills', 'career_objective', 'responsibilities',\n            'educational_institution_name', 'certification_skills',\n            'major_field_of_studies'\n        ]\n    \n        all_text_features = {}\n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n    \n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n    \n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n    \n        # Handle categorical features\n        categorical_columns = [\n             'locations', 'result_types',\n            'extra_curricular_activity_types', 'role_positions', 'proficiency_levels'\n        ]\n        categorical_df = self.encode_categorical_features(df, categorical_columns, is_train)\n    \n        # Calculate and add weighted composite feature\n        df = self.calculate_weighted_feature(df)\n        numeric_df['composite_feature'] = df['composite_feature']\n    \n        # Combine all features\n        combined_features = pd.concat([numeric_df, text_feature_df, categorical_df], axis=1)\n    \n        # Sanitize feature names\n        combined_features = self.sanitize_feature_names(combined_features)\n    \n        # Return the final feature DataFrame\n        return combined_features\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:29:02.974125Z","iopub.execute_input":"2024-12-28T06:29:02.974408Z","iopub.status.idle":"2024-12-28T06:29:02.995748Z","shell.execute_reply.started":"2024-12-28T06:29:02.974388Z","shell.execute_reply":"2024-12-28T06:29:02.994839Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:30:29.572911Z","iopub.execute_input":"2024-12-28T06:30:29.573197Z","iopub.status.idle":"2024-12-28T06:30:29.745435Z","shell.execute_reply.started":"2024-12-28T06:30:29.573177Z","shell.execute_reply":"2024-12-28T06:30:29.744770Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass OptimizedLightGBM:\n    def __init__(self, n_trials=50):\n        self.n_trials = n_trials\n        self.best_params = None\n        \n    def objective(self, trial, X, y):\n        param = {\n            'objective': 'regression_l2',\n            'metric': 'l2',\n            'boosting_type': 'gbdt',\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'verbose': -1,\n            \n            # GPU-compatible hyperparameters\n            'num_leaves': trial.suggest_int('num_leaves', 31, 128),  # Reduced max leaves\n            'max_depth': trial.suggest_int('max_depth', 5, 12),      # Reduced max depth\n            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n            'min_child_samples': trial.suggest_int('min_child_samples', 20, 80),\n            'max_bin': trial.suggest_int('max_bin', 63, 255),  # GPU-compatible bin size\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n        }\n        \n        # K-fold cross-validation\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        scores = []\n        \n        for train_idx, val_idx in kf.split(X):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            train_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_val, label=y_val)\n            \n            try:\n                model = lgb.train(\n                    param,\n                    train_data,\n                    valid_sets=[val_data],\n                    num_boost_round=1000,\n                    callbacks=[\n                        lgb.early_stopping(stopping_rounds=50),\n                        lgb.log_evaluation(period=0)\n                    ]\n                )\n                \n                preds = model.predict(X_val)\n                fold_score = mean_squared_error(y_val, preds)\n                scores.append(fold_score)\n            except lgb.basic.LightGBMError as e:\n                # Return a large error score if the parameters are invalid\n                return float('inf')\n        \n        return np.mean(scores)\n\n    def find_best_params(self, X, y):\n        study = optuna.create_study(direction='minimize')\n        study.optimize(lambda trial: self.objective(trial, X, y),\n                      n_trials=self.n_trials)\n        \n        self.best_params = study.best_params\n        print(\"\\nOptimization Results:\")\n        print(\"Best MSE:\", study.best_value)\n        print(\"Best parameters:\", self.best_params)\n        return self.best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T20:24:43.644745Z","iopub.execute_input":"2024-12-27T20:24:43.645109Z","iopub.status.idle":"2024-12-27T20:24:43.655312Z","shell.execute_reply.started":"2024-12-27T20:24:43.645081Z","shell.execute_reply":"2024-12-27T20:24:43.654243Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"def main():\n    # Load datasets\n    # train_df = pd.read_csv('/kaggle/input/kuet-dset-2/train.csv')\n    # test_df = pd.read_csv('/kaggle/input/kuet-dset-2/test.csv')\n    \n    # Feature Engineering\n    fe = FeatureEngineer()\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n    \n    y = train_df['matched_score'].values\n    \n    # Find best parameters\n    # print(\"Finding optimal hyperparameters...\")\n    # optimizer = OptimizedLightGBM(n_trials=5)\n    # best_params = optimizer.find_best_params(train_features, y)\n    \n    # # Add fixed parameters to best_params\n    # best_params.update({\n    #     'objective': 'regression_l2',\n    #     'metric': 'l2',\n    #     'boosting_type': 'gbdt',\n    #     'device': 'gpu',\n    #     'gpu_platform_id': 0,\n    #     'gpu_device_id': 0,\n    #     'verbose': -1,\n    #     'max_bin': 255  # Ensure GPU compatibility\n    # })\n\n    params = {\n        'objective': 'regression_l2',\n        'metric': 'l2',\n        'num_leaves': 96,\n        'max_depth': 12,\n        'learning_rate': 0.03199484792860085,\n        'feature_fraction': 0.6706304075294632,\n        'bagging_fraction': 0.958672143301124,\n        'bagging_freq': 3,\n        'min_child_samples': 65,\n        'lambda_l1': 0.014964605006024168,\n        'lambda_l2': 4.20761346366579e-08,\n        'max_bin': 178,\n        'gpu_use_dp': True,  # Enable multi-GPU training\n        'tree_learner': 'data_parallel',  # Use multi-GPU setup\n        'verbose': -1\n        \n    }\n    \n    # Cross-validation and model training\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(len(train_features))\n    test_predictions = np.zeros(len(test_features))\n    feature_importance_df = pd.DataFrame()\n    fold_scores = []\n    \n    print(\"\\nTraining final model with best parameters...\")\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n        print(f\"\\nFold {fold + 1}/5\")\n        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val)\n        \n        model = lgb.train(\n            # best_params,\n            params,\n            train_data,\n            valid_sets=[train_data, val_data],\n            num_boost_round=1000,\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(100)\n            ]\n        )\n        \n        val_preds = model.predict(X_val)\n        oof_predictions[val_idx] = val_preds\n        test_predictions += model.predict(test_features) / kf.n_splits\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n        fold_scores.append(fold_rmse)\n        print(f\"Fold {fold + 1} RMSE: {fold_rmse:.6f}\")\n        \n        fold_importance = pd.DataFrame({\n            \"feature\": train_features.columns,\n            \"importance\": model.feature_importance(),\n            \"fold\": fold + 1\n        })\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)\n    \n    # Calculate final metrics\n    final_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n    # final_r2 = r2_score(y, oof_predictions)\n    \n    print(\"\\nFinal Model Performance:\")\n    print(f\"Average RMSE: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n    print(f\"Overall RMSE: {final_rmse:.6f}\")\n    # print(f\"R2 Score: {final_r2:.6f}\")\n    \n    # Save predictions\n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_predictions\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission saved to submission.csv\")\n    \n    # Save feature importance\n    feature_importance = (feature_importance_df.groupby('feature')['importance']\n                        .mean()\n                        .sort_values(ascending=False))\n    feature_importance.to_csv('feature_importance.csv')\n    \n    print(\"\\nTop 10 Most Important Features:\")\n    print(feature_importance.head(10))\n    \n    return submission, feature_importance_df\n\nfrom sklearn.model_selection import KFold  # Add this import\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# The rest of your code remains the same...\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:38:29.807764Z","iopub.execute_input":"2024-12-28T06:38:29.808080Z","iopub.status.idle":"2024-12-28T06:40:32.613752Z","shell.execute_reply.started":"2024-12-28T06:38:29.808060Z","shell.execute_reply":"2024-12-28T06:40:32.612849Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-b9e451783e1d>:10: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  if pd.isna(text):\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Transforming test data...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-b9e451783e1d>:10: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  if pd.isna(text):\n","output_type":"stream"},{"name":"stdout","text":"\nTraining final model with best parameters...\n\nFold 1/5\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00708438\tvalid_1's l2: 0.0110742\n[200]\ttraining's l2: 0.00482655\tvalid_1's l2: 0.00982697\n[300]\ttraining's l2: 0.00360304\tvalid_1's l2: 0.00935696\n[400]\ttraining's l2: 0.00277475\tvalid_1's l2: 0.00911554\n[500]\ttraining's l2: 0.0021343\tvalid_1's l2: 0.0089902\n[600]\ttraining's l2: 0.00166381\tvalid_1's l2: 0.00891884\n[700]\ttraining's l2: 0.00133246\tvalid_1's l2: 0.00889866\n[800]\ttraining's l2: 0.0010698\tvalid_1's l2: 0.00888588\nEarly stopping, best iteration is:\n[821]\ttraining's l2: 0.00102268\tvalid_1's l2: 0.00887873\nFold 1 RMSE: 0.094227\n\nFold 2/5\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00704047\tvalid_1's l2: 0.0107779\n[200]\ttraining's l2: 0.00472106\tvalid_1's l2: 0.00943009\n[300]\ttraining's l2: 0.00351843\tvalid_1's l2: 0.00903166\n[400]\ttraining's l2: 0.00267171\tvalid_1's l2: 0.00879264\n[500]\ttraining's l2: 0.00208287\tvalid_1's l2: 0.00869955\n[600]\ttraining's l2: 0.00161312\tvalid_1's l2: 0.00864283\n[700]\ttraining's l2: 0.00129596\tvalid_1's l2: 0.00862165\n[800]\ttraining's l2: 0.00103621\tvalid_1's l2: 0.00860957\nEarly stopping, best iteration is:\n[817]\ttraining's l2: 0.00100131\tvalid_1's l2: 0.0086022\nFold 2 RMSE: 0.092748\n\nFold 3/5\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00717518\tvalid_1's l2: 0.0103136\n[200]\ttraining's l2: 0.00483366\tvalid_1's l2: 0.00896153\n[300]\ttraining's l2: 0.00357804\tvalid_1's l2: 0.00852741\n[400]\ttraining's l2: 0.00271716\tvalid_1's l2: 0.00832978\n[500]\ttraining's l2: 0.00211912\tvalid_1's l2: 0.00827903\n[600]\ttraining's l2: 0.00163083\tvalid_1's l2: 0.00823653\n[700]\ttraining's l2: 0.00130612\tvalid_1's l2: 0.00824029\nEarly stopping, best iteration is:\n[658]\ttraining's l2: 0.00143014\tvalid_1's l2: 0.00822624\nFold 3 RMSE: 0.090699\n\nFold 4/5\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0069958\tvalid_1's l2: 0.010686\n[200]\ttraining's l2: 0.00477067\tvalid_1's l2: 0.00941979\n[300]\ttraining's l2: 0.00353839\tvalid_1's l2: 0.00896595\n[400]\ttraining's l2: 0.00264792\tvalid_1's l2: 0.00876801\n[500]\ttraining's l2: 0.00203308\tvalid_1's l2: 0.00864849\n[600]\ttraining's l2: 0.00160026\tvalid_1's l2: 0.00856151\n[700]\ttraining's l2: 0.00125019\tvalid_1's l2: 0.00852595\n[800]\ttraining's l2: 0.00100114\tvalid_1's l2: 0.0084962\n[900]\ttraining's l2: 0.000806918\tvalid_1's l2: 0.00847418\n[1000]\ttraining's l2: 0.000655554\tvalid_1's l2: 0.00846682\nDid not meet early stopping. Best iteration is:\n[987]\ttraining's l2: 0.000672414\tvalid_1's l2: 0.00846405\nFold 4 RMSE: 0.092000\n\nFold 5/5\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00729414\tvalid_1's l2: 0.0101091\n[200]\ttraining's l2: 0.00485358\tvalid_1's l2: 0.00881102\n[300]\ttraining's l2: 0.00361318\tvalid_1's l2: 0.00840562\n[400]\ttraining's l2: 0.00270561\tvalid_1's l2: 0.00826501\n[500]\ttraining's l2: 0.00209493\tvalid_1's l2: 0.00817604\n[600]\ttraining's l2: 0.00164492\tvalid_1's l2: 0.00812977\n[700]\ttraining's l2: 0.00131778\tvalid_1's l2: 0.00808016\nEarly stopping, best iteration is:\n[702]\ttraining's l2: 0.00131074\tvalid_1's l2: 0.0080753\nFold 5 RMSE: 0.089863\n\nFinal Model Performance:\nAverage RMSE: 0.091907 ± 0.001532\nOverall RMSE: 0.091920\n\nSubmission saved to submission.csv\n\nTop 10 Most Important Features:\nfeature\nskills_text_452              286.0\nresponsibilities_text_476    269.2\nresponsibilities_text_453    257.0\nskills_text_450              241.6\nskills_text_451              232.6\nresponsibilities_text_455    231.4\nresponsibilities_text_477    231.2\nresponsibilities_text_468    229.2\nresponsibilities_text_463    224.2\nresponsibilities_text_470    218.2\nName: importance, dtype: float64\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(train_df.columns)\nprint(test_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T19:37:43.675545Z","iopub.execute_input":"2024-12-27T19:37:43.675868Z","iopub.status.idle":"2024-12-27T19:37:43.681149Z","shell.execute_reply.started":"2024-12-27T19:37:43.675835Z","shell.execute_reply":"2024-12-27T19:37:43.680258Z"}},"outputs":[{"name":"stdout","text":"Index(['address', 'career_objective', 'skills', 'educational_institution_name',\n       'degree_names', 'passing_years', 'educational_results', 'result_types',\n       'major_field_of_studies', 'professional_company_names', 'company_urls',\n       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n       'locations', 'responsibilities', 'extra_curricular_activity_types',\n       'extra_curricular_organization_names',\n       'extra_curricular_organization_links', 'role_positions', 'languages',\n       'proficiency_levels', 'certification_providers', 'certification_skills',\n       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n       'educationaL_requirements', 'experiencere_requirement',\n       'age_requirement', 'responsibilities.1', 'skills_required',\n       'matched_score'],\n      dtype='object')\nIndex(['ID', 'address', 'career_objective', 'skills',\n       'educational_institution_name', 'degree_names', 'passing_years',\n       'educational_results', 'result_types', 'major_field_of_studies',\n       'professional_company_names', 'company_urls', 'start_dates',\n       'end_dates', 'related_skils_in_job', 'positions', 'locations',\n       'responsibilities', 'extra_curricular_activity_types',\n       'extra_curricular_organization_names',\n       'extra_curricular_organization_links', 'role_positions', 'languages',\n       'proficiency_levels', 'certification_providers', 'certification_skills',\n       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n       'educationaL_requirements', 'experiencere_requirement',\n       'age_requirement', 'responsibilities.1', 'skills_required'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T19:59:50.062544Z","iopub.execute_input":"2024-12-27T19:59:50.062858Z","iopub.status.idle":"2024-12-27T20:03:28.639888Z","shell.execute_reply.started":"2024-12-27T19:59:50.062835Z","shell.execute_reply":"2024-12-27T20:03:28.638920Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\nTransforming test data...\nTraining fold 1...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0122969\tvalid_1's l2: 0.0145609\n[200]\ttraining's l2: 0.00777817\tvalid_1's l2: 0.0117397\n[300]\ttraining's l2: 0.00567534\tvalid_1's l2: 0.0105121\n[400]\ttraining's l2: 0.00451381\tvalid_1's l2: 0.00991951\n[500]\ttraining's l2: 0.00374165\tvalid_1's l2: 0.00954762\n[600]\ttraining's l2: 0.00320871\tvalid_1's l2: 0.00935105\n[700]\ttraining's l2: 0.00280399\tvalid_1's l2: 0.00923491\n[800]\ttraining's l2: 0.0024527\tvalid_1's l2: 0.00912971\n[900]\ttraining's l2: 0.0021446\tvalid_1's l2: 0.00905242\n[1000]\ttraining's l2: 0.00188949\tvalid_1's l2: 0.00899401\nDid not meet early stopping. Best iteration is:\n[999]\ttraining's l2: 0.00189165\tvalid_1's l2: 0.00899389\nTraining fold 2...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0124233\tvalid_1's l2: 0.0150714\n[200]\ttraining's l2: 0.00774646\tvalid_1's l2: 0.0116513\n[300]\ttraining's l2: 0.00560613\tvalid_1's l2: 0.0102117\n[400]\ttraining's l2: 0.00444131\tvalid_1's l2: 0.00955866\n[500]\ttraining's l2: 0.00366715\tvalid_1's l2: 0.0092116\n[600]\ttraining's l2: 0.0031402\tvalid_1's l2: 0.00901317\n[700]\ttraining's l2: 0.002728\tvalid_1's l2: 0.0088893\n[800]\ttraining's l2: 0.00237846\tvalid_1's l2: 0.00881281\n[900]\ttraining's l2: 0.00208079\tvalid_1's l2: 0.0087236\n[1000]\ttraining's l2: 0.00182558\tvalid_1's l2: 0.00866616\nDid not meet early stopping. Best iteration is:\n[998]\ttraining's l2: 0.00182968\tvalid_1's l2: 0.00866548\nTraining fold 3...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0125104\tvalid_1's l2: 0.0144617\n[200]\ttraining's l2: 0.00789306\tvalid_1's l2: 0.0111564\n[300]\ttraining's l2: 0.00569715\tvalid_1's l2: 0.00976458\n[400]\ttraining's l2: 0.00453074\tvalid_1's l2: 0.0091237\n[500]\ttraining's l2: 0.0038014\tvalid_1's l2: 0.00883369\n[600]\ttraining's l2: 0.00326508\tvalid_1's l2: 0.00863366\n[700]\ttraining's l2: 0.00282438\tvalid_1's l2: 0.00847576\n[800]\ttraining's l2: 0.00247329\tvalid_1's l2: 0.00838698\n[900]\ttraining's l2: 0.00216802\tvalid_1's l2: 0.00829745\n[1000]\ttraining's l2: 0.00190781\tvalid_1's l2: 0.00821903\nDid not meet early stopping. Best iteration is:\n[998]\ttraining's l2: 0.00191212\tvalid_1's l2: 0.00821844\nTraining fold 4...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0122159\tvalid_1's l2: 0.0151241\n[200]\ttraining's l2: 0.0077162\tvalid_1's l2: 0.0117726\n[300]\ttraining's l2: 0.00566146\tvalid_1's l2: 0.0104126\n[400]\ttraining's l2: 0.00449623\tvalid_1's l2: 0.00972557\n[500]\ttraining's l2: 0.00374449\tvalid_1's l2: 0.00936278\n[600]\ttraining's l2: 0.00320946\tvalid_1's l2: 0.00917391\n[700]\ttraining's l2: 0.00279643\tvalid_1's l2: 0.00903002\n[800]\ttraining's l2: 0.00243451\tvalid_1's l2: 0.00890194\n[900]\ttraining's l2: 0.00213815\tvalid_1's l2: 0.00881049\n[1000]\ttraining's l2: 0.00188494\tvalid_1's l2: 0.00876582\nDid not meet early stopping. Best iteration is:\n[990]\ttraining's l2: 0.00190901\tvalid_1's l2: 0.0087642\nTraining fold 5...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.012414\tvalid_1's l2: 0.0143693\n[200]\ttraining's l2: 0.00782183\tvalid_1's l2: 0.0109475\n[300]\ttraining's l2: 0.00570195\tvalid_1's l2: 0.0096242\n[400]\ttraining's l2: 0.00454439\tvalid_1's l2: 0.00899409\n[500]\ttraining's l2: 0.00377918\tvalid_1's l2: 0.00864412\n[600]\ttraining's l2: 0.00323269\tvalid_1's l2: 0.00848758\n[700]\ttraining's l2: 0.00279921\tvalid_1's l2: 0.00834554\n[800]\ttraining's l2: 0.00244174\tvalid_1's l2: 0.00826099\n[900]\ttraining's l2: 0.00212978\tvalid_1's l2: 0.00819624\n[1000]\ttraining's l2: 0.00187125\tvalid_1's l2: 0.00814916\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's l2: 0.00187125\tvalid_1's l2: 0.00814916\nOOF MSE: 0.008558\nSubmission saved to submission.csv\n","output_type":"stream"}],"execution_count":65}]}