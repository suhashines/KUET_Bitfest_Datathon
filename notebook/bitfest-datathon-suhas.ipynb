{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import re\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TextProcessor\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, max_features=300):\n",
    "        self.tfidf_models = {}\n",
    "        self.count_models = {}\n",
    "        self.svd_models = {}\n",
    "        self.nmf_models = {}\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return ''\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', 'NUM', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def process_list(self, text):\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        try:\n",
    "            items = literal_eval(text)\n",
    "            return [self.clean_text(item) for item in items]\n",
    "        except:\n",
    "            return [self.clean_text(item) for item in str(text).split(',')]\n",
    "\n",
    "    def fit_transform_text(self, texts, feature_name):\n",
    "        processed_texts = [\n",
    "            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n",
    "            for text in texts\n",
    "        ]\n",
    "        self.tfidf_models[feature_name] = TfidfVectorizer(\n",
    "            max_features=self.max_features,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english'\n",
    "        )\n",
    "        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n",
    "        self.count_models[feature_name] = CountVectorizer(\n",
    "            max_features=self.max_features // 2,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english'\n",
    "        )\n",
    "        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n",
    "        self.svd_models[feature_name] = TruncatedSVD(n_components=50, random_state=42)\n",
    "        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n",
    "        self.nmf_models[feature_name] = NMF(n_components=30, random_state=42)\n",
    "        nmf_matrix = self.nmf_models[feature_name].fit_transform(tfidf_matrix)\n",
    "        return np.hstack([\n",
    "            tfidf_matrix.toarray(),\n",
    "            count_matrix.toarray(),\n",
    "            svd_matrix,\n",
    "            nmf_matrix\n",
    "        ])\n",
    "\n",
    "    def transform_text(self, texts, feature_name):\n",
    "        processed_texts = [\n",
    "            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n",
    "            for text in texts\n",
    "        ]\n",
    "        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n",
    "        count_matrix = self.count_models[feature_name].transform(processed_texts)\n",
    "        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n",
    "        nmf_matrix = self.nmf_models[feature_name].transform(tfidf_matrix)\n",
    "        return np.hstack([\n",
    "            tfidf_matrix.toarray(),\n",
    "            count_matrix.toarray(),\n",
    "            svd_matrix,\n",
    "            nmf_matrix\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_WEIGHTS = {\n",
    "    'skills_required': 0.12,\n",
    "    'locations': 0.10,\n",
    "    'experience_requirement': 0.09,\n",
    "    'job_position_name': 0.08,\n",
    "    'educational_requirements': 0.07,\n",
    "    'major_field_of_studies': 0.06,\n",
    "    'responsibilities.1': 0.05,\n",
    "    'passing_years': 0.05,\n",
    "    'career_objective': 0.04,\n",
    "    'skills': 0.03,\n",
    "    'age_requirement': 0.03,\n",
    "    'start_dates': 0.02,\n",
    "    'responsibilities': 0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.text_processor = TextProcessor()\n",
    "        self.scaler = StandardScaler()\n",
    "        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "        self.categorical_encoders = {}\n",
    "\n",
    "    def extract_years_experience(self, row):\n",
    "        try:\n",
    "            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n",
    "            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n",
    "            if not end_years:\n",
    "                end_years = [2024]\n",
    "            experiences = [e - s for s, e in zip(start_years, end_years)]\n",
    "            return {\n",
    "                'total_experience': sum(experiences),\n",
    "                'max_experience': max(experiences) if experiences else 0,\n",
    "                'num_positions': len(experiences)\n",
    "            }\n",
    "        except:\n",
    "            return {'total_experience': 0, 'max_experience': 0, 'num_positions': 0}\n",
    "\n",
    "    def clean_education_result(self, result_str):\n",
    "        if pd.isna(result_str):\n",
    "            return 0\n",
    "        try:\n",
    "            if result_str.startswith('['):\n",
    "                result_str = literal_eval(result_str)[0]\n",
    "            result_str = str(result_str).upper()\n",
    "            if result_str in ['N/A', 'NONE', 'NAN', '']:\n",
    "                return 0\n",
    "            result_str = result_str.replace('%', '')\n",
    "            return float(result_str)\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def extract_education_features(self, row):\n",
    "        try:\n",
    "            degree = str(row['degree_names']).lower() if not pd.isna(row['degree_names']) else ''\n",
    "            result = self.clean_education_result(row['educational_results'])\n",
    "            edu_score = 0\n",
    "            if 'phd' in degree or 'doctorate' in degree:\n",
    "                edu_score = 4\n",
    "            elif 'master' in degree:\n",
    "                edu_score = 3\n",
    "            elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n",
    "                edu_score = 2\n",
    "            elif 'diploma' in degree or 'certificate' in degree:\n",
    "                edu_score = 1\n",
    "            return {\n",
    "                'education_score': edu_score,\n",
    "                'education_result': result,\n",
    "                'education_weight': edu_score * (result / 100 if result > 0 else 1)\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'education_score': 0,\n",
    "                'education_result': 0,\n",
    "                'education_weight': 0\n",
    "            }\n",
    "\n",
    "    def calculate_weighted_feature(self, df):\n",
    "        \"\"\"Calculates a composite feature using FEATURE_WEIGHTS.\"\"\"\n",
    "        df['composite_feature'] = 0  # Initialize composite feature\n",
    "        for feature, weight in FEATURE_WEIGHTS.items():\n",
    "            if feature in df.columns:\n",
    "                numeric_feature = pd.to_numeric(df[feature], errors='coerce').fillna(0)\n",
    "                df['composite_feature'] += numeric_feature * weight\n",
    "        return df\n",
    "\n",
    "    def encode_categorical_features(self, df, categorical_columns, is_train=True):\n",
    "        \"\"\"\n",
    "        Encodes categorical columns using One-Hot or Label Encoding.\n",
    "        \"\"\"\n",
    "        encoded_dfs = []\n",
    "        for col in categorical_columns:\n",
    "            if is_train:\n",
    "                if df[col].nunique() <= 50:  # Threshold for OneHotEncoding\n",
    "                    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "                    encoded = encoder.fit_transform(df[[col]].fillna('Unknown'))\n",
    "                    self.categorical_encoders[col] = encoder\n",
    "                    df_encoded = pd.DataFrame(\n",
    "                        encoded,\n",
    "                        columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n",
    "                        index=df.index\n",
    "                    )\n",
    "                else:\n",
    "                    encoder = LabelEncoder()\n",
    "                    encoded = encoder.fit_transform(df[col].fillna('Unknown'))\n",
    "                    self.categorical_encoders[col] = encoder\n",
    "                    df_encoded = pd.DataFrame(\n",
    "                        {f\"{col}_label\": encoded}, index=df.index\n",
    "                    )\n",
    "            else:\n",
    "                encoder = self.categorical_encoders.get(col)\n",
    "                if encoder:\n",
    "                    if isinstance(encoder, OneHotEncoder):\n",
    "                        encoded = encoder.transform(df[[col]].fillna('Unknown'))\n",
    "                        df_encoded = pd.DataFrame(\n",
    "                            encoded,\n",
    "                            columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n",
    "                            index=df.index\n",
    "                        )\n",
    "                    elif isinstance(encoder, LabelEncoder):\n",
    "                        encoded = encoder.transform(df[col].fillna('Unknown'))\n",
    "                        df_encoded = pd.DataFrame(\n",
    "                            {f\"{col}_label\": encoded}, index=df.index\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported encoder type for column '{col}'\")\n",
    "                else:\n",
    "                    # Handle cases where the encoder is not available\n",
    "                    print(f\"Warning: No encoder found for column '{col}'. Using default encoding.\")\n",
    "                    df_encoded = pd.DataFrame(\n",
    "                        {f\"{col}_label\": [0] * len(df)}, index=df.index\n",
    "                    )\n",
    "    \n",
    "            # Append the encoded DataFrame to the list\n",
    "            encoded_dfs.append(df_encoded)\n",
    "        \n",
    "        # Concatenate all encoded columns\n",
    "        if encoded_dfs:\n",
    "            return pd.concat(encoded_dfs, axis=1)\n",
    "        else:\n",
    "            # If no valid encoding was applied, return an empty DataFrame\n",
    "            return pd.DataFrame(index=df.index)\n",
    "\n",
    "\n",
    "\n",
    "    def sanitize_feature_names(self, df):\n",
    "        \"\"\"Clean feature names to remove special characters unsupported by LightGBM.\"\"\"\n",
    "        df.columns = [\n",
    "            re.sub(r'[^\\w\\.-]', '_', col).replace('__', '_') for col in df.columns\n",
    "        ]\n",
    "        return df\n",
    "    def transform(self, df, is_train=True):\n",
    "        # Numeric features from experience and education\n",
    "        exp_features = df.apply(self.extract_years_experience, axis=1)\n",
    "        edu_features = df.apply(self.extract_education_features, axis=1)\n",
    "    \n",
    "        feature_dict = {}\n",
    "        for feat in ['total_experience', 'max_experience', 'num_positions']:\n",
    "            feature_dict[feat] = [x[feat] for x in exp_features]\n",
    "        for feat in ['education_score', 'education_result', 'education_weight']:\n",
    "            feature_dict[feat] = [x[feat] for x in edu_features]\n",
    "    \n",
    "        # Basic numeric count features\n",
    "        feature_dict['num_skills'] = df['skills'].fillna('').str.count(',') + 1\n",
    "        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n",
    "        feature_dict['num_languages'] = df['languages'].fillna('').str.count(',') + 1\n",
    "    \n",
    "        # Additional *interaction* features\n",
    "        feature_dict['experience_per_position'] = np.array(feature_dict['total_experience']) / (\n",
    "            np.array(feature_dict['num_positions']) + 0.1\n",
    "        )\n",
    "        feature_dict['result_x_edu_score'] = (\n",
    "            np.array(feature_dict['education_result']) * np.array(feature_dict['education_score'])\n",
    "        )\n",
    "    \n",
    "        numeric_df = pd.DataFrame(feature_dict, index=df.index)\n",
    "    \n",
    "        # Drop constant columns\n",
    "        constant_cols = numeric_df.columns[numeric_df.nunique() <= 1]\n",
    "        if len(constant_cols) > 0:\n",
    "            print(f\"Dropping constant columns: {list(constant_cols)}\")\n",
    "            numeric_df = numeric_df.drop(columns=constant_cols)\n",
    "    \n",
    "        # Power transform or scale numeric features\n",
    "        if is_train:\n",
    "            try:\n",
    "                numeric_arr = self.power_transformer.fit_transform(numeric_df)\n",
    "            except Exception as e:\n",
    "                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n",
    "                numeric_arr = self.scaler.fit_transform(numeric_df)\n",
    "        else:\n",
    "            try:\n",
    "                numeric_arr = self.power_transformer.transform(numeric_df)\n",
    "            except Exception as e:\n",
    "                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n",
    "                numeric_arr = self.scaler.transform(numeric_df)\n",
    "    \n",
    "        numeric_df = pd.DataFrame(numeric_arr, columns=numeric_df.columns, index=numeric_df.index)\n",
    "    \n",
    "        # Text features\n",
    "        text_features = [\n",
    "            'skills', 'career_objective', 'responsibilities',\n",
    "            'educational_institution_name', 'certification_skills',\n",
    "            'major_field_of_studies'\n",
    "        ]\n",
    "    \n",
    "        all_text_features = {}\n",
    "        for feature in text_features:\n",
    "            if is_train:\n",
    "                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n",
    "            else:\n",
    "                text_matrix = self.text_processor.transform_text(df[feature], feature)\n",
    "    \n",
    "            for i in range(text_matrix.shape[1]):\n",
    "                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n",
    "    \n",
    "        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n",
    "    \n",
    "        # Handle categorical features\n",
    "        categorical_columns = [\n",
    "             'locations', 'result_types',\n",
    "            'extra_curricular_activity_types', 'role_positions', 'proficiency_levels'\n",
    "        ]\n",
    "        categorical_df = self.encode_categorical_features(df, categorical_columns, is_train)\n",
    "    \n",
    "        # Calculate and add weighted composite feature\n",
    "        df = self.calculate_weighted_feature(df)\n",
    "        numeric_df['composite_feature'] = df['composite_feature']\n",
    "    \n",
    "        # Combine all features\n",
    "        combined_features = pd.concat([numeric_df, text_feature_df, categorical_df], axis=1)\n",
    "    \n",
    "        # Sanitize feature names\n",
    "        combined_features = self.sanitize_feature_names(combined_features)\n",
    "    \n",
    "        # Return the final feature DataFrame\n",
    "        return combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class OptimizedLightGBM:\n",
    "    def __init__(self, n_trials=50):\n",
    "        self.n_trials = n_trials\n",
    "        self.best_params = None\n",
    "        \n",
    "    def objective(self, trial, X, y):\n",
    "        param = {\n",
    "            'objective': 'regression_l2',\n",
    "            'metric': 'l2',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'device': 'gpu',\n",
    "            'gpu_platform_id': 0,\n",
    "            'gpu_device_id': 0,\n",
    "            'verbose': -1,\n",
    "            \n",
    "            # GPU-compatible hyperparameters\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 31, 128),  # Reduced max leaves\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 12),      # Reduced max depth\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n",
    "            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 20, 80),\n",
    "            'max_bin': trial.suggest_int('max_bin', 63, 255),  # GPU-compatible bin size\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n",
    "        }\n",
    "        \n",
    "        # K-fold cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        \n",
    "        for train_idx, val_idx in kf.split(X):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val)\n",
    "            \n",
    "            try:\n",
    "                model = lgb.train(\n",
    "                    param,\n",
    "                    train_data,\n",
    "                    valid_sets=[val_data],\n",
    "                    num_boost_round=1000,\n",
    "                    callbacks=[\n",
    "                        lgb.early_stopping(stopping_rounds=50),\n",
    "                        lgb.log_evaluation(period=0)\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                preds = model.predict(X_val)\n",
    "                fold_score = mean_squared_error(y_val, preds)\n",
    "                scores.append(fold_score)\n",
    "            except lgb.basic.LightGBMError as e:\n",
    "                # Return a large error score if the parameters are invalid\n",
    "                return float('inf')\n",
    "        \n",
    "        return np.mean(scores)\n",
    "\n",
    "    def find_best_params(self, X, y):\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: self.objective(trial, X, y),\n",
    "                      n_trials=self.n_trials)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        print(\"\\nOptimization Results:\")\n",
    "        print(\"Best MSE:\", study.best_value)\n",
    "        print(\"Best parameters:\", self.best_params)\n",
    "        return self.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load datasets\n",
    "    # train_df = pd.read_csv('/kaggle/input/kuet-dset-2/train.csv')\n",
    "    # test_df = pd.read_csv('/kaggle/input/kuet-dset-2/test.csv')\n",
    "    \n",
    "    # Feature Engineering\n",
    "    fe = FeatureEngineer()\n",
    "    print(\"Transforming train data...\")\n",
    "    train_features = fe.transform(train_df, is_train=True)\n",
    "    print(\"Transforming test data...\")\n",
    "    test_features = fe.transform(test_df, is_train=False)\n",
    "    \n",
    "    y = train_df['matched_score'].values\n",
    "    \n",
    "    # Find best parameters\n",
    "    # print(\"Finding optimal hyperparameters...\")\n",
    "    # optimizer = OptimizedLightGBM(n_trials=5)\n",
    "    # best_params = optimizer.find_best_params(train_features, y)\n",
    "    \n",
    "    # # Add fixed parameters to best_params\n",
    "    # best_params.update({\n",
    "    #     'objective': 'regression_l2',\n",
    "    #     'metric': 'l2',\n",
    "    #     'boosting_type': 'gbdt',\n",
    "    #     'device': 'gpu',\n",
    "    #     'gpu_platform_id': 0,\n",
    "    #     'gpu_device_id': 0,\n",
    "    #     'verbose': -1,\n",
    "    #     'max_bin': 255  # Ensure GPU compatibility\n",
    "    # })\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression_l2',\n",
    "        'metric': 'l2',\n",
    "        'num_leaves': 96,\n",
    "        'max_depth': 12,\n",
    "        'learning_rate': 0.03199484792860085,\n",
    "        'feature_fraction': 0.6706304075294632,\n",
    "        'bagging_fraction': 0.958672143301124,\n",
    "        'bagging_freq': 3,\n",
    "        'min_child_samples': 65,\n",
    "        'lambda_l1': 0.014964605006024168,\n",
    "        'lambda_l2': 4.20761346366579e-08,\n",
    "        'max_bin': 178,\n",
    "        'gpu_use_dp': True,  # Enable multi-GPU training\n",
    "        'tree_learner': 'data_parallel',  # Use multi-GPU setup\n",
    "        'verbose': -1\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Cross-validation and model training\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_predictions = np.zeros(len(train_features))\n",
    "    test_predictions = np.zeros(len(test_features))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    fold_scores = []\n",
    "    \n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n",
    "        print(f\"\\nFold {fold + 1}/5\")\n",
    "        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            # best_params,\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[train_data, val_data],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=50),\n",
    "                lgb.log_evaluation(100)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        val_preds = model.predict(X_val)\n",
    "        oof_predictions[val_idx] = val_preds\n",
    "        test_predictions += model.predict(test_features) / kf.n_splits\n",
    "        \n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "        fold_scores.append(fold_rmse)\n",
    "        print(f\"Fold {fold + 1} RMSE: {fold_rmse:.6f}\")\n",
    "        \n",
    "        fold_importance = pd.DataFrame({\n",
    "            \"feature\": train_features.columns,\n",
    "            \"importance\": model.feature_importance(),\n",
    "            \"fold\": fold + 1\n",
    "        })\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "    # final_r2 = r2_score(y, oof_predictions)\n",
    "    \n",
    "    print(\"\\nFinal Model Performance:\")\n",
    "    print(f\"Average RMSE: {np.mean(fold_scores):.6f} Â± {np.std(fold_scores):.6f}\")\n",
    "    print(f\"Overall RMSE: {final_rmse:.6f}\")\n",
    "    # print(f\"R2 Score: {final_r2:.6f}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    submission = pd.DataFrame({\n",
    "        'ID': test_df['ID'],\n",
    "        'matched_score': test_predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nSubmission saved to submission.csv\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance = (feature_importance_df.groupby('feature')['importance']\n",
    "                        .mean()\n",
    "                        .sort_values(ascending=False))\n",
    "    feature_importance.to_csv('feature_importance.csv')\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    return submission, feature_importance_df\n",
    "\n",
    "from sklearn.model_selection import KFold  # Add this import\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# The rest of your code remains the same...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns)\n",
    "print(test_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10606811,
     "sourceId": 90798,
     "sourceType": "competition"
    },
    {
     "datasetId": 6385201,
     "sourceId": 10314104,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
