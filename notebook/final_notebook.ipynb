{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset Analysis","metadata":{}},{"cell_type":"markdown","source":"## Install dependencies","metadata":{}},{"cell_type":"code","source":"%pip install pandas seaborn matplotlib scikit-learn numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:19.917283Z","iopub.execute_input":"2024-12-28T19:43:19.917594Z","iopub.status.idle":"2024-12-28T19:43:23.092962Z","shell.execute_reply.started":"2024-12-28T19:43:19.917572Z","shell.execute_reply":"2024-12-28T19:43:23.091743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:23.094523Z","iopub.execute_input":"2024-12-28T19:43:23.094855Z","iopub.status.idle":"2024-12-28T19:43:24.573217Z","shell.execute_reply.started":"2024-12-28T19:43:23.094825Z","shell.execute_reply":"2024-12-28T19:43:24.572319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the train and test datasets","metadata":{}},{"cell_type":"code","source":"new_train_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/train.csv\")\nnew_test_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/test.csv\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:28.078318Z","iopub.execute_input":"2024-12-28T19:43:28.078956Z","iopub.status.idle":"2024-12-28T19:43:28.457576Z","shell.execute_reply.started":"2024-12-28T19:43:28.078913Z","shell.execute_reply":"2024-12-28T19:43:28.456571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_train_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/train.csv\", dtype={\"skills_required\": str})\nprint(new_train_data['skills_required'][3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:31.123890Z","iopub.execute_input":"2024-12-28T19:43:31.124224Z","iopub.status.idle":"2024-12-28T19:43:31.261940Z","shell.execute_reply.started":"2024-12-28T19:43:31.124196Z","shell.execute_reply":"2024-12-28T19:43:31.261070Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check basic structure of the datasets","metadata":{}},{"cell_type":"code","source":"print(\"Train Dataset Overview:\\n\")\nprint(new_train_data.info())\nprint(\"\\nTest Dataset Overview:\\n\")\nprint(new_test_data.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:34.318398Z","iopub.execute_input":"2024-12-28T19:43:34.318698Z","iopub.status.idle":"2024-12-28T19:43:34.361564Z","shell.execute_reply.started":"2024-12-28T19:43:34.318673Z","shell.execute_reply":"2024-12-28T19:43:34.360862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display summary statistics for numerical columns","metadata":{}},{"cell_type":"code","source":"print(\"\\nTrain Dataset Numerical Summary:\\n\", new_train_data.describe())\nprint(\"\\nTest Dataset Numerical Summary:\\n\", new_test_data.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:39.011792Z","iopub.execute_input":"2024-12-28T19:43:39.012140Z","iopub.status.idle":"2024-12-28T19:43:39.029745Z","shell.execute_reply.started":"2024-12-28T19:43:39.012109Z","shell.execute_reply":"2024-12-28T19:43:39.029075Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Display first few rows of the datasets","metadata":{}},{"cell_type":"code","source":"print(\"\\nTrain Dataset Sample Rows:\\n\", new_train_data.head())\nprint(\"\\nTest Dataset Sample Rows:\\n\", new_test_data.head()) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:41.745911Z","iopub.execute_input":"2024-12-28T19:43:41.746284Z","iopub.status.idle":"2024-12-28T19:43:41.767255Z","shell.execute_reply.started":"2024-12-28T19:43:41.746257Z","shell.execute_reply":"2024-12-28T19:43:41.766359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Missing Values Analysis","metadata":{}},{"cell_type":"markdown","source":"### Calculate missing value percentages","metadata":{}},{"cell_type":"code","source":"train_missing = new_train_data.isnull().mean() * 100\ntest_missing = new_test_data.isnull().mean() * 100\n\nprint(\"Train Dataset Missing Values (%):\\n\", train_missing[train_missing > 0].sort_values(ascending=False))\nprint(\"\\nTest Dataset Missing Values (%):\\n\", test_missing[test_missing > 0].sort_values(ascending=False))","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:47.025888Z","iopub.execute_input":"2024-12-28T19:43:47.026261Z","iopub.status.idle":"2024-12-28T19:43:47.047174Z","shell.execute_reply.started":"2024-12-28T19:43:47.026225Z","shell.execute_reply":"2024-12-28T19:43:47.046335Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Unique Value Analysis","metadata":{}},{"cell_type":"code","source":"# Display unique values in each column\nfor col in new_train_data.columns:\n    print(f\"Column: {col}\")\n    print(f\"Unique Values: {new_train_data[col].nunique()}\")\n    print(\"-\" * 50)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:50.723742Z","iopub.execute_input":"2024-12-28T19:43:50.724047Z","iopub.status.idle":"2024-12-28T19:43:50.758743Z","shell.execute_reply.started":"2024-12-28T19:43:50.724021Z","shell.execute_reply":"2024-12-28T19:43:50.757795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"markdown","source":"## Visualizing the target variable distribution","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.histplot(new_train_data['matched_score'], kde=True, color='blue')\nplt.title('Distribution of Matched Score')\nplt.xlabel('Matched Score')\nplt.ylabel('Frequency')\nplt.xticks(np.arange(0, 1.05, 0.05))\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:53.031212Z","iopub.execute_input":"2024-12-28T19:43:53.031524Z","iopub.status.idle":"2024-12-28T19:43:53.495865Z","shell.execute_reply.started":"2024-12-28T19:43:53.031496Z","shell.execute_reply":"2024-12-28T19:43:53.494997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analyzing the impact of skills_required on matched_score","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nsns.boxplot(x='skills_required', y='matched_score', data=new_train_data)\nplt.title('Skills Required vs Matched Score')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:43:57.703963Z","iopub.execute_input":"2024-12-28T19:43:57.704252Z","iopub.status.idle":"2024-12-28T19:43:58.357027Z","shell.execute_reply.started":"2024-12-28T19:43:57.704232Z","shell.execute_reply":"2024-12-28T19:43:58.356110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_train_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/train.csv\", dtype={\"skills_required\": str})\n\n# Fill missing values in skills_required\nnew_train_data['skills_required'] = new_train_data['skills_required'].fillna(\"Unknown\")\n\n# Group rare categories into \"Other\" (optional)\nthreshold = 10  # Adjust based on your dataset\ncategory_counts = new_train_data['skills_required'].value_counts()\nnew_train_data['skills_required'] = new_train_data['skills_required'].apply(\n    lambda x: x if category_counts[x] > threshold else 'Other'\n)\n\n# Sort skills_required categories by median matched_score\norder = new_train_data.groupby('skills_required')['matched_score'].median().sort_values().index\n\nplt.figure(figsize=(25, 15))\nsns.boxplot(x='skills_required', y='matched_score', data=new_train_data, order=order)\nplt.title('Impact of Skills Required on Matched Score', fontsize=16)\nplt.xlabel('Skills Required', fontsize=12)\nplt.ylabel('Matched Score', fontsize=12)\nplt.xticks(rotation=90, fontsize=10) \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:01.772192Z","iopub.execute_input":"2024-12-28T19:44:01.772630Z","iopub.status.idle":"2024-12-28T19:44:02.987682Z","shell.execute_reply.started":"2024-12-28T19:44:01.772588Z","shell.execute_reply":"2024-12-28T19:44:02.986633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation matrix for numerical columns","metadata":{}},{"cell_type":"code","source":"numerical_cols = new_train_data.select_dtypes(include=['float64', 'int64']).columns\ncorr_matrix = new_train_data[numerical_cols].corr()\n\nplt.figure(figsize=(25, 15))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.tight_layout()\nplt.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:06.530943Z","iopub.execute_input":"2024-12-28T19:44:06.531251Z","iopub.status.idle":"2024-12-28T19:44:06.991208Z","shell.execute_reply.started":"2024-12-28T19:44:06.531229Z","shell.execute_reply":"2024-12-28T19:44:06.990314Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handle Missing Values","metadata":{}},{"cell_type":"code","source":"\n# Filling missing values with a placeholder or strategy\nnew_train_data.fillna(\"Unknown\", inplace=True)\nnew_test_data.fillna(\"Unknown\", inplace=True)\n\nprint(\"Missing values handled successfully.\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:13.404228Z","iopub.execute_input":"2024-12-28T19:44:13.404537Z","iopub.status.idle":"2024-12-28T19:44:13.434767Z","shell.execute_reply.started":"2024-12-28T19:44:13.404510Z","shell.execute_reply":"2024-12-28T19:44:13.433885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Encoding Categorical Features","metadata":{}},{"cell_type":"code","source":"categorical_cols = new_train_data.select_dtypes(include=['object']).columns\nlabel_encoder = LabelEncoder()\n\nfor col in categorical_cols:\n    new_train_data[col] = label_encoder.fit_transform(new_train_data[col])\n    new_test_data[col] = label_encoder.transform(new_test_data[col])\n\nprint(\"Categorical columns encoded successfully.\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:16.558942Z","iopub.execute_input":"2024-12-28T19:44:16.559240Z","iopub.status.idle":"2024-12-28T19:44:16.643513Z","shell.execute_reply.started":"2024-12-28T19:44:16.559218Z","shell.execute_reply":"2024-12-28T19:44:16.642714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Data for Modeling","metadata":{}},{"cell_type":"code","source":"\n# Drop columns not useful for modeling\nX = new_train_data.drop(columns=['matched_score', 'address'])\ny = new_train_data['matched_score']\n\n# Train-test split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Data split into training and validation sets successfully.\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:21.395773Z","iopub.execute_input":"2024-12-28T19:44:21.396115Z","iopub.status.idle":"2024-12-28T19:44:21.411746Z","shell.execute_reply.started":"2024-12-28T19:44:21.396086Z","shell.execute_reply":"2024-12-28T19:44:21.410984Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance Using Random Forest","metadata":{}},{"cell_type":"code","source":"\n# Train Random Forest Regressor\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\n# Plot feature importance\nimportance = rf.feature_importances_\nindices = np.argsort(importance)[::-1]\nfeatures = X_train.columns\n\nplt.figure(figsize=(12, 8))\nsns.barplot(x=importance[indices], y=features[indices], palette='viridis')\nplt.title('Feature Importance')\nplt.xlabel('Importance Score')\nplt.ylabel('Features')\nplt.show()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:23.572379Z","iopub.execute_input":"2024-12-28T19:44:23.572661Z","iopub.status.idle":"2024-12-28T19:44:30.122383Z","shell.execute_reply.started":"2024-12-28T19:44:23.572639Z","shell.execute_reply":"2024-12-28T19:44:30.121550Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save Cleaned Data","metadata":{}},{"cell_type":"code","source":"new_train_data.to_csv(\"cleaned_train_data.csv\", index=False)\nnew_test_data.to_csv(\"cleaned_test_data.csv\", index=False)\n\nprint(\"Cleaned data saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:37.434093Z","iopub.execute_input":"2024-12-28T19:44:37.434392Z","iopub.status.idle":"2024-12-28T19:44:37.498735Z","shell.execute_reply.started":"2024-12-28T19:44:37.434369Z","shell.execute_reply":"2024-12-28T19:44:37.498044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport re\nfrom ast import literal_eval\nfrom datetime import datetime\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:41.483998Z","iopub.execute_input":"2024-12-28T19:44:41.484322Z","iopub.status.idle":"2024-12-28T19:44:45.166185Z","shell.execute_reply.started":"2024-12-28T19:44:41.484295Z","shell.execute_reply":"2024-12-28T19:44:45.165486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CV Matching System Documentation\n\n## Components\n```bash\n1. TextProcessor - Text feature processing\n2. FeatureEngineering - Feature creation and transformation\n3. ModelTraining - XGBoost and CatBoost implementation\n```\n## Data Pipeline\n```bash\n1. Text Processing\n2. Feature Engineering\n3. Model Training\n4. Prediction\n```","metadata":{}},{"cell_type":"markdown","source":"## Model Architecture\n\n```json\n1. Text Processing Layer\n    - TF-IDF Vectorization\n    - Count Vectorization\n    - SVD Dimension Reduction\n\n2. Feature Engineering Layer\n    - Numerical Features\n    - Categorical Encoding\n    - Text Features\n\n3. Model Layer\n    - XGBoost\n    - Ensemble\n```","metadata":{}},{"cell_type":"markdown","source":"## TextProcessor","metadata":{}},{"cell_type":"markdown","source":"A class used to preprocess and transform text data using various techniques such as TF-IDF, Count Vectorization, SVD, and NMF.\n\nAttributes\n----------\n```bash\ntfidf_models : dict\n    A dictionary to store TF-IDF vectorizer models for different features.\ncount_models : dict\n    A dictionary to store Count vectorizer models for different features.\nsvd_models : dict\n    A dictionary to store Truncated SVD models for different features.\nnmf_models : dict\n    A dictionary to store NMF models for different features.\nmax_features : int\n    The maximum number of features to consider for vectorization.\n```\nMethods\n-------\n```bash\nclean_text(text)\n    Cleans the input text by converting to lowercase, removing punctuation, replacing digits with 'NUM', and stripping extra whitespace.\nprocess_list(text)\n    Processes a string representation of a list, cleaning each item in the list.\nfit_transform_text(texts, feature_name)\n    Fits and transforms the input texts using TF-IDF, Count Vectorization, SVD, and NMF, and stores the models.\ntransform_text(texts, feature_name)\n    Transforms the input texts using the previously fitted models.\n```","metadata":{}},{"cell_type":"code","source":"class TextProcessor:\n    \"\"\"\n    A class used to preprocess and transform text data using various techniques such as TF-IDF, Count Vectorization, SVD, and NMF.\n\n    Attributes\n    ----------\n    tfidf_models : dict\n        A dictionary to store TF-IDF vectorizer models for different features.\n    count_models : dict\n        A dictionary to store Count vectorizer models for different features.\n    svd_models : dict\n        A dictionary to store Truncated SVD models for different features.\n    nmf_models : dict\n        A dictionary to store NMF models for different features.\n    max_features : int\n        The maximum number of features to consider for vectorization.\n\n    Methods\n    -------\n    clean_text(text)\n        Cleans the input text by converting to lowercase, removing punctuation, replacing digits with 'NUM', and stripping extra whitespace.\n    process_list(text)\n        Processes a string representation of a list, cleaning each item in the list.\n    fit_transform_text(texts, feature_name)\n        Fits and transforms the input texts using TF-IDF, Count Vectorization, SVD, and NMF, and stores the models.\n    transform_text(texts, feature_name)\n        Transforms the input texts using the previously fitted models.\n    \"\"\"\n    def __init__(self, max_features=300):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.nmf_models = {}\n        self.max_features = max_features\n\n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', 'NUM', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n\n    def process_list(self, text):\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 3),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features // 2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50, random_state=42)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        self.nmf_models[feature_name] = NMF(n_components=30, random_state=42)\n        nmf_matrix = self.nmf_models[feature_name].fit_transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        nmf_matrix = self.nmf_models[feature_name].transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:50.887725Z","iopub.execute_input":"2024-12-28T19:44:50.888360Z","iopub.status.idle":"2024-12-28T19:44:50.899252Z","shell.execute_reply.started":"2024-12-28T19:44:50.888327Z","shell.execute_reply":"2024-12-28T19:44:50.898304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURE_WEIGHTS = {\n    'skills_required': 0.12,\n    'locations': 0.10,\n    'experience_requirement': 0.09,\n    'job_position_name': 0.08,\n    'educational_requirements': 0.07,\n    'major_field_of_studies': 0.06,\n    'responsibilities.1': 0.05,\n    'passing_years': 0.05,\n    'career_objective': 0.04,\n    'skills': 0.03,\n    'age_requirement': 0.03,\n    'start_dates': 0.02,\n    'responsibilities': 0.02\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:54.700148Z","iopub.execute_input":"2024-12-28T19:44:54.700435Z","iopub.status.idle":"2024-12-28T19:44:54.704674Z","shell.execute_reply.started":"2024-12-28T19:44:54.700414Z","shell.execute_reply":"2024-12-28T19:44:54.703752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A class used to perform feature engineering on a dataset.\nAttributes\n----------\n```bash\ntext_processor : TextProcessor\n    An instance of TextProcessor for handling text features.\nscaler : StandardScaler\n    An instance of StandardScaler for scaling numeric features.\npower_transformer : PowerTransformer\n    An instance of PowerTransformer for transforming numeric features.\ncategorical_encoders : dict\n    A dictionary to store encoders for categorical features.\n```\nMethods\n-------\n```bash\nextract_years_experience(row)\n    Extracts experience-related features from a row of data.\nclean_education_result(result_str)\n    Cleans and converts educational result strings to numeric values.\nextract_education_features(row)\n    Extracts education-related features from a row of data.\ncalculate_weighted_feature(df)\n    Calculates a composite feature using predefined feature weights.\nencode_categorical_features(df, categorical_columns, is_train=True)\nsanitize_feature_names(df)\n    Cleans feature names to remove special characters unsupported by LightGBM.\ntransform(df, is_train=True)\n    Transforms the input DataFrame by extracting, encoding, and scaling features.\n```","metadata":{}},{"cell_type":"code","source":"class FeatureEngineer:\n    '''\n    A class used to perform feature engineering on a dataset.\n    Attributes\n    ----------\n    text_processor : TextProcessor\n        An instance of TextProcessor for handling text features.\n    scaler : StandardScaler\n        An instance of StandardScaler for scaling numeric features.\n    power_transformer : PowerTransformer\n        An instance of PowerTransformer for transforming numeric features.\n    categorical_encoders : dict\n        A dictionary to store encoders for categorical features.\n    Methods\n    -------\n    extract_years_experience(row)\n        Extracts experience-related features from a row of data.\n    clean_education_result(result_str)\n        Cleans and converts educational result strings to numeric values.\n    extract_education_features(row)\n        Extracts education-related features from a row of data.\n    calculate_weighted_feature(df)\n        Calculates a composite feature using predefined feature weights.\n    encode_categorical_features(df, categorical_columns, is_train=True)\n    sanitize_feature_names(df)\n        Cleans feature names to remove special characters unsupported by LightGBM.\n    transform(df, is_train=True)\n        Transforms the input DataFrame by extracting, encoding, and scaling features.\n    '''\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.scaler = StandardScaler()\n        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n        self.categorical_encoders = {}\n\n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]\n            experiences = [e - s for s, e in zip(start_years, end_years)]\n            return {\n                'total_experience': sum(experiences),\n                'max_experience': max(experiences) if experiences else 0,\n                'num_positions': len(experiences)\n            }\n        except:\n            return {'total_experience': 0, 'max_experience': 0, 'num_positions': 0}\n\n    def clean_education_result(self, result_str):\n        if pd.isna(result_str):\n            return 0\n        try:\n            if result_str.startswith('['):\n                result_str = literal_eval(result_str)[0]\n            result_str = str(result_str).upper()\n            if result_str in ['N/A', 'NONE', 'NAN', '']:\n                return 0\n            result_str = result_str.replace('%', '')\n            return float(result_str)\n        except:\n            return 0\n\n    def extract_education_features(self, row):\n        try:\n            degree = str(row['degree_names']).lower() if not pd.isna(row['degree_names']) else ''\n            result = self.clean_education_result(row['educational_results'])\n            edu_score = 0\n            if 'phd' in degree or 'doctorate' in degree:\n                edu_score = 4\n            elif 'master' in degree:\n                edu_score = 3\n            elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n                edu_score = 2\n            elif 'diploma' in degree or 'certificate' in degree:\n                edu_score = 1\n            return {\n                'education_score': edu_score,\n                'education_result': result,\n                'education_weight': edu_score * (result / 100 if result > 0 else 1)\n            }\n        except:\n            return {\n                'education_score': 0,\n                'education_result': 0,\n                'education_weight': 0\n            }\n\n    def calculate_weighted_feature(self, df):\n        \"\"\"Calculates a composite feature using FEATURE_WEIGHTS.\"\"\"\n        df['composite_feature'] = 0  # Initialize composite feature\n        for feature, weight in FEATURE_WEIGHTS.items():\n            if feature in df.columns:\n                numeric_feature = pd.to_numeric(df[feature], errors='coerce').fillna(0)\n                df['composite_feature'] += numeric_feature * weight\n        return df\n\n    def encode_categorical_features(self, df, categorical_columns, is_train=True):\n        \"\"\"\n        Encodes categorical columns using One-Hot or Label Encoding.\n        \"\"\"\n        encoded_dfs = []\n        for col in categorical_columns:\n            if is_train:\n                if df[col].nunique() <= 50:  # Threshold for OneHotEncoding\n                    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n                    encoded = encoder.fit_transform(df[[col]].fillna('Unknown'))\n                    self.categorical_encoders[col] = encoder\n                    df_encoded = pd.DataFrame(\n                        encoded,\n                        columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n                        index=df.index\n                    )\n                else:\n                    encoder = LabelEncoder()\n                    encoded = encoder.fit_transform(df[col].fillna('Unknown'))\n                    self.categorical_encoders[col] = encoder\n                    df_encoded = pd.DataFrame(\n                        {f\"{col}_label\": encoded}, index=df.index\n                    )\n            else:\n                encoder = self.categorical_encoders.get(col)\n                if encoder:\n                    if isinstance(encoder, OneHotEncoder):\n                        encoded = encoder.transform(df[[col]].fillna('Unknown'))\n                        df_encoded = pd.DataFrame(\n                            encoded,\n                            columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n                            index=df.index\n                        )\n                    elif isinstance(encoder, LabelEncoder):\n                        encoded = encoder.transform(df[col].fillna('Unknown'))\n                        df_encoded = pd.DataFrame(\n                            {f\"{col}_label\": encoded}, index=df.index\n                        )\n                    else:\n                        raise ValueError(f\"Unsupported encoder type for column '{col}'\")\n                else:\n                    # Handle cases where the encoder is not available\n                    print(f\"Warning: No encoder found for column '{col}'. Using default encoding.\")\n                    df_encoded = pd.DataFrame(\n                        {f\"{col}_label\": [0] * len(df)}, index=df.index\n                    )\n    \n            # Append the encoded DataFrame to the list\n            encoded_dfs.append(df_encoded)\n        \n        # Concatenate all encoded columns\n        if encoded_dfs:\n            return pd.concat(encoded_dfs, axis=1)\n        else:\n            # If no valid encoding was applied, return an empty DataFrame\n            return pd.DataFrame(index=df.index)\n\n\n\n    def sanitize_feature_names(self, df):\n        \"\"\"Clean feature names to remove special characters unsupported by LightGBM.\"\"\"\n        df.columns = [\n            re.sub(r'[^\\w\\.-]', '_', col).replace('__', '_') for col in df.columns\n        ]\n        return df\n    def transform(self, df, is_train=True):\n        # Numeric features from experience and education\n        exp_features = df.apply(self.extract_years_experience, axis=1)\n        edu_features = df.apply(self.extract_education_features, axis=1)\n    \n        feature_dict = {}\n        for feat in ['total_experience', 'max_experience', 'num_positions']:\n            feature_dict[feat] = [x[feat] for x in exp_features]\n        for feat in ['education_score', 'education_result', 'education_weight']:\n            feature_dict[feat] = [x[feat] for x in edu_features]\n    \n        # Basic numeric count features\n        feature_dict['num_skills'] = df['skills'].fillna('').str.count(',') + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        feature_dict['num_languages'] = df['languages'].fillna('').str.count(',') + 1\n    \n        # Additional *interaction* features\n        feature_dict['experience_per_position'] = np.array(feature_dict['total_experience']) / (\n            np.array(feature_dict['num_positions']) + 0.1\n        )\n        feature_dict['result_x_edu_score'] = (\n            np.array(feature_dict['education_result']) * np.array(feature_dict['education_score'])\n        )\n    \n        numeric_df = pd.DataFrame(feature_dict, index=df.index)\n    \n        # Drop constant columns\n        constant_cols = numeric_df.columns[numeric_df.nunique() <= 1]\n        if len(constant_cols) > 0:\n            print(f\"Dropping constant columns: {list(constant_cols)}\")\n            numeric_df = numeric_df.drop(columns=constant_cols)\n    \n        # Power transform or scale numeric features\n        if is_train:\n            try:\n                numeric_arr = self.power_transformer.fit_transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.fit_transform(numeric_df)\n        else:\n            try:\n                numeric_arr = self.power_transformer.transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.transform(numeric_df)\n    \n        numeric_df = pd.DataFrame(numeric_arr, columns=numeric_df.columns, index=numeric_df.index)\n    \n        # Text features\n        text_features = [\n            'skills', 'career_objective', 'responsibilities',\n            'educational_institution_name', 'certification_skills',\n            'major_field_of_studies'\n        ]\n    \n        all_text_features = {}\n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n    \n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n    \n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n    \n        # Handle categorical features\n        categorical_columns = [\n             'locations', 'result_types',\n            'extra_curricular_activity_types', 'role_positions', 'proficiency_levels'\n        ]\n        categorical_df = self.encode_categorical_features(df, categorical_columns, is_train)\n    \n        # Calculate and add weighted composite feature\n        df = self.calculate_weighted_feature(df)\n        numeric_df['composite_feature'] = df['composite_feature']\n    \n        # Combine all features\n        combined_features = pd.concat([numeric_df, text_feature_df, categorical_df], axis=1)\n    \n        # Sanitize feature names\n        combined_features = self.sanitize_feature_names(combined_features)\n    \n        # Return the final feature DataFrame\n        return combined_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:44:57.497460Z","iopub.execute_input":"2024-12-28T19:44:57.497748Z","iopub.status.idle":"2024-12-28T19:44:57.519797Z","shell.execute_reply.started":"2024-12-28T19:44:57.497726Z","shell.execute_reply":"2024-12-28T19:44:57.518726Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\ntest_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:45:04.131076Z","iopub.execute_input":"2024-12-28T19:45:04.131414Z","iopub.status.idle":"2024-12-28T19:45:04.305523Z","shell.execute_reply.started":"2024-12-28T19:45:04.131386Z","shell.execute_reply":"2024-12-28T19:45:04.304742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LightGBM model optimizer using Optuna for hyperparameter tuning. \n- This class implements hyperparameter optimization for LightGBM models using Optuna's Bayesian optimization framework.\n- It performs k-fold cross-validation to evaluate parameter combinations and finds the optimal set of hyperparameters for regression tasks.\n\nParameters\n----------\n```\nn_trials : int, default=50\n    Number of optimization trials to perform with Optuna.\n```\nAttributes\n----------\n```\nbest_params : dict or None\n    Best hyperparameters found during optimization. None before optimization is performed.\n```\nMethods\n-------\n```\nobjective(trial, X, y)\n    Defines the objective function for Optuna optimization.\nfind_best_params(X, y)\n    Executes the optimization process to find best hyperparameters.\n```\nNotes\n-----\n- Uses GPU acceleration for training when available\n- Implements 5-fold cross-validation\n- Optimizes for mean squared error (MSE)\n- Uses early stopping to prevent overfitting\n- Handles invalid parameter combinations gracefully\nExamples\n--------\n```\n>>> optimizer = OptimizedLightGBM(n_trials=100)\n>>> best_params = optimizer.find_best_params(X_train, y_train)\n```","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass OptimizedLightGBM:\n    \"\"\"LightGBM model optimizer using Optuna for hyperparameter tuning.\n    This class implements hyperparameter optimization for LightGBM models using Optuna's\n    Bayesian optimization framework. It performs k-fold cross-validation to evaluate \n    parameter combinations and finds the optimal set of hyperparameters for regression tasks.\n    Parameters\n    ----------\n    n_trials : int, default=50\n        Number of optimization trials to perform with Optuna.\n    Attributes\n    ----------\n    best_params : dict or None\n        Best hyperparameters found during optimization. None before optimization is performed.\n    Methods\n    -------\n    objective(trial, X, y)\n        Defines the objective function for Optuna optimization.\n    find_best_params(X, y)\n        Executes the optimization process to find best hyperparameters.\n    Notes\n    -----\n    - Uses GPU acceleration for training when available\n    - Implements 5-fold cross-validation\n    - Optimizes for mean squared error (MSE)\n    - Uses early stopping to prevent overfitting\n    - Handles invalid parameter combinations gracefully\n    Examples\n    --------\n    >>> optimizer = OptimizedLightGBM(n_trials=100)\n    >>> best_params = optimizer.find_best_params(X_train, y_train)\n    \"\"\"\n    def __init__(self, n_trials=50):\n        self.n_trials = n_trials\n        self.best_params = None\n        \n    def objective(self, trial, X, y):\n        param = {\n            'objective': 'regression_l2',\n            'metric': 'l2',\n            'boosting_type': 'gbdt',\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'verbose': -1,\n            \n            # GPU-compatible hyperparameters\n            'num_leaves': trial.suggest_int('num_leaves', 31, 128),  # Reduced max leaves\n            'max_depth': trial.suggest_int('max_depth', 5, 12),      # Reduced max depth\n            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n            'min_child_samples': trial.suggest_int('min_child_samples', 20, 80),\n            'max_bin': trial.suggest_int('max_bin', 63, 255),  # GPU-compatible bin size\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n        }\n        \n        # K-fold cross-validation\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        scores = []\n        \n        for train_idx, val_idx in kf.split(X):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            train_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_val, label=y_val)\n            \n            try:\n                model = lgb.train(\n                    param,\n                    train_data,\n                    valid_sets=[val_data],\n                    num_boost_round=1000,\n                    callbacks=[\n                        lgb.early_stopping(stopping_rounds=50),\n                        lgb.log_evaluation(period=0)\n                    ]\n                )\n                \n                preds = model.predict(X_val)\n                fold_score = mean_squared_error(y_val, preds)\n                scores.append(fold_score)\n            except lgb.basic.LightGBMError as e:\n                # Return a large error score if the parameters are invalid\n                return float('inf')\n        \n        return np.mean(scores)\n\n    def find_best_params(self, X, y):\n        study = optuna.create_study(direction='minimize')\n        study.optimize(lambda trial: self.objective(trial, X, y),\n                      n_trials=self.n_trials)\n        \n        self.best_params = study.best_params\n        print(\"\\nOptimization Results:\")\n        print(\"Best MSE:\", study.best_value)\n        print(\"Best parameters:\", self.best_params)\n        return self.best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:45:13.637745Z","iopub.execute_input":"2024-12-28T19:45:13.638098Z","iopub.status.idle":"2024-12-28T19:45:13.648020Z","shell.execute_reply.started":"2024-12-28T19:45:13.638068Z","shell.execute_reply":"2024-12-28T19:45:13.647107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    fe = FeatureEngineer()\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n    \n    y = train_df['matched_score'].values\n    # Find best parameters\n    print(\"Finding optimal hyperparameters...\")\n    optimizer = OptimizedLightGBM(n_trials=100)\n    best_params = optimizer.find_best_params(train_features, y)\n    \n    # params = {\n    #     'objective': 'regression_l2',\n    #     'metric': 'l2',\n    #     'num_leaves': 96,\n    #     'max_depth': 12,\n    #     'learning_rate': 0.03199484792860085,\n    #     'feature_fraction': 0.6706304075294632,\n    #     'bagging_fraction': 0.958672143301124,\n    #     'bagging_freq': 3,\n    #     'min_child_samples': 65,\n    #     'lambda_l1': 0.014964605006024168,\n    #     'lambda_l2': 4.20761346366579e-08,\n    #     'max_bin': 178,\n    #     'gpu_use_dp': True,  # Enable multi-GPU training\n    #     'tree_learner': 'data_parallel',  # Use multi-GPU setup\n    #     'verbose': -1\n        \n    # }\n    \n    # Cross-validation and model training\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(len(train_features))\n    test_predictions = np.zeros(len(test_features))\n    feature_importance_df = pd.DataFrame()\n    fold_scores = []\n    \n    print(\"\\nTraining final model with best parameters...\")\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n        print(f\"\\nFold {fold + 1}/5\")\n        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val)\n        \n        model = lgb.train(\n            best_params,\n            # params,\n            train_data,\n            valid_sets=[train_data, val_data],\n            num_boost_round=1000,\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(100)\n            ]\n        )\n        \n        val_preds = model.predict(X_val)\n        oof_predictions[val_idx] = val_preds\n        test_predictions += model.predict(test_features) / kf.n_splits\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n        fold_scores.append(fold_rmse)\n        print(f\"Fold {fold + 1} RMSE: {fold_rmse:.6f}\")\n        \n        fold_importance = pd.DataFrame({\n            \"feature\": train_features.columns,\n            \"importance\": model.feature_importance(),\n            \"fold\": fold + 1\n        })\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)\n    \n    # Calculate final metrics\n    final_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n    # final_r2 = r2_score(y, oof_predictions)\n    \n    print(\"\\nFinal Model Performance:\")\n    print(f\"Average RMSE: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n    print(f\"Overall RMSE: {final_rmse:.6f}\")\n    # print(f\"R2 Score: {final_r2:.6f}\")\n    \n    # Save predictions\n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_predictions\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission saved to submission.csv\")\n    \n    # Save feature importance\n    feature_importance = (feature_importance_df.groupby('feature')['importance']\n                        .mean()\n                        .sort_values(ascending=False))\n    feature_importance.to_csv('feature_importance.csv')\n    \n    print(\"\\nTop 10 Most Important Features:\")\n    print(feature_importance.head(10))\n    \n    return submission, feature_importance_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:47:23.611578Z","iopub.execute_input":"2024-12-28T19:47:23.611887Z","iopub.status.idle":"2024-12-28T19:47:23.621673Z","shell.execute_reply.started":"2024-12-28T19:47:23.611864Z","shell.execute_reply":"2024-12-28T19:47:23.620978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T19:47:29.160088Z","iopub.execute_input":"2024-12-28T19:47:29.160382Z","iopub.status.idle":"2024-12-28T19:50:06.001231Z","shell.execute_reply.started":"2024-12-28T19:47:29.160361Z","shell.execute_reply":"2024-12-28T19:50:06.000264Z"}},"outputs":[],"execution_count":null}]}