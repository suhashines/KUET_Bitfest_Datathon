{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"},{"sourceId":10314104,"sourceType":"datasetVersion","datasetId":6385201}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport re\nfrom ast import literal_eval\nfrom datetime import datetime\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T18:45:47.841005Z","iopub.execute_input":"2024-12-28T18:45:47.841264Z","iopub.status.idle":"2024-12-28T18:45:52.376727Z","shell.execute_reply.started":"2024-12-28T18:45:47.841235Z","shell.execute_reply":"2024-12-28T18:45:52.376053Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"###############################################################################\n# TextProcessor\n###############################################################################","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self, max_features=300):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.nmf_models = {}\n        self.max_features = max_features\n\n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', 'NUM', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n\n    def process_list(self, text):\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 3),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features // 2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50, random_state=42)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        self.nmf_models[feature_name] = NMF(n_components=30, random_state=42)\n        nmf_matrix = self.nmf_models[feature_name].fit_transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        nmf_matrix = self.nmf_models[feature_name].transform(tfidf_matrix)\n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T18:45:54.308071Z","iopub.execute_input":"2024-12-28T18:45:54.308378Z","iopub.status.idle":"2024-12-28T18:45:54.317819Z","shell.execute_reply.started":"2024-12-28T18:45:54.308352Z","shell.execute_reply":"2024-12-28T18:45:54.316917Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"FEATURE_WEIGHTS = {\n    'skills_required': 0.12,\n    'locations': 0.10,\n    'experience_requirement': 0.09,\n    'job_position_name': 0.08,\n    'educational_requirements': 0.07,\n    'major_field_of_studies': 0.06,\n    'responsibilities.1': 0.05,\n    'passing_years': 0.05,\n    'career_objective': 0.04,\n    'skills': 0.03,\n    'age_requirement': 0.03,\n    'start_dates': 0.02,\n    'responsibilities': 0.02\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T18:45:57.476628Z","iopub.execute_input":"2024-12-28T18:45:57.476910Z","iopub.status.idle":"2024-12-28T18:45:57.481006Z","shell.execute_reply.started":"2024-12-28T18:45:57.476890Z","shell.execute_reply":"2024-12-28T18:45:57.480055Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.scaler = StandardScaler()\n        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n        self.categorical_encoders = {}\n\n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]\n            experiences = [e - s for s, e in zip(start_years, end_years)]\n            return {\n                'total_experience': sum(experiences),\n                'max_experience': max(experiences) if experiences else 0,\n                'num_positions': len(experiences)\n            }\n        except:\n            return {'total_experience': 0, 'max_experience': 0, 'num_positions': 0}\n\n    def clean_education_result(self, result_str):\n        if pd.isna(result_str):\n            return 0\n        try:\n            if result_str.startswith('['):\n                result_str = literal_eval(result_str)[0]\n            result_str = str(result_str).upper()\n            if result_str in ['N/A', 'NONE', 'NAN', '']:\n                return 0\n            result_str = result_str.replace('%', '')\n            return float(result_str)\n        except:\n            return 0\n\n    def extract_education_features(self, row):\n        try:\n            degree = str(row['degree_names']).lower() if not pd.isna(row['degree_names']) else ''\n            result = self.clean_education_result(row['educational_results'])\n            edu_score = 0\n            if 'phd' in degree or 'doctorate' in degree:\n                edu_score = 4\n            elif 'master' in degree:\n                edu_score = 3\n            elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n                edu_score = 2\n            elif 'diploma' in degree or 'certificate' in degree:\n                edu_score = 1\n            return {\n                'education_score': edu_score,\n                'education_result': result,\n                'education_weight': edu_score * (result / 100 if result > 0 else 1)\n            }\n        except:\n            return {\n                'education_score': 0,\n                'education_result': 0,\n                'education_weight': 0\n            }\n\n    def calculate_weighted_feature(self, df):\n        \"\"\"Calculates a composite feature using FEATURE_WEIGHTS.\"\"\"\n        df['composite_feature'] = 0  # Initialize composite feature\n        for feature, weight in FEATURE_WEIGHTS.items():\n            if feature in df.columns:\n                numeric_feature = pd.to_numeric(df[feature], errors='coerce').fillna(0)\n                df['composite_feature'] += numeric_feature * weight\n        return df\n\n    def encode_categorical_features(self, df, categorical_columns, is_train=True):\n        \"\"\"\n        Encodes categorical columns using One-Hot or Label Encoding.\n        \"\"\"\n        encoded_dfs = []\n        for col in categorical_columns:\n            if is_train:\n                if df[col].nunique() <= 50:  # Threshold for OneHotEncoding\n                    encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n                    encoded = encoder.fit_transform(df[[col]].fillna('Unknown'))\n                    self.categorical_encoders[col] = encoder\n                    df_encoded = pd.DataFrame(\n                        encoded,\n                        columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n                        index=df.index\n                    )\n                else:\n                    encoder = LabelEncoder()\n                    encoded = encoder.fit_transform(df[col].fillna('Unknown'))\n                    self.categorical_encoders[col] = encoder\n                    df_encoded = pd.DataFrame(\n                        {f\"{col}_label\": encoded}, index=df.index\n                    )\n            else:\n                encoder = self.categorical_encoders.get(col)\n                if encoder:\n                    if isinstance(encoder, OneHotEncoder):\n                        encoded = encoder.transform(df[[col]].fillna('Unknown'))\n                        df_encoded = pd.DataFrame(\n                            encoded,\n                            columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]],\n                            index=df.index\n                        )\n                    elif isinstance(encoder, LabelEncoder):\n                        encoded = encoder.transform(df[col].fillna('Unknown'))\n                        df_encoded = pd.DataFrame(\n                            {f\"{col}_label\": encoded}, index=df.index\n                        )\n                    else:\n                        raise ValueError(f\"Unsupported encoder type for column '{col}'\")\n                else:\n                    # Handle cases where the encoder is not available\n                    print(f\"Warning: No encoder found for column '{col}'. Using default encoding.\")\n                    df_encoded = pd.DataFrame(\n                        {f\"{col}_label\": [0] * len(df)}, index=df.index\n                    )\n    \n            # Append the encoded DataFrame to the list\n            encoded_dfs.append(df_encoded)\n        \n        # Concatenate all encoded columns\n        if encoded_dfs:\n            return pd.concat(encoded_dfs, axis=1)\n        else:\n            # If no valid encoding was applied, return an empty DataFrame\n            return pd.DataFrame(index=df.index)\n\n\n\n    def sanitize_feature_names(self, df):\n        \"\"\"Clean feature names to remove special characters unsupported by LightGBM.\"\"\"\n        df.columns = [\n            re.sub(r'[^\\w\\.-]', '_', col).replace('__', '_') for col in df.columns\n        ]\n        return df\n    def transform(self, df, is_train=True):\n        # Numeric features from experience and education\n        exp_features = df.apply(self.extract_years_experience, axis=1)\n        edu_features = df.apply(self.extract_education_features, axis=1)\n    \n        feature_dict = {}\n        for feat in ['total_experience', 'max_experience', 'num_positions']:\n            feature_dict[feat] = [x[feat] for x in exp_features]\n        for feat in ['education_score', 'education_result', 'education_weight']:\n            feature_dict[feat] = [x[feat] for x in edu_features]\n    \n        # Basic numeric count features\n        feature_dict['num_skills'] = df['skills'].fillna('').str.count(',') + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        feature_dict['num_languages'] = df['languages'].fillna('').str.count(',') + 1\n    \n        # Additional *interaction* features\n        feature_dict['experience_per_position'] = np.array(feature_dict['total_experience']) / (\n            np.array(feature_dict['num_positions']) + 0.1\n        )\n        feature_dict['result_x_edu_score'] = (\n            np.array(feature_dict['education_result']) * np.array(feature_dict['education_score'])\n        )\n    \n        numeric_df = pd.DataFrame(feature_dict, index=df.index)\n    \n        # Drop constant columns\n        constant_cols = numeric_df.columns[numeric_df.nunique() <= 1]\n        if len(constant_cols) > 0:\n            print(f\"Dropping constant columns: {list(constant_cols)}\")\n            numeric_df = numeric_df.drop(columns=constant_cols)\n    \n        # Power transform or scale numeric features\n        if is_train:\n            try:\n                numeric_arr = self.power_transformer.fit_transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.fit_transform(numeric_df)\n        else:\n            try:\n                numeric_arr = self.power_transformer.transform(numeric_df)\n            except Exception as e:\n                print(f\"PowerTransformer failed: {e}. Falling back to StandardScaler.\")\n                numeric_arr = self.scaler.transform(numeric_df)\n    \n        numeric_df = pd.DataFrame(numeric_arr, columns=numeric_df.columns, index=numeric_df.index)\n    \n        # Text features\n        text_features = [\n            'skills', 'career_objective', 'responsibilities',\n            'educational_institution_name', 'certification_skills',\n            'major_field_of_studies'\n        ]\n    \n        all_text_features = {}\n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n    \n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n    \n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n    \n        # Handle categorical features\n        categorical_columns = [\n             'locations', 'result_types',\n            'extra_curricular_activity_types', 'role_positions', 'proficiency_levels'\n        ]\n        categorical_df = self.encode_categorical_features(df, categorical_columns, is_train)\n    \n        # Calculate and add weighted composite feature\n        df = self.calculate_weighted_feature(df)\n        numeric_df['composite_feature'] = df['composite_feature']\n    \n        # Combine all features\n        combined_features = pd.concat([numeric_df, text_feature_df, categorical_df], axis=1)\n    \n        # Sanitize feature names\n        combined_features = self.sanitize_feature_names(combined_features)\n    \n        # Return the final feature DataFrame\n        return combined_features\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T18:46:22.590297Z","iopub.execute_input":"2024-12-28T18:46:22.590627Z","iopub.status.idle":"2024-12-28T18:46:22.611700Z","shell.execute_reply.started":"2024-12-28T18:46:22.590602Z","shell.execute_reply":"2024-12-28T18:46:22.610890Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T18:46:41.931842Z","iopub.execute_input":"2024-12-28T18:46:41.932107Z","iopub.status.idle":"2024-12-28T18:46:42.102115Z","shell.execute_reply.started":"2024-12-28T18:46:41.932086Z","shell.execute_reply":"2024-12-28T18:46:42.101440Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass OptimizedLightGBM:\n    def __init__(self, n_trials=50):\n        self.n_trials = n_trials\n        self.best_params = None\n        \n    def objective(self, trial, X, y):\n        param = {\n            'objective': 'regression_l2',\n            'metric': 'l2',\n            'boosting_type': 'gbdt',\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'verbose': -1,\n            \n            # GPU-compatible hyperparameters\n            'num_leaves': trial.suggest_int('num_leaves', 31, 128),  # Reduced max leaves\n            'max_depth': trial.suggest_int('max_depth', 5, 12),      # Reduced max depth\n            'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.05),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n            'min_child_samples': trial.suggest_int('min_child_samples', 20, 80),\n            'max_bin': trial.suggest_int('max_bin', 63, 255),  # GPU-compatible bin size\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 50),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n        }\n        \n        # K-fold cross-validation\n        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n        scores = []\n        \n        for train_idx, val_idx in kf.split(X):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            train_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_val, label=y_val)\n            \n            try:\n                model = lgb.train(\n                    param,\n                    train_data,\n                    valid_sets=[val_data],\n                    num_boost_round=1000,\n                    callbacks=[\n                        lgb.early_stopping(stopping_rounds=50),\n                        lgb.log_evaluation(period=0)\n                    ]\n                )\n                \n                preds = model.predict(X_val)\n                fold_score = mean_squared_error(y_val, preds)\n                scores.append(fold_score)\n            except lgb.basic.LightGBMError as e:\n                # Return a large error score if the parameters are invalid\n                return float('inf')\n        \n        return np.mean(scores)\n\n    def find_best_params(self, X, y):\n        study = optuna.create_study(direction='minimize')\n        study.optimize(lambda trial: self.objective(trial, X, y),\n                      n_trials=self.n_trials)\n        \n        self.best_params = study.best_params\n        print(\"\\nOptimization Results:\")\n        print(\"Best MSE:\", study.best_value)\n        print(\"Best parameters:\", self.best_params)\n        return self.best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T18:46:32.672819Z","iopub.execute_input":"2024-12-28T18:46:32.673093Z","iopub.status.idle":"2024-12-28T18:46:32.682606Z","shell.execute_reply.started":"2024-12-28T18:46:32.673072Z","shell.execute_reply":"2024-12-28T18:46:32.681922Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def main():\n    # Load datasets\n    # train_df = pd.read_csv('/kaggle/input/kuet-dset-2/train.csv')\n    # test_df = pd.read_csv('/kaggle/input/kuet-dset-2/test.csv')\n    \n    # Feature Engineering\n    fe = FeatureEngineer()\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n    \n    y = train_df['matched_score'].values\n    \n    # Find best parameters\n    print(\"Finding optimal hyperparameters...\")\n    optimizer = OptimizedLightGBM(n_trials=100)\n    best_params = optimizer.find_best_params(train_features, y)\n    \n    # Add fixed parameters to best_params\n    best_params.update({\n        'objective': 'regression_l2',\n        'metric': 'l2',\n        'boosting_type': 'gbdt',\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'verbose': -1,\n        'max_bin': 255  # Ensure GPU compatibility\n    })\n\n    # params = {\n    #     'objective': 'regression_l2',\n    #     'metric': 'l2',\n    #     'num_leaves': 96,\n    #     'max_depth': 12,\n    #     'learning_rate': 0.03199484792860085,\n    #     'feature_fraction': 0.6706304075294632,\n    #     'bagging_fraction': 0.958672143301124,\n    #     'bagging_freq': 3,\n    #     'min_child_samples': 65,\n    #     'lambda_l1': 0.014964605006024168,\n    #     'lambda_l2': 4.20761346366579e-08,\n    #     'max_bin': 178,\n    #     'gpu_use_dp': True,  # Enable multi-GPU training\n    #     'tree_learner': 'data_parallel',  # Use multi-GPU setup\n    #     'verbose': -1\n        \n    # }\n    \n    # Cross-validation and model training\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof_predictions = np.zeros(len(train_features))\n    test_predictions = np.zeros(len(test_features))\n    feature_importance_df = pd.DataFrame()\n    fold_scores = []\n    \n    print(\"\\nTraining final model with best parameters...\")\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n        print(f\"\\nFold {fold + 1}/5\")\n        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n        \n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val)\n        \n        model = lgb.train(\n            best_params,\n            # params,\n            train_data,\n            valid_sets=[train_data, val_data],\n            num_boost_round=1000,\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50),\n                lgb.log_evaluation(100)\n            ]\n        )\n        \n        val_preds = model.predict(X_val)\n        oof_predictions[val_idx] = val_preds\n        test_predictions += model.predict(test_features) / kf.n_splits\n        \n        fold_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n        fold_scores.append(fold_rmse)\n        print(f\"Fold {fold + 1} RMSE: {fold_rmse:.6f}\")\n        \n        fold_importance = pd.DataFrame({\n            \"feature\": train_features.columns,\n            \"importance\": model.feature_importance(),\n            \"fold\": fold + 1\n        })\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance], axis=0)\n    \n    # Calculate final metrics\n    final_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n    # final_r2 = r2_score(y, oof_predictions)\n    \n    print(\"\\nFinal Model Performance:\")\n    print(f\"Average RMSE: {np.mean(fold_scores):.6f} ± {np.std(fold_scores):.6f}\")\n    print(f\"Overall RMSE: {final_rmse:.6f}\")\n    # print(f\"R2 Score: {final_r2:.6f}\")\n    \n    # Save predictions\n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_predictions\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"\\nSubmission saved to submission.csv\")\n    \n    # Save feature importance\n    feature_importance = (feature_importance_df.groupby('feature')['importance']\n                        .mean()\n                        .sort_values(ascending=False))\n    feature_importance.to_csv('feature_importance.csv')\n    \n    print(\"\\nTop 10 Most Important Features:\")\n    print(feature_importance.head(10))\n    \n    return submission, feature_importance_df\n\nfrom sklearn.model_selection import KFold  # Add this import\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# The rest of your code remains the same...\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df.columns)\nprint(test_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T19:37:43.675545Z","iopub.execute_input":"2024-12-27T19:37:43.675868Z","iopub.status.idle":"2024-12-27T19:37:43.681149Z","shell.execute_reply.started":"2024-12-27T19:37:43.675835Z","shell.execute_reply":"2024-12-27T19:37:43.680258Z"}},"outputs":[{"name":"stdout","text":"Index(['address', 'career_objective', 'skills', 'educational_institution_name',\n       'degree_names', 'passing_years', 'educational_results', 'result_types',\n       'major_field_of_studies', 'professional_company_names', 'company_urls',\n       'start_dates', 'end_dates', 'related_skils_in_job', 'positions',\n       'locations', 'responsibilities', 'extra_curricular_activity_types',\n       'extra_curricular_organization_names',\n       'extra_curricular_organization_links', 'role_positions', 'languages',\n       'proficiency_levels', 'certification_providers', 'certification_skills',\n       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n       'educationaL_requirements', 'experiencere_requirement',\n       'age_requirement', 'responsibilities.1', 'skills_required',\n       'matched_score'],\n      dtype='object')\nIndex(['ID', 'address', 'career_objective', 'skills',\n       'educational_institution_name', 'degree_names', 'passing_years',\n       'educational_results', 'result_types', 'major_field_of_studies',\n       'professional_company_names', 'company_urls', 'start_dates',\n       'end_dates', 'related_skils_in_job', 'positions', 'locations',\n       'responsibilities', 'extra_curricular_activity_types',\n       'extra_curricular_organization_names',\n       'extra_curricular_organization_links', 'role_positions', 'languages',\n       'proficiency_levels', 'certification_providers', 'certification_skills',\n       'online_links', 'issue_dates', 'expiry_dates', '﻿job_position_name',\n       'educationaL_requirements', 'experiencere_requirement',\n       'age_requirement', 'responsibilities.1', 'skills_required'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T19:59:50.062544Z","iopub.execute_input":"2024-12-27T19:59:50.062858Z","iopub.status.idle":"2024-12-27T20:03:28.639888Z","shell.execute_reply.started":"2024-12-27T19:59:50.062835Z","shell.execute_reply":"2024-12-27T20:03:28.638920Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\nTransforming test data...\nTraining fold 1...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0122969\tvalid_1's l2: 0.0145609\n[200]\ttraining's l2: 0.00777817\tvalid_1's l2: 0.0117397\n[300]\ttraining's l2: 0.00567534\tvalid_1's l2: 0.0105121\n[400]\ttraining's l2: 0.00451381\tvalid_1's l2: 0.00991951\n[500]\ttraining's l2: 0.00374165\tvalid_1's l2: 0.00954762\n[600]\ttraining's l2: 0.00320871\tvalid_1's l2: 0.00935105\n[700]\ttraining's l2: 0.00280399\tvalid_1's l2: 0.00923491\n[800]\ttraining's l2: 0.0024527\tvalid_1's l2: 0.00912971\n[900]\ttraining's l2: 0.0021446\tvalid_1's l2: 0.00905242\n[1000]\ttraining's l2: 0.00188949\tvalid_1's l2: 0.00899401\nDid not meet early stopping. Best iteration is:\n[999]\ttraining's l2: 0.00189165\tvalid_1's l2: 0.00899389\nTraining fold 2...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0124233\tvalid_1's l2: 0.0150714\n[200]\ttraining's l2: 0.00774646\tvalid_1's l2: 0.0116513\n[300]\ttraining's l2: 0.00560613\tvalid_1's l2: 0.0102117\n[400]\ttraining's l2: 0.00444131\tvalid_1's l2: 0.00955866\n[500]\ttraining's l2: 0.00366715\tvalid_1's l2: 0.0092116\n[600]\ttraining's l2: 0.0031402\tvalid_1's l2: 0.00901317\n[700]\ttraining's l2: 0.002728\tvalid_1's l2: 0.0088893\n[800]\ttraining's l2: 0.00237846\tvalid_1's l2: 0.00881281\n[900]\ttraining's l2: 0.00208079\tvalid_1's l2: 0.0087236\n[1000]\ttraining's l2: 0.00182558\tvalid_1's l2: 0.00866616\nDid not meet early stopping. Best iteration is:\n[998]\ttraining's l2: 0.00182968\tvalid_1's l2: 0.00866548\nTraining fold 3...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0125104\tvalid_1's l2: 0.0144617\n[200]\ttraining's l2: 0.00789306\tvalid_1's l2: 0.0111564\n[300]\ttraining's l2: 0.00569715\tvalid_1's l2: 0.00976458\n[400]\ttraining's l2: 0.00453074\tvalid_1's l2: 0.0091237\n[500]\ttraining's l2: 0.0038014\tvalid_1's l2: 0.00883369\n[600]\ttraining's l2: 0.00326508\tvalid_1's l2: 0.00863366\n[700]\ttraining's l2: 0.00282438\tvalid_1's l2: 0.00847576\n[800]\ttraining's l2: 0.00247329\tvalid_1's l2: 0.00838698\n[900]\ttraining's l2: 0.00216802\tvalid_1's l2: 0.00829745\n[1000]\ttraining's l2: 0.00190781\tvalid_1's l2: 0.00821903\nDid not meet early stopping. Best iteration is:\n[998]\ttraining's l2: 0.00191212\tvalid_1's l2: 0.00821844\nTraining fold 4...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.0122159\tvalid_1's l2: 0.0151241\n[200]\ttraining's l2: 0.0077162\tvalid_1's l2: 0.0117726\n[300]\ttraining's l2: 0.00566146\tvalid_1's l2: 0.0104126\n[400]\ttraining's l2: 0.00449623\tvalid_1's l2: 0.00972557\n[500]\ttraining's l2: 0.00374449\tvalid_1's l2: 0.00936278\n[600]\ttraining's l2: 0.00320946\tvalid_1's l2: 0.00917391\n[700]\ttraining's l2: 0.00279643\tvalid_1's l2: 0.00903002\n[800]\ttraining's l2: 0.00243451\tvalid_1's l2: 0.00890194\n[900]\ttraining's l2: 0.00213815\tvalid_1's l2: 0.00881049\n[1000]\ttraining's l2: 0.00188494\tvalid_1's l2: 0.00876582\nDid not meet early stopping. Best iteration is:\n[990]\ttraining's l2: 0.00190901\tvalid_1's l2: 0.0087642\nTraining fold 5...\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.012414\tvalid_1's l2: 0.0143693\n[200]\ttraining's l2: 0.00782183\tvalid_1's l2: 0.0109475\n[300]\ttraining's l2: 0.00570195\tvalid_1's l2: 0.0096242\n[400]\ttraining's l2: 0.00454439\tvalid_1's l2: 0.00899409\n[500]\ttraining's l2: 0.00377918\tvalid_1's l2: 0.00864412\n[600]\ttraining's l2: 0.00323269\tvalid_1's l2: 0.00848758\n[700]\ttraining's l2: 0.00279921\tvalid_1's l2: 0.00834554\n[800]\ttraining's l2: 0.00244174\tvalid_1's l2: 0.00826099\n[900]\ttraining's l2: 0.00212978\tvalid_1's l2: 0.00819624\n[1000]\ttraining's l2: 0.00187125\tvalid_1's l2: 0.00814916\nDid not meet early stopping. Best iteration is:\n[1000]\ttraining's l2: 0.00187125\tvalid_1's l2: 0.00814916\nOOF MSE: 0.008558\nSubmission saved to submission.csv\n","output_type":"stream"}],"execution_count":65}]}