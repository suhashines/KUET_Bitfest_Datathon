{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport re\nfrom ast import literal_eval\nfrom datetime import datetime\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:07:29.769416Z","iopub.execute_input":"2024-12-27T09:07:29.769660Z","iopub.status.idle":"2024-12-27T09:07:34.080023Z","shell.execute_reply.started":"2024-12-27T09:07:29.769640Z","shell.execute_reply":"2024-12-27T09:07:34.079347Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"###############################################################################\n# TextProcessor\n###############################################################################","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self, max_features=300):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.nmf_models = {}\n        self.max_features = max_features\n    \n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', 'NUM', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    \n    def process_list(self, text):\n        \"\"\"Convert list-like strings to a list of cleaned tokens.\"\"\"\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        \n        # TF-IDF\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 3),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        \n        # Count\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features // 2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        \n        # SVD\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50, random_state=42)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        \n        # NMF\n        self.nmf_models[feature_name] = NMF(n_components=30, random_state=42)\n        nmf_matrix = self.nmf_models[feature_name].fit_transform(tfidf_matrix)\n        \n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [\n            ' '.join(self.process_list(text)) if isinstance(text, str) else ''\n            for text in texts\n        ]\n        \n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        nmf_matrix = self.nmf_models[feature_name].transform(tfidf_matrix)\n        \n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix,\n            nmf_matrix\n        ])\n\n###############################################################################\n# FeatureEngineer\n###############################################################################\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.scaler = StandardScaler()\n        self.power_transformer = PowerTransformer(method='yeo-johnson', standardize=False)  \n        # You can also try method='box-cox' if data > 0.\n\n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]\n            experiences = [e - s for s, e in zip(start_years, end_years)]\n            return {\n                'total_experience': sum(experiences),\n                'max_experience': max(experiences) if experiences else 0,\n                'num_positions': len(experiences)\n            }\n        except:\n            return {'total_experience': 0, 'max_experience': 0, 'num_positions': 0}\n    \n    def clean_education_result(self, result_str):\n        if pd.isna(result_str):\n            return 0\n        try:\n            # Handle list-like strings\n            if result_str.startswith('['):\n                result_str = literal_eval(result_str)[0]\n            \n            result_str = str(result_str).upper()\n            if result_str in ['N/A', 'NONE', 'NAN', '']:\n                return 0\n                \n            # Remove % and convert to float\n            result_str = result_str.replace('%', '')\n            return float(result_str)\n        except:\n            return 0\n    \n    def extract_education_features(self, row):\n        try:\n            degree = str(row['degree_names']).lower() if not pd.isna(row['degree_names']) else ''\n            result = self.clean_education_result(row['educational_results'])\n            \n            edu_score = 0\n            if 'phd' in degree or 'doctorate' in degree:\n                edu_score = 4\n            elif 'master' in degree:\n                edu_score = 3\n            elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n                edu_score = 2\n            elif 'diploma' in degree or 'certificate' in degree:\n                edu_score = 1\n                \n            return {\n                'education_score': edu_score,\n                'education_result': result,\n                'education_weight': edu_score * (result/100 if result > 0 else 1)\n            }\n        except:\n            return {\n                'education_score': 0,\n                'education_result': 0,\n                'education_weight': 0\n            }\n\n    def transform(self, df, is_train=True):\n        # Numeric features from experience and education\n        exp_features = df.apply(self.extract_years_experience, axis=1)\n        edu_features = df.apply(self.extract_education_features, axis=1)\n        \n        feature_dict = {}\n        for feat in ['total_experience', 'max_experience', 'num_positions']:\n            feature_dict[feat] = [x[feat] for x in exp_features]\n        for feat in ['education_score', 'education_result', 'education_weight']:\n            feature_dict[feat] = [x[feat] for x in edu_features]\n        \n        # Basic numeric count features\n        feature_dict['num_skills'] = df['skills'].fillna('').str.count(',') + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        feature_dict['num_languages'] = df['languages'].fillna('').str.count(',') + 1\n\n        # Additional *interaction* features to give the model more nuance\n        feature_dict['experience_per_position'] = np.array(feature_dict['total_experience']) / (\n            np.array(feature_dict['num_positions']) + 0.1\n        )\n        feature_dict['result_x_edu_score'] = (\n            np.array(feature_dict['education_result']) * np.array(feature_dict['education_score'])\n        )\n\n        # Text features\n        text_features = [\n            'skills', 'career_objective', 'responsibilities',\n            'educational_institution_name', 'certification_skills',\n            'major_field_of_studies'\n        ]\n        \n        all_text_features = {}\n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n            \n            # Keep dimension names stable\n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n        \n        # Skills match ratio\n        df['skills_required'] = df['skills_required'].fillna('')\n        df['skills'] = df['skills'].fillna('')\n        required_skills = df['skills_required'].apply(self.text_processor.process_list)\n        candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n        match_ratios = []\n        for req, cand in zip(required_skills, candidate_skills):\n            req_set = set(req)\n            cand_set = set(cand)\n            if len(req_set) == 0:\n                match_ratios.append(0.0)\n            else:\n                match_ratios.append(len(req_set.intersection(cand_set)) / len(req_set))\n        feature_dict['skills_match_ratio'] = match_ratios\n        \n        # Combine numeric features\n        numeric_df = pd.DataFrame(feature_dict, index=df.index)\n        \n        # Power transform or scale numeric features\n        # (You can also chain PowerTransformer -> StandardScaler, etc. to see if it helps)\n        if is_train:\n            numeric_arr = self.power_transformer.fit_transform(numeric_df)\n            numeric_arr = StandardScaler().fit_transform(numeric_arr)  # extra standard scaling\n        else:\n            numeric_arr = self.power_transformer.transform(numeric_df)\n            numeric_arr = StandardScaler().fit_transform(numeric_arr)\n        \n        numeric_df = pd.DataFrame(numeric_arr, columns=numeric_df.columns, index=numeric_df.index)\n        \n        # Combine with text features\n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n        \n        # Final combined feature matrix\n        return pd.concat([numeric_df, text_feature_df], axis=1)\n\n###############################################################################\n# Training and Ensembling\n###############################################################################\ndef train_and_ensemble():\n    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n    fe = FeatureEngineer()\n\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n\n    y = train_df['matched_score'].values\n\n    # We will do a small manual hyperparam search for LightGBM\n    # (Replace with Optuna or Hyperopt for a more thorough search)\n    lgb_params_list = [\n        {\n            'objective': 'regression_l2',\n            'metric': 'l2',\n            'num_leaves': 31,\n            'learning_rate': 0.005,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'reg_alpha': 0.1,\n            'reg_lambda': 0.1,\n            'min_child_samples': 20,\n            'force_col_wise': True,\n            'seed': 42\n        },\n        {\n            'objective': 'regression_l2',\n            'metric': 'l2',\n            'num_leaves': 64,\n            'learning_rate': 0.003,\n            'feature_fraction': 0.9,\n            'bagging_fraction': 0.9,\n            'bagging_freq': 3,\n            'reg_alpha': 0.2,\n            'reg_lambda': 0.2,\n            'min_child_samples': 25,\n            'force_col_wise': True,\n            'max_bin': 255,\n            'seed': 42\n        }\n    ]\n\n    # XGBoost parameters\n    xgb_params = {\n        'objective': 'reg:squarederror',\n        'learning_rate': 0.005,\n        'max_depth': 6,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'lambda': 2,\n        'alpha': 1,\n        'seed': 42\n    }\n\n    # CatBoost parameters\n    cat_params = {\n        'iterations': 3000,\n        'learning_rate': 0.006,\n        'depth': 6,\n        'eval_metric': 'MSE',\n        'loss_function': 'MSE',\n        'random_seed': 42,\n        'use_best_model': True,\n        'verbose': 100\n    }\n\n    # Weâ€™ll do repeated cross validation to get more robust estimates\n    rkf = RepeatedKFold(n_splits=5, n_repeats=2, random_state=42)  # total 10 folds\n\n    # We'll store out-of-fold predictions for each model\n    oof_preds_lgb_list = []\n    oof_preds_xgb = np.zeros(len(train_df))\n    oof_preds_cat = np.zeros(len(train_df))\n\n    # We'll also keep an array for each test prediction to average them later\n    test_preds_lgb_list = []\n    test_preds_xgb = np.zeros(len(test_df))\n    test_preds_cat = np.zeros(len(test_df))\n\n    # Loop through different sets of LightGBM params\n    for lgb_params in lgb_params_list:\n        oof_preds_lgb = np.zeros(len(train_df))\n        test_preds_lgb = np.zeros(len(test_df))\n        \n        # Cross-validation loop\n        for fold, (train_idx, val_idx) in enumerate(rkf.split(train_features)):\n            X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            \n            train_data_lgb = lgb.Dataset(X_train, label=y_train)\n            val_data_lgb = lgb.Dataset(X_val, label=y_val)\n\n            model_lgb = lgb.train(\n                lgb_params,\n                train_data_lgb,\n                num_boost_round=5000,\n                valid_sets=[train_data_lgb, val_data_lgb],\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=100),\n                    lgb.log_evaluation(100)\n                ]\n            )\n\n            oof_preds_lgb[val_idx] = model_lgb.predict(X_val)\n            test_preds_lgb += model_lgb.predict(test_features) / rkf.get_n_splits()\n\n        oof_preds_lgb_list.append(oof_preds_lgb)\n        test_preds_lgb_list.append(test_preds_lgb)\n\n    # Train a single XGBoost model with the same folds\n    for fold, (train_idx, val_idx) in enumerate(rkf.split(train_features)):\n        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        xgb_train = xgb.DMatrix(X_train, y_train)\n        xgb_val = xgb.DMatrix(X_val, y_val)\n        xgb_test = xgb.DMatrix(test_features)\n\n        model_xgb = xgb.train(\n            xgb_params,\n            xgb_train,\n            num_boost_round=5000,\n            evals=[(xgb_val, 'val')],\n            early_stopping_rounds=100,\n            verbose_eval=False\n        )\n\n        oof_preds_xgb[val_idx] = model_xgb.predict(xgb_val)\n        test_preds_xgb += model_xgb.predict(xgb_test) / rkf.get_n_splits()\n\n    # Train a single CatBoost model with the same folds\n    for fold, (train_idx, val_idx) in enumerate(rkf.split(train_features)):\n        X_train, X_val = train_features.iloc[train_idx], train_features.iloc[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        model_cat = cb.CatBoostRegressor(**cat_params)\n        model_cat.fit(\n            X_train, y_train,\n            eval_set=(X_val, y_val),\n            use_best_model=True,\n            verbose=False  # set to True if you want to see the CatBoost logs\n        )\n        oof_preds_cat[val_idx] = model_cat.predict(X_val)\n        test_preds_cat += model_cat.predict(test_features) / rkf.get_n_splits()\n\n    # Evaluate each set of oof predictions for LGB\n    for i, oof_lgb in enumerate(oof_preds_lgb_list):\n        score = mean_squared_error(y, oof_lgb)\n        print(f\"LGB Model {i+1} CV MSE: {score:.6f}\")\n\n    # Evaluate XGB\n    xgb_score = mean_squared_error(y, oof_preds_xgb)\n    print(f\"XGB CV MSE: {xgb_score:.6f}\")\n\n    # Evaluate CatBoost\n    cat_score = mean_squared_error(y, oof_preds_cat)\n    print(f\"CATBoost CV MSE: {cat_score:.6f}\")\n\n    # Simple ensemble #1:\n    # Average all LightGBM versions + XGB + Cat\n    # If you had 2 LGB sets of oof predictions, you can average them\n    avg_lgb = np.mean(np.column_stack(oof_preds_lgb_list), axis=1)\n    oof_ensemble_1 = (avg_lgb + oof_preds_xgb + oof_preds_cat) / 3.0\n    ensemble_1_score = mean_squared_error(y, oof_ensemble_1)\n    print(f\"Ensemble #1 (mean of LGBs + XGB + CAT) CV MSE: {ensemble_1_score:.6f}\")\n\n    # Predict on the test set with Ensemble #1\n    avg_lgb_test = np.mean(np.column_stack(test_preds_lgb_list), axis=1)\n    test_preds_ensemble_1 = (avg_lgb_test + test_preds_xgb + test_preds_cat) / 3.0\n\n    # You can also experiment with different weighting strategies\n    # For instance: if LGB is strong, do something like\n    # ensemble_pred = 0.5*avg_lgb + 0.25*xgb + 0.25*cat\n    # Or pick weights based on CV performance of each model.\n    \n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_preds_ensemble_1\n    })\n    submission.to_csv('submission_ensemble.csv', index=False)\n    print(\"Ensemble submission saved to submission_ensemble.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:13:08.826798Z","iopub.execute_input":"2024-12-27T09:13:08.827083Z","iopub.status.idle":"2024-12-27T09:13:08.858395Z","shell.execute_reply.started":"2024-12-27T09:13:08.827061Z","shell.execute_reply":"2024-12-27T09:13:08.857651Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_and_ensemble()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-27T09:13:13.220435Z","iopub.execute_input":"2024-12-27T09:13:13.220854Z","iopub.status.idle":"2024-12-27T11:32:23.602356Z","shell.execute_reply.started":"2024-12-27T09:13:13.220823Z","shell.execute_reply":"2024-12-27T11:32:23.601585Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-eb01e682e713>:10: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  if pd.isna(text):\n","output_type":"stream"},{"name":"stdout","text":"Transforming test data...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-eb01e682e713>:10: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n  if pd.isna(text):\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Total Bins 66483\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2631\n[LightGBM] [Info] Start training from score 0.658422\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0189821\tvalid_1's l2: 0.0190193\n[200]\ttraining's l2: 0.0145127\tvalid_1's l2: 0.0157731\n[300]\ttraining's l2: 0.0120032\tvalid_1's l2: 0.0140275\n[400]\ttraining's l2: 0.0103249\tvalid_1's l2: 0.0129199\n[500]\ttraining's l2: 0.0091082\tvalid_1's l2: 0.0120948\n[600]\ttraining's l2: 0.00819705\tvalid_1's l2: 0.0114649\n[700]\ttraining's l2: 0.00752525\tvalid_1's l2: 0.0110112\n[800]\ttraining's l2: 0.00698283\tvalid_1's l2: 0.0106508\n[900]\ttraining's l2: 0.00655994\tvalid_1's l2: 0.0103949\n[1000]\ttraining's l2: 0.00618909\tvalid_1's l2: 0.0101748\n[1100]\ttraining's l2: 0.00588546\tvalid_1's l2: 0.0100136\n[1200]\ttraining's l2: 0.00561879\tvalid_1's l2: 0.00987969\n[1300]\ttraining's l2: 0.00537029\tvalid_1's l2: 0.00974495\n[1400]\ttraining's l2: 0.00514901\tvalid_1's l2: 0.00964071\n[1500]\ttraining's l2: 0.00493812\tvalid_1's l2: 0.00953116\n[1600]\ttraining's l2: 0.00474306\tvalid_1's l2: 0.00944371\n[1700]\ttraining's l2: 0.00456503\tvalid_1's l2: 0.00936975\n[1800]\ttraining's l2: 0.00439313\tvalid_1's l2: 0.00929048\n[1900]\ttraining's l2: 0.00422079\tvalid_1's l2: 0.0092238\n[2000]\ttraining's l2: 0.00408056\tvalid_1's l2: 0.0091797\n[2100]\ttraining's l2: 0.00393929\tvalid_1's l2: 0.00913263\n[2200]\ttraining's l2: 0.00380847\tvalid_1's l2: 0.00908707\n[2300]\ttraining's l2: 0.00368118\tvalid_1's l2: 0.00904243\n[2400]\ttraining's l2: 0.00355577\tvalid_1's l2: 0.00899948\n[2500]\ttraining's l2: 0.003439\tvalid_1's l2: 0.00896039\n[2600]\ttraining's l2: 0.00332937\tvalid_1's l2: 0.00893385\n[2700]\ttraining's l2: 0.00322617\tvalid_1's l2: 0.00890649\n[2800]\ttraining's l2: 0.00312294\tvalid_1's l2: 0.00887439\n[2900]\ttraining's l2: 0.00302492\tvalid_1's l2: 0.00885307\n[3000]\ttraining's l2: 0.00292955\tvalid_1's l2: 0.00882826\n[3100]\ttraining's l2: 0.00284517\tvalid_1's l2: 0.00881273\n[3200]\ttraining's l2: 0.00275666\tvalid_1's l2: 0.00879496\n[3300]\ttraining's l2: 0.00267338\tvalid_1's l2: 0.00876806\n[3400]\ttraining's l2: 0.00259148\tvalid_1's l2: 0.00874543\n[3500]\ttraining's l2: 0.00251403\tvalid_1's l2: 0.00872985\n[3600]\ttraining's l2: 0.00243722\tvalid_1's l2: 0.00871043\n[3700]\ttraining's l2: 0.0023631\tvalid_1's l2: 0.00868845\n[3800]\ttraining's l2: 0.00229704\tvalid_1's l2: 0.00868121\n[3900]\ttraining's l2: 0.00222983\tvalid_1's l2: 0.00867537\n[4000]\ttraining's l2: 0.00216647\tvalid_1's l2: 0.00866211\n[4100]\ttraining's l2: 0.00210413\tvalid_1's l2: 0.0086455\n[4200]\ttraining's l2: 0.00204246\tvalid_1's l2: 0.00863278\n[4300]\ttraining's l2: 0.00198429\tvalid_1's l2: 0.00862135\n[4400]\ttraining's l2: 0.00192599\tvalid_1's l2: 0.008611\n[4500]\ttraining's l2: 0.00187125\tvalid_1's l2: 0.00859544\n[4600]\ttraining's l2: 0.00182068\tvalid_1's l2: 0.00858688\n[4700]\ttraining's l2: 0.0017719\tvalid_1's l2: 0.00857797\n[4800]\ttraining's l2: 0.0017215\tvalid_1's l2: 0.00857518\n[4900]\ttraining's l2: 0.00167364\tvalid_1's l2: 0.00856843\n[5000]\ttraining's l2: 0.00162762\tvalid_1's l2: 0.00855821\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00162762\tvalid_1's l2: 0.00855821\n[LightGBM] [Info] Total Bins 66672\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2729\n[LightGBM] [Info] Start training from score 0.661442\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0189647\tvalid_1's l2: 0.0199477\n[200]\ttraining's l2: 0.0145892\tvalid_1's l2: 0.0159907\n[300]\ttraining's l2: 0.0120621\tvalid_1's l2: 0.0138864\n[400]\ttraining's l2: 0.0103315\tvalid_1's l2: 0.012529\n[500]\ttraining's l2: 0.00910811\tvalid_1's l2: 0.0116697\n[600]\ttraining's l2: 0.00818576\tvalid_1's l2: 0.0109983\n[700]\ttraining's l2: 0.00749931\tvalid_1's l2: 0.010532\n[800]\ttraining's l2: 0.00696033\tvalid_1's l2: 0.0101817\n[900]\ttraining's l2: 0.00651241\tvalid_1's l2: 0.00992892\n[1000]\ttraining's l2: 0.00615728\tvalid_1's l2: 0.00973883\n[1100]\ttraining's l2: 0.00585379\tvalid_1's l2: 0.00959111\n[1200]\ttraining's l2: 0.00555364\tvalid_1's l2: 0.00942465\n[1300]\ttraining's l2: 0.00531331\tvalid_1's l2: 0.00931108\n[1400]\ttraining's l2: 0.00509333\tvalid_1's l2: 0.00921493\n[1500]\ttraining's l2: 0.00489595\tvalid_1's l2: 0.00915547\n[1600]\ttraining's l2: 0.00470265\tvalid_1's l2: 0.00908236\n[1700]\ttraining's l2: 0.00453221\tvalid_1's l2: 0.00902049\n[1800]\ttraining's l2: 0.00436769\tvalid_1's l2: 0.00896705\n[1900]\ttraining's l2: 0.00420573\tvalid_1's l2: 0.00891581\n[2000]\ttraining's l2: 0.00405601\tvalid_1's l2: 0.00887572\n[2100]\ttraining's l2: 0.00391242\tvalid_1's l2: 0.00882901\n[2200]\ttraining's l2: 0.00377859\tvalid_1's l2: 0.00878929\n[2300]\ttraining's l2: 0.00364684\tvalid_1's l2: 0.00875402\n[2400]\ttraining's l2: 0.00352443\tvalid_1's l2: 0.00872155\n[2500]\ttraining's l2: 0.00340636\tvalid_1's l2: 0.00869811\n[2600]\ttraining's l2: 0.00329763\tvalid_1's l2: 0.00867516\n[2700]\ttraining's l2: 0.00319462\tvalid_1's l2: 0.00864717\n[2800]\ttraining's l2: 0.00309648\tvalid_1's l2: 0.00863038\n[2900]\ttraining's l2: 0.00299892\tvalid_1's l2: 0.00860081\n[3000]\ttraining's l2: 0.00290792\tvalid_1's l2: 0.00857848\n[3100]\ttraining's l2: 0.00281481\tvalid_1's l2: 0.00856084\n[3200]\ttraining's l2: 0.00272977\tvalid_1's l2: 0.00853999\n[3300]\ttraining's l2: 0.00264763\tvalid_1's l2: 0.00852585\n[3400]\ttraining's l2: 0.00256578\tvalid_1's l2: 0.00851275\n[3500]\ttraining's l2: 0.00248601\tvalid_1's l2: 0.00849849\n[3600]\ttraining's l2: 0.00241194\tvalid_1's l2: 0.00848783\n[3700]\ttraining's l2: 0.00234277\tvalid_1's l2: 0.00847761\n[3800]\ttraining's l2: 0.0022706\tvalid_1's l2: 0.00847032\n[3900]\ttraining's l2: 0.00220403\tvalid_1's l2: 0.0084591\n[4000]\ttraining's l2: 0.00214143\tvalid_1's l2: 0.00845099\n[4100]\ttraining's l2: 0.0020796\tvalid_1's l2: 0.00844473\n[4200]\ttraining's l2: 0.0020204\tvalid_1's l2: 0.00843801\n[4300]\ttraining's l2: 0.00196255\tvalid_1's l2: 0.00843119\n[4400]\ttraining's l2: 0.00190939\tvalid_1's l2: 0.00842096\n[4500]\ttraining's l2: 0.0018553\tvalid_1's l2: 0.00842125\n[4600]\ttraining's l2: 0.00180532\tvalid_1's l2: 0.00841332\n[4700]\ttraining's l2: 0.00175463\tvalid_1's l2: 0.00840467\n[4800]\ttraining's l2: 0.0017073\tvalid_1's l2: 0.00840144\n[4900]\ttraining's l2: 0.00166018\tvalid_1's l2: 0.00839265\n[5000]\ttraining's l2: 0.00161442\tvalid_1's l2: 0.00838613\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00161442\tvalid_1's l2: 0.00838613\n[LightGBM] [Info] Total Bins 66577\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2670\n[LightGBM] [Info] Start training from score 0.661377\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.019056\tvalid_1's l2: 0.0195195\n[200]\ttraining's l2: 0.0147196\tvalid_1's l2: 0.0156744\n[300]\ttraining's l2: 0.0121566\tvalid_1's l2: 0.013558\n[400]\ttraining's l2: 0.0104899\tvalid_1's l2: 0.012292\n[500]\ttraining's l2: 0.00922003\tvalid_1's l2: 0.01132\n[600]\ttraining's l2: 0.00831999\tvalid_1's l2: 0.0106449\n[700]\ttraining's l2: 0.0076101\tvalid_1's l2: 0.0101348\n[800]\ttraining's l2: 0.00706785\tvalid_1's l2: 0.00978433\n[900]\ttraining's l2: 0.00661861\tvalid_1's l2: 0.00951338\n[1000]\ttraining's l2: 0.00625742\tvalid_1's l2: 0.00930427\n[1100]\ttraining's l2: 0.00594332\tvalid_1's l2: 0.00914891\n[1200]\ttraining's l2: 0.00566477\tvalid_1's l2: 0.00901318\n[1300]\ttraining's l2: 0.00541895\tvalid_1's l2: 0.00890711\n[1400]\ttraining's l2: 0.00520062\tvalid_1's l2: 0.00881642\n[1500]\ttraining's l2: 0.00498953\tvalid_1's l2: 0.0087383\n[1600]\ttraining's l2: 0.00479625\tvalid_1's l2: 0.00866285\n[1700]\ttraining's l2: 0.00460987\tvalid_1's l2: 0.00860257\n[1800]\ttraining's l2: 0.00444291\tvalid_1's l2: 0.00855132\n[1900]\ttraining's l2: 0.00428719\tvalid_1's l2: 0.00850349\n[2000]\ttraining's l2: 0.00413594\tvalid_1's l2: 0.00846341\n[2100]\ttraining's l2: 0.00398781\tvalid_1's l2: 0.00841217\n[2200]\ttraining's l2: 0.00384756\tvalid_1's l2: 0.00837391\n[2300]\ttraining's l2: 0.00371536\tvalid_1's l2: 0.00833355\n[2400]\ttraining's l2: 0.00358854\tvalid_1's l2: 0.00829935\n[2500]\ttraining's l2: 0.00346973\tvalid_1's l2: 0.00827823\n[2600]\ttraining's l2: 0.0033545\tvalid_1's l2: 0.00824589\n[2700]\ttraining's l2: 0.00324844\tvalid_1's l2: 0.00823155\n[2800]\ttraining's l2: 0.00314326\tvalid_1's l2: 0.00821226\n[2900]\ttraining's l2: 0.00304477\tvalid_1's l2: 0.0081919\n[3000]\ttraining's l2: 0.00294922\tvalid_1's l2: 0.00816992\n[3100]\ttraining's l2: 0.00285491\tvalid_1's l2: 0.00815335\n[3200]\ttraining's l2: 0.00276719\tvalid_1's l2: 0.00813991\n[3300]\ttraining's l2: 0.00268403\tvalid_1's l2: 0.00811946\n[3400]\ttraining's l2: 0.002603\tvalid_1's l2: 0.00810554\n[3500]\ttraining's l2: 0.00252305\tvalid_1's l2: 0.00808644\n[3600]\ttraining's l2: 0.00244879\tvalid_1's l2: 0.00806942\n[3700]\ttraining's l2: 0.00237806\tvalid_1's l2: 0.00806241\n[3800]\ttraining's l2: 0.00230836\tvalid_1's l2: 0.00803658\n[3900]\ttraining's l2: 0.00223838\tvalid_1's l2: 0.00802949\n[4000]\ttraining's l2: 0.0021734\tvalid_1's l2: 0.00802208\n[4100]\ttraining's l2: 0.00211167\tvalid_1's l2: 0.00801405\n[4200]\ttraining's l2: 0.00205109\tvalid_1's l2: 0.00800134\n[4300]\ttraining's l2: 0.00199305\tvalid_1's l2: 0.00799546\n[4400]\ttraining's l2: 0.00193728\tvalid_1's l2: 0.0079886\n[4500]\ttraining's l2: 0.00188264\tvalid_1's l2: 0.0079818\n[4600]\ttraining's l2: 0.00183077\tvalid_1's l2: 0.0079752\n[4700]\ttraining's l2: 0.00178107\tvalid_1's l2: 0.00796933\n[4800]\ttraining's l2: 0.00173253\tvalid_1's l2: 0.00796587\n[4900]\ttraining's l2: 0.00168449\tvalid_1's l2: 0.00795471\n[5000]\ttraining's l2: 0.00163763\tvalid_1's l2: 0.00795075\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00163763\tvalid_1's l2: 0.00795075\n[LightGBM] [Info] Total Bins 66530\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2671\n[LightGBM] [Info] Start training from score 0.661045\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0187484\tvalid_1's l2: 0.0201994\n[200]\ttraining's l2: 0.0144372\tvalid_1's l2: 0.0163781\n[300]\ttraining's l2: 0.0119434\tvalid_1's l2: 0.0142667\n[400]\ttraining's l2: 0.0102868\tvalid_1's l2: 0.0129321\n[500]\ttraining's l2: 0.00907512\tvalid_1's l2: 0.0119528\n[600]\ttraining's l2: 0.0081851\tvalid_1's l2: 0.0112424\n[700]\ttraining's l2: 0.00748085\tvalid_1's l2: 0.0107032\n[800]\ttraining's l2: 0.00693558\tvalid_1's l2: 0.010324\n[900]\ttraining's l2: 0.00648672\tvalid_1's l2: 0.0100446\n[1000]\ttraining's l2: 0.00611112\tvalid_1's l2: 0.00981402\n[1100]\ttraining's l2: 0.00580464\tvalid_1's l2: 0.00965898\n[1200]\ttraining's l2: 0.00553396\tvalid_1's l2: 0.0095217\n[1300]\ttraining's l2: 0.00529114\tvalid_1's l2: 0.0094063\n[1400]\ttraining's l2: 0.00507146\tvalid_1's l2: 0.00930797\n[1500]\ttraining's l2: 0.00486348\tvalid_1's l2: 0.00921526\n[1600]\ttraining's l2: 0.00467091\tvalid_1's l2: 0.00914792\n[1700]\ttraining's l2: 0.00449262\tvalid_1's l2: 0.00908189\n[1800]\ttraining's l2: 0.00432776\tvalid_1's l2: 0.00901367\n[1900]\ttraining's l2: 0.0041701\tvalid_1's l2: 0.00896023\n[2000]\ttraining's l2: 0.00401966\tvalid_1's l2: 0.00891363\n[2100]\ttraining's l2: 0.00388487\tvalid_1's l2: 0.00887372\n[2200]\ttraining's l2: 0.00375605\tvalid_1's l2: 0.00884042\n[2300]\ttraining's l2: 0.0036349\tvalid_1's l2: 0.008803\n[2400]\ttraining's l2: 0.00351878\tvalid_1's l2: 0.00876702\n[2500]\ttraining's l2: 0.00339802\tvalid_1's l2: 0.0087326\n[2600]\ttraining's l2: 0.00328762\tvalid_1's l2: 0.00870126\n[2700]\ttraining's l2: 0.00318017\tvalid_1's l2: 0.00867312\n[2800]\ttraining's l2: 0.00307934\tvalid_1's l2: 0.00864798\n[2900]\ttraining's l2: 0.00298252\tvalid_1's l2: 0.00863054\n[3000]\ttraining's l2: 0.00289128\tvalid_1's l2: 0.0086214\n[3100]\ttraining's l2: 0.00280191\tvalid_1's l2: 0.00861284\n[3200]\ttraining's l2: 0.00271825\tvalid_1's l2: 0.00859388\n[3300]\ttraining's l2: 0.00263472\tvalid_1's l2: 0.00858668\n[3400]\ttraining's l2: 0.00255224\tvalid_1's l2: 0.00856756\n[3500]\ttraining's l2: 0.00247671\tvalid_1's l2: 0.00855638\n[3600]\ttraining's l2: 0.00240138\tvalid_1's l2: 0.00854367\n[3700]\ttraining's l2: 0.00232887\tvalid_1's l2: 0.00853402\n[3800]\ttraining's l2: 0.00226381\tvalid_1's l2: 0.00852617\n[3900]\ttraining's l2: 0.00219584\tvalid_1's l2: 0.00850958\n[4000]\ttraining's l2: 0.0021284\tvalid_1's l2: 0.00849894\n[4100]\ttraining's l2: 0.00206824\tvalid_1's l2: 0.00848699\n[4200]\ttraining's l2: 0.00200977\tvalid_1's l2: 0.00847629\n[4300]\ttraining's l2: 0.0019535\tvalid_1's l2: 0.00846624\n[4400]\ttraining's l2: 0.00189976\tvalid_1's l2: 0.00845521\n[4500]\ttraining's l2: 0.00184688\tvalid_1's l2: 0.00844645\n[4600]\ttraining's l2: 0.0017948\tvalid_1's l2: 0.00843209\n[4700]\ttraining's l2: 0.00174391\tvalid_1's l2: 0.00842477\n[4800]\ttraining's l2: 0.00169978\tvalid_1's l2: 0.00841853\n[4900]\ttraining's l2: 0.00165259\tvalid_1's l2: 0.00840859\n[5000]\ttraining's l2: 0.00160938\tvalid_1's l2: 0.00840431\nDid not meet early stopping. Best iteration is:\n[4996]\ttraining's l2: 0.0016109\tvalid_1's l2: 0.00840383\n[LightGBM] [Info] Total Bins 66835\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2801\n[LightGBM] [Info] Start training from score 0.661050\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0189664\tvalid_1's l2: 0.0195339\n[200]\ttraining's l2: 0.0146418\tvalid_1's l2: 0.0155773\n[300]\ttraining's l2: 0.012142\tvalid_1's l2: 0.013453\n[400]\ttraining's l2: 0.0104584\tvalid_1's l2: 0.012117\n[500]\ttraining's l2: 0.0092556\tvalid_1's l2: 0.0112227\n[600]\ttraining's l2: 0.00830917\tvalid_1's l2: 0.0104888\n[700]\ttraining's l2: 0.00761365\tvalid_1's l2: 0.00998738\n[800]\ttraining's l2: 0.00708475\tvalid_1's l2: 0.00964675\n[900]\ttraining's l2: 0.00664049\tvalid_1's l2: 0.00938194\n[1000]\ttraining's l2: 0.00626059\tvalid_1's l2: 0.00918089\n[1100]\ttraining's l2: 0.00594278\tvalid_1's l2: 0.00902541\n[1200]\ttraining's l2: 0.00566394\tvalid_1's l2: 0.0088945\n[1300]\ttraining's l2: 0.00540746\tvalid_1's l2: 0.00879433\n[1400]\ttraining's l2: 0.00517189\tvalid_1's l2: 0.00870623\n[1500]\ttraining's l2: 0.00497377\tvalid_1's l2: 0.00863313\n[1600]\ttraining's l2: 0.00477542\tvalid_1's l2: 0.00854815\n[1700]\ttraining's l2: 0.00460351\tvalid_1's l2: 0.00849671\n[1800]\ttraining's l2: 0.00444143\tvalid_1's l2: 0.00844991\n[1900]\ttraining's l2: 0.00427683\tvalid_1's l2: 0.0083965\n[2000]\ttraining's l2: 0.00412379\tvalid_1's l2: 0.0083458\n[2100]\ttraining's l2: 0.00398502\tvalid_1's l2: 0.00831392\n[2200]\ttraining's l2: 0.00384665\tvalid_1's l2: 0.00828799\n[2300]\ttraining's l2: 0.00371596\tvalid_1's l2: 0.00825383\n[2400]\ttraining's l2: 0.00359213\tvalid_1's l2: 0.00822761\n[2500]\ttraining's l2: 0.00347764\tvalid_1's l2: 0.00820134\n[2600]\ttraining's l2: 0.00336156\tvalid_1's l2: 0.00817844\n[2700]\ttraining's l2: 0.00325322\tvalid_1's l2: 0.00815297\n[2800]\ttraining's l2: 0.00315189\tvalid_1's l2: 0.00813271\n[2900]\ttraining's l2: 0.00305361\tvalid_1's l2: 0.00810821\n[3000]\ttraining's l2: 0.00295669\tvalid_1's l2: 0.00808734\n[3100]\ttraining's l2: 0.0028661\tvalid_1's l2: 0.00807444\n[3200]\ttraining's l2: 0.00277585\tvalid_1's l2: 0.00806198\n[3300]\ttraining's l2: 0.00269115\tvalid_1's l2: 0.00804545\n[3400]\ttraining's l2: 0.00261151\tvalid_1's l2: 0.00803591\n[3500]\ttraining's l2: 0.00253247\tvalid_1's l2: 0.00802347\n[3600]\ttraining's l2: 0.00245276\tvalid_1's l2: 0.00801653\n[3700]\ttraining's l2: 0.00238059\tvalid_1's l2: 0.00800282\n[3800]\ttraining's l2: 0.00231049\tvalid_1's l2: 0.0079986\n[3900]\ttraining's l2: 0.00224428\tvalid_1's l2: 0.007992\n[4000]\ttraining's l2: 0.00217754\tvalid_1's l2: 0.0079784\n[4100]\ttraining's l2: 0.00211899\tvalid_1's l2: 0.00797195\n[4200]\ttraining's l2: 0.00206409\tvalid_1's l2: 0.00796506\n[4300]\ttraining's l2: 0.00200442\tvalid_1's l2: 0.00795726\n[4400]\ttraining's l2: 0.00194984\tvalid_1's l2: 0.00794723\n[4500]\ttraining's l2: 0.00189546\tvalid_1's l2: 0.00794221\n[4600]\ttraining's l2: 0.00184105\tvalid_1's l2: 0.0079396\n[4700]\ttraining's l2: 0.00178969\tvalid_1's l2: 0.0079373\n[4800]\ttraining's l2: 0.00173889\tvalid_1's l2: 0.00793382\n[4900]\ttraining's l2: 0.00169154\tvalid_1's l2: 0.0079287\n[5000]\ttraining's l2: 0.00164484\tvalid_1's l2: 0.00791822\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00164484\tvalid_1's l2: 0.00791822\n[LightGBM] [Info] Total Bins 66642\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2724\n[LightGBM] [Info] Start training from score 0.661659\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0188855\tvalid_1's l2: 0.0198277\n[200]\ttraining's l2: 0.0145092\tvalid_1's l2: 0.0159634\n[300]\ttraining's l2: 0.0119816\tvalid_1's l2: 0.0140064\n[400]\ttraining's l2: 0.0102657\tvalid_1's l2: 0.0127641\n[500]\ttraining's l2: 0.0090343\tvalid_1's l2: 0.0119093\n[600]\ttraining's l2: 0.00813706\tvalid_1's l2: 0.0112589\n[700]\ttraining's l2: 0.00745714\tvalid_1's l2: 0.0108222\n[800]\ttraining's l2: 0.00689519\tvalid_1's l2: 0.0104542\n[900]\ttraining's l2: 0.00645322\tvalid_1's l2: 0.0101824\n[1000]\ttraining's l2: 0.00609428\tvalid_1's l2: 0.00998871\n[1100]\ttraining's l2: 0.00578725\tvalid_1's l2: 0.00983513\n[1200]\ttraining's l2: 0.00552277\tvalid_1's l2: 0.00970514\n[1300]\ttraining's l2: 0.0052759\tvalid_1's l2: 0.00959775\n[1400]\ttraining's l2: 0.0050587\tvalid_1's l2: 0.00951017\n[1500]\ttraining's l2: 0.00485978\tvalid_1's l2: 0.0094349\n[1600]\ttraining's l2: 0.00466209\tvalid_1's l2: 0.00935895\n[1700]\ttraining's l2: 0.00448414\tvalid_1's l2: 0.0092972\n[1800]\ttraining's l2: 0.00430961\tvalid_1's l2: 0.00924164\n[1900]\ttraining's l2: 0.00414788\tvalid_1's l2: 0.0092006\n[2000]\ttraining's l2: 0.00400981\tvalid_1's l2: 0.0091603\n[2100]\ttraining's l2: 0.00387094\tvalid_1's l2: 0.00912875\n[2200]\ttraining's l2: 0.00373976\tvalid_1's l2: 0.00911803\n[2300]\ttraining's l2: 0.0036077\tvalid_1's l2: 0.00908865\n[2400]\ttraining's l2: 0.0034854\tvalid_1's l2: 0.00906269\n[2500]\ttraining's l2: 0.00337137\tvalid_1's l2: 0.00903735\n[2600]\ttraining's l2: 0.00325994\tvalid_1's l2: 0.0090082\n[2700]\ttraining's l2: 0.00315479\tvalid_1's l2: 0.00898363\n[2800]\ttraining's l2: 0.00305737\tvalid_1's l2: 0.00896458\n[2900]\ttraining's l2: 0.00296434\tvalid_1's l2: 0.008945\n[3000]\ttraining's l2: 0.00287059\tvalid_1's l2: 0.00893926\n[3100]\ttraining's l2: 0.00277845\tvalid_1's l2: 0.00892196\n[3200]\ttraining's l2: 0.00269116\tvalid_1's l2: 0.00890528\n[3300]\ttraining's l2: 0.00261272\tvalid_1's l2: 0.00889535\n[3400]\ttraining's l2: 0.00253232\tvalid_1's l2: 0.00888312\n[3500]\ttraining's l2: 0.00245754\tvalid_1's l2: 0.00887121\n[3600]\ttraining's l2: 0.00238247\tvalid_1's l2: 0.00886051\n[3700]\ttraining's l2: 0.00231326\tvalid_1's l2: 0.00885397\n[3800]\ttraining's l2: 0.00224671\tvalid_1's l2: 0.00885127\n[3900]\ttraining's l2: 0.00217899\tvalid_1's l2: 0.00884014\n[4000]\ttraining's l2: 0.0021128\tvalid_1's l2: 0.00882875\n[4100]\ttraining's l2: 0.00205426\tvalid_1's l2: 0.00882302\n[4200]\ttraining's l2: 0.00199553\tvalid_1's l2: 0.00881387\n[4300]\ttraining's l2: 0.00193846\tvalid_1's l2: 0.00881241\nEarly stopping, best iteration is:\n[4249]\ttraining's l2: 0.00196693\tvalid_1's l2: 0.00881067\n[LightGBM] [Info] Total Bins 66564\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2759\n[LightGBM] [Info] Start training from score 0.661229\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0189063\tvalid_1's l2: 0.0195354\n[200]\ttraining's l2: 0.014524\tvalid_1's l2: 0.0157911\n[300]\ttraining's l2: 0.012047\tvalid_1's l2: 0.0138491\n[400]\ttraining's l2: 0.0103656\tvalid_1's l2: 0.0125909\n[500]\ttraining's l2: 0.00912891\tvalid_1's l2: 0.0116594\n[600]\ttraining's l2: 0.00822537\tvalid_1's l2: 0.0110259\n[700]\ttraining's l2: 0.00754099\tvalid_1's l2: 0.0105956\n[800]\ttraining's l2: 0.00698788\tvalid_1's l2: 0.0102562\n[900]\ttraining's l2: 0.00655115\tvalid_1's l2: 0.0100269\n[1000]\ttraining's l2: 0.00619151\tvalid_1's l2: 0.0098597\n[1100]\ttraining's l2: 0.0058664\tvalid_1's l2: 0.00968792\n[1200]\ttraining's l2: 0.00559767\tvalid_1's l2: 0.0095682\n[1300]\ttraining's l2: 0.00534605\tvalid_1's l2: 0.00947858\n[1400]\ttraining's l2: 0.00511521\tvalid_1's l2: 0.0093915\n[1500]\ttraining's l2: 0.0049121\tvalid_1's l2: 0.00933069\n[1600]\ttraining's l2: 0.00471927\tvalid_1's l2: 0.00926526\n[1700]\ttraining's l2: 0.00454241\tvalid_1's l2: 0.00920511\n[1800]\ttraining's l2: 0.00437986\tvalid_1's l2: 0.00915562\n[1900]\ttraining's l2: 0.00421908\tvalid_1's l2: 0.00910583\n[2000]\ttraining's l2: 0.00406673\tvalid_1's l2: 0.00906661\n[2100]\ttraining's l2: 0.00391769\tvalid_1's l2: 0.00901563\n[2200]\ttraining's l2: 0.00378177\tvalid_1's l2: 0.00898022\n[2300]\ttraining's l2: 0.00365207\tvalid_1's l2: 0.00894196\n[2400]\ttraining's l2: 0.00352921\tvalid_1's l2: 0.00892151\n[2500]\ttraining's l2: 0.0034164\tvalid_1's l2: 0.00889024\n[2600]\ttraining's l2: 0.00330215\tvalid_1's l2: 0.0088557\n[2700]\ttraining's l2: 0.00319562\tvalid_1's l2: 0.00883281\n[2800]\ttraining's l2: 0.0030981\tvalid_1's l2: 0.0088081\n[2900]\ttraining's l2: 0.00299784\tvalid_1's l2: 0.00878563\n[3000]\ttraining's l2: 0.00290882\tvalid_1's l2: 0.00877308\n[3100]\ttraining's l2: 0.00281903\tvalid_1's l2: 0.00875925\n[3200]\ttraining's l2: 0.00273264\tvalid_1's l2: 0.00874293\n[3300]\ttraining's l2: 0.00265145\tvalid_1's l2: 0.0087282\n[3400]\ttraining's l2: 0.00257468\tvalid_1's l2: 0.00871258\n[3500]\ttraining's l2: 0.00249769\tvalid_1's l2: 0.00869234\n[3600]\ttraining's l2: 0.00242335\tvalid_1's l2: 0.00867672\n[3700]\ttraining's l2: 0.0023518\tvalid_1's l2: 0.00866175\n[3800]\ttraining's l2: 0.00228531\tvalid_1's l2: 0.00865742\n[3900]\ttraining's l2: 0.00222087\tvalid_1's l2: 0.00865144\n[4000]\ttraining's l2: 0.00215783\tvalid_1's l2: 0.00863791\n[4100]\ttraining's l2: 0.00209799\tvalid_1's l2: 0.00862306\n[4200]\ttraining's l2: 0.0020383\tvalid_1's l2: 0.00861292\n[4300]\ttraining's l2: 0.00197995\tvalid_1's l2: 0.00860634\n[4400]\ttraining's l2: 0.00192383\tvalid_1's l2: 0.00860311\n[4500]\ttraining's l2: 0.00186963\tvalid_1's l2: 0.00859523\n[4600]\ttraining's l2: 0.00181736\tvalid_1's l2: 0.00859333\n[4700]\ttraining's l2: 0.00176669\tvalid_1's l2: 0.00859415\nEarly stopping, best iteration is:\n[4617]\ttraining's l2: 0.00180883\tvalid_1's l2: 0.00859003\n[LightGBM] [Info] Total Bins 66750\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2762\n[LightGBM] [Info] Start training from score 0.659689\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0188382\tvalid_1's l2: 0.0194837\n[200]\ttraining's l2: 0.0144758\tvalid_1's l2: 0.015911\n[300]\ttraining's l2: 0.0119552\tvalid_1's l2: 0.0139881\n[400]\ttraining's l2: 0.010292\tvalid_1's l2: 0.0127071\n[500]\ttraining's l2: 0.00905724\tvalid_1's l2: 0.011749\n[600]\ttraining's l2: 0.00813842\tvalid_1's l2: 0.0110037\n[700]\ttraining's l2: 0.00746729\tvalid_1's l2: 0.0105173\n[800]\ttraining's l2: 0.00694001\tvalid_1's l2: 0.010164\n[900]\ttraining's l2: 0.00649715\tvalid_1's l2: 0.00989148\n[1000]\ttraining's l2: 0.00614159\tvalid_1's l2: 0.00968736\n[1100]\ttraining's l2: 0.00583724\tvalid_1's l2: 0.00951884\n[1200]\ttraining's l2: 0.005556\tvalid_1's l2: 0.00936858\n[1300]\ttraining's l2: 0.00531396\tvalid_1's l2: 0.00924965\n[1400]\ttraining's l2: 0.00509372\tvalid_1's l2: 0.00915625\n[1500]\ttraining's l2: 0.00489198\tvalid_1's l2: 0.00907288\n[1600]\ttraining's l2: 0.0047027\tvalid_1's l2: 0.00900167\n[1700]\ttraining's l2: 0.00453047\tvalid_1's l2: 0.00894636\n[1800]\ttraining's l2: 0.00435997\tvalid_1's l2: 0.00888671\n[1900]\ttraining's l2: 0.00419826\tvalid_1's l2: 0.00882953\n[2000]\ttraining's l2: 0.00405251\tvalid_1's l2: 0.0087908\n[2100]\ttraining's l2: 0.00391621\tvalid_1's l2: 0.00875229\n[2200]\ttraining's l2: 0.00377838\tvalid_1's l2: 0.00871071\n[2300]\ttraining's l2: 0.00365159\tvalid_1's l2: 0.00867767\n[2400]\ttraining's l2: 0.00352733\tvalid_1's l2: 0.00864171\n[2500]\ttraining's l2: 0.00341468\tvalid_1's l2: 0.00861317\n[2600]\ttraining's l2: 0.00330278\tvalid_1's l2: 0.00859416\n[2700]\ttraining's l2: 0.00319705\tvalid_1's l2: 0.0085667\n[2800]\ttraining's l2: 0.00309735\tvalid_1's l2: 0.00854936\n[2900]\ttraining's l2: 0.00299961\tvalid_1's l2: 0.00853158\n[3000]\ttraining's l2: 0.00289995\tvalid_1's l2: 0.00850919\n[3100]\ttraining's l2: 0.00281087\tvalid_1's l2: 0.00848772\n[3200]\ttraining's l2: 0.00272579\tvalid_1's l2: 0.00847414\n[3300]\ttraining's l2: 0.00264386\tvalid_1's l2: 0.00846329\n[3400]\ttraining's l2: 0.00256151\tvalid_1's l2: 0.00843692\n[3500]\ttraining's l2: 0.00248464\tvalid_1's l2: 0.00842854\n[3600]\ttraining's l2: 0.00241114\tvalid_1's l2: 0.00842468\nEarly stopping, best iteration is:\n[3555]\ttraining's l2: 0.00244267\tvalid_1's l2: 0.00842285\n[LightGBM] [Info] Total Bins 66586\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2687\n[LightGBM] [Info] Start training from score 0.661627\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0187134\tvalid_1's l2: 0.0208646\n[200]\ttraining's l2: 0.0144727\tvalid_1's l2: 0.0167479\n[300]\ttraining's l2: 0.012015\tvalid_1's l2: 0.0145049\n[400]\ttraining's l2: 0.0103563\tvalid_1's l2: 0.0130529\n[500]\ttraining's l2: 0.00911865\tvalid_1's l2: 0.0120289\n[600]\ttraining's l2: 0.00821162\tvalid_1's l2: 0.0113039\n[700]\ttraining's l2: 0.00752115\tvalid_1's l2: 0.0107764\n[800]\ttraining's l2: 0.00699002\tvalid_1's l2: 0.0103996\n[900]\ttraining's l2: 0.00655477\tvalid_1's l2: 0.0101246\n[1000]\ttraining's l2: 0.00619715\tvalid_1's l2: 0.00991146\n[1100]\ttraining's l2: 0.00588282\tvalid_1's l2: 0.00972925\n[1200]\ttraining's l2: 0.00559829\tvalid_1's l2: 0.00957815\n[1300]\ttraining's l2: 0.00535096\tvalid_1's l2: 0.00945808\n[1400]\ttraining's l2: 0.00512961\tvalid_1's l2: 0.00936994\n[1500]\ttraining's l2: 0.00493267\tvalid_1's l2: 0.00929584\n[1600]\ttraining's l2: 0.00474471\tvalid_1's l2: 0.00922203\n[1700]\ttraining's l2: 0.00456382\tvalid_1's l2: 0.00915512\n[1800]\ttraining's l2: 0.00438852\tvalid_1's l2: 0.00909378\n[1900]\ttraining's l2: 0.00423356\tvalid_1's l2: 0.00904359\n[2000]\ttraining's l2: 0.00408977\tvalid_1's l2: 0.00900203\n[2100]\ttraining's l2: 0.00395282\tvalid_1's l2: 0.00895163\n[2200]\ttraining's l2: 0.00382073\tvalid_1's l2: 0.00891379\n[2300]\ttraining's l2: 0.00369698\tvalid_1's l2: 0.00889106\n[2400]\ttraining's l2: 0.00356813\tvalid_1's l2: 0.00884628\n[2500]\ttraining's l2: 0.00345012\tvalid_1's l2: 0.00879987\n[2600]\ttraining's l2: 0.00334004\tvalid_1's l2: 0.00877289\n[2700]\ttraining's l2: 0.0032244\tvalid_1's l2: 0.00873699\n[2800]\ttraining's l2: 0.00312507\tvalid_1's l2: 0.00871075\n[2900]\ttraining's l2: 0.0030311\tvalid_1's l2: 0.00869162\n[3000]\ttraining's l2: 0.00293743\tvalid_1's l2: 0.00867121\n[3100]\ttraining's l2: 0.00284738\tvalid_1's l2: 0.00865654\n[3200]\ttraining's l2: 0.00275923\tvalid_1's l2: 0.00863537\n[3300]\ttraining's l2: 0.00267552\tvalid_1's l2: 0.00861795\n[3400]\ttraining's l2: 0.00259489\tvalid_1's l2: 0.00859251\n[3500]\ttraining's l2: 0.00251881\tvalid_1's l2: 0.00857535\n[3600]\ttraining's l2: 0.00244447\tvalid_1's l2: 0.00856082\n[3700]\ttraining's l2: 0.00237076\tvalid_1's l2: 0.00854417\n[3800]\ttraining's l2: 0.00230381\tvalid_1's l2: 0.00853655\n[3900]\ttraining's l2: 0.00223768\tvalid_1's l2: 0.0085283\n[4000]\ttraining's l2: 0.0021737\tvalid_1's l2: 0.0085219\n[4100]\ttraining's l2: 0.00211065\tvalid_1's l2: 0.00851129\n[4200]\ttraining's l2: 0.00205072\tvalid_1's l2: 0.00850351\n[4300]\ttraining's l2: 0.00199348\tvalid_1's l2: 0.00849738\n[4400]\ttraining's l2: 0.00193714\tvalid_1's l2: 0.0084953\n[4500]\ttraining's l2: 0.00188477\tvalid_1's l2: 0.00849076\n[4600]\ttraining's l2: 0.00183347\tvalid_1's l2: 0.00848775\n[4700]\ttraining's l2: 0.00178328\tvalid_1's l2: 0.00848\n[4800]\ttraining's l2: 0.00173358\tvalid_1's l2: 0.00847083\n[4900]\ttraining's l2: 0.00168621\tvalid_1's l2: 0.00846315\n[5000]\ttraining's l2: 0.0016401\tvalid_1's l2: 0.00845269\nDid not meet early stopping. Best iteration is:\n[4994]\ttraining's l2: 0.00164283\tvalid_1's l2: 0.00845151\n[LightGBM] [Info] Total Bins 66677\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 2728\n[LightGBM] [Info] Start training from score 0.659132\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.019121\tvalid_1's l2: 0.0187091\n[200]\ttraining's l2: 0.0146359\tvalid_1's l2: 0.0153515\n[300]\ttraining's l2: 0.0120402\tvalid_1's l2: 0.013528\n[400]\ttraining's l2: 0.0103474\tvalid_1's l2: 0.0123902\n[500]\ttraining's l2: 0.00911233\tvalid_1's l2: 0.0115625\n[600]\ttraining's l2: 0.00821263\tvalid_1's l2: 0.0110138\n[700]\ttraining's l2: 0.00752998\tvalid_1's l2: 0.010584\n[800]\ttraining's l2: 0.00701127\tvalid_1's l2: 0.0103065\n[900]\ttraining's l2: 0.00658369\tvalid_1's l2: 0.010065\n[1000]\ttraining's l2: 0.00620742\tvalid_1's l2: 0.00985918\n[1100]\ttraining's l2: 0.00589559\tvalid_1's l2: 0.00970919\n[1200]\ttraining's l2: 0.0056152\tvalid_1's l2: 0.00955603\n[1300]\ttraining's l2: 0.00537066\tvalid_1's l2: 0.00943306\n[1400]\ttraining's l2: 0.00514608\tvalid_1's l2: 0.00932033\n[1500]\ttraining's l2: 0.0049436\tvalid_1's l2: 0.00924119\n[1600]\ttraining's l2: 0.00475871\tvalid_1's l2: 0.00917728\n[1700]\ttraining's l2: 0.00458014\tvalid_1's l2: 0.00909966\n[1800]\ttraining's l2: 0.00441471\tvalid_1's l2: 0.00905079\n[1900]\ttraining's l2: 0.00425634\tvalid_1's l2: 0.00899251\n[2000]\ttraining's l2: 0.004101\tvalid_1's l2: 0.00892246\n[2100]\ttraining's l2: 0.00396343\tvalid_1's l2: 0.00887733\n[2200]\ttraining's l2: 0.00383126\tvalid_1's l2: 0.00882193\n[2300]\ttraining's l2: 0.0037132\tvalid_1's l2: 0.00878664\n[2400]\ttraining's l2: 0.00358793\tvalid_1's l2: 0.00875071\n[2500]\ttraining's l2: 0.0034661\tvalid_1's l2: 0.00871115\n[2600]\ttraining's l2: 0.00335638\tvalid_1's l2: 0.00868286\n[2700]\ttraining's l2: 0.00324414\tvalid_1's l2: 0.00864704\n[2800]\ttraining's l2: 0.00313691\tvalid_1's l2: 0.00861745\n[2900]\ttraining's l2: 0.00303553\tvalid_1's l2: 0.00859263\n[3000]\ttraining's l2: 0.00294212\tvalid_1's l2: 0.00857386\n[3100]\ttraining's l2: 0.00285238\tvalid_1's l2: 0.00855433\n[3200]\ttraining's l2: 0.00276509\tvalid_1's l2: 0.00852943\n[3300]\ttraining's l2: 0.00267891\tvalid_1's l2: 0.00850344\n[3400]\ttraining's l2: 0.00259522\tvalid_1's l2: 0.00848274\n[3500]\ttraining's l2: 0.00251728\tvalid_1's l2: 0.0084732\n[3600]\ttraining's l2: 0.0024406\tvalid_1's l2: 0.00846223\n[3700]\ttraining's l2: 0.00236763\tvalid_1's l2: 0.00844923\n[3800]\ttraining's l2: 0.00229875\tvalid_1's l2: 0.00843648\n[3900]\ttraining's l2: 0.0022309\tvalid_1's l2: 0.00841591\n[4000]\ttraining's l2: 0.00216654\tvalid_1's l2: 0.00840347\n[4100]\ttraining's l2: 0.00210347\tvalid_1's l2: 0.00838851\n[4200]\ttraining's l2: 0.00204184\tvalid_1's l2: 0.00837357\n[4300]\ttraining's l2: 0.00198232\tvalid_1's l2: 0.00836279\n[4400]\ttraining's l2: 0.00192555\tvalid_1's l2: 0.00835358\n[4500]\ttraining's l2: 0.00187158\tvalid_1's l2: 0.00835035\n[4600]\ttraining's l2: 0.00182042\tvalid_1's l2: 0.00834784\n[4700]\ttraining's l2: 0.0017712\tvalid_1's l2: 0.00834166\n[4800]\ttraining's l2: 0.00172273\tvalid_1's l2: 0.00833088\n[4900]\ttraining's l2: 0.00167601\tvalid_1's l2: 0.00832605\n[5000]\ttraining's l2: 0.0016296\tvalid_1's l2: 0.00831764\nDid not meet early stopping. Best iteration is:\n[4991]\ttraining's l2: 0.00163406\tvalid_1's l2: 0.00831696\n[LightGBM] [Info] Total Bins 67373\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.658422\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0208141\tvalid_1's l2: 0.0206126\n[200]\ttraining's l2: 0.0161347\tvalid_1's l2: 0.0172232\n[300]\ttraining's l2: 0.0131187\tvalid_1's l2: 0.0151345\n[400]\ttraining's l2: 0.0110692\tvalid_1's l2: 0.0138015\n[500]\ttraining's l2: 0.00958633\tvalid_1's l2: 0.012872\n[600]\ttraining's l2: 0.00843891\tvalid_1's l2: 0.0121659\n[700]\ttraining's l2: 0.00754464\tvalid_1's l2: 0.0116442\n[800]\ttraining's l2: 0.0068227\tvalid_1's l2: 0.0112044\n[900]\ttraining's l2: 0.00623775\tvalid_1's l2: 0.010868\n[1000]\ttraining's l2: 0.00574188\tvalid_1's l2: 0.010564\n[1100]\ttraining's l2: 0.00532462\tvalid_1's l2: 0.0103281\n[1200]\ttraining's l2: 0.00496703\tvalid_1's l2: 0.0101321\n[1300]\ttraining's l2: 0.00466352\tvalid_1's l2: 0.00997607\n[1400]\ttraining's l2: 0.00439981\tvalid_1's l2: 0.00984328\n[1500]\ttraining's l2: 0.0041632\tvalid_1's l2: 0.00973348\n[1600]\ttraining's l2: 0.00394961\tvalid_1's l2: 0.00962682\n[1700]\ttraining's l2: 0.00375056\tvalid_1's l2: 0.00953367\n[1800]\ttraining's l2: 0.00356921\tvalid_1's l2: 0.00944723\n[1900]\ttraining's l2: 0.00340666\tvalid_1's l2: 0.00936876\n[2000]\ttraining's l2: 0.00326219\tvalid_1's l2: 0.00931355\n[2100]\ttraining's l2: 0.00312598\tvalid_1's l2: 0.00926856\n[2200]\ttraining's l2: 0.00299729\tvalid_1's l2: 0.00922754\n[2300]\ttraining's l2: 0.0028742\tvalid_1's l2: 0.00918832\n[2400]\ttraining's l2: 0.00275816\tvalid_1's l2: 0.00914395\n[2500]\ttraining's l2: 0.00265063\tvalid_1's l2: 0.00910641\n[2600]\ttraining's l2: 0.00254875\tvalid_1's l2: 0.00907306\n[2700]\ttraining's l2: 0.00245172\tvalid_1's l2: 0.00904803\n[2800]\ttraining's l2: 0.00236256\tvalid_1's l2: 0.00902438\n[2900]\ttraining's l2: 0.00227334\tvalid_1's l2: 0.00899334\n[3000]\ttraining's l2: 0.00219124\tvalid_1's l2: 0.0089729\n[3100]\ttraining's l2: 0.00211346\tvalid_1's l2: 0.00894786\n[3200]\ttraining's l2: 0.00203777\tvalid_1's l2: 0.00892451\n[3300]\ttraining's l2: 0.00196723\tvalid_1's l2: 0.00891202\n[3400]\ttraining's l2: 0.00189848\tvalid_1's l2: 0.00889437\n[3500]\ttraining's l2: 0.00183312\tvalid_1's l2: 0.00887675\n[3600]\ttraining's l2: 0.00177007\tvalid_1's l2: 0.00885976\n[3700]\ttraining's l2: 0.00171094\tvalid_1's l2: 0.00884571\n[3800]\ttraining's l2: 0.00165368\tvalid_1's l2: 0.00882781\n[3900]\ttraining's l2: 0.00159869\tvalid_1's l2: 0.00881544\n[4000]\ttraining's l2: 0.00154517\tvalid_1's l2: 0.00880716\n[4100]\ttraining's l2: 0.001495\tvalid_1's l2: 0.00879758\n[4200]\ttraining's l2: 0.00144546\tvalid_1's l2: 0.00878409\n[4300]\ttraining's l2: 0.00139864\tvalid_1's l2: 0.00877662\n[4400]\ttraining's l2: 0.00135474\tvalid_1's l2: 0.00876858\n[4500]\ttraining's l2: 0.00131243\tvalid_1's l2: 0.00876175\n[4600]\ttraining's l2: 0.00127131\tvalid_1's l2: 0.00875201\n[4700]\ttraining's l2: 0.00123152\tvalid_1's l2: 0.00874807\n[4800]\ttraining's l2: 0.00119288\tvalid_1's l2: 0.00874426\n[4900]\ttraining's l2: 0.00115663\tvalid_1's l2: 0.00873874\n[5000]\ttraining's l2: 0.00112167\tvalid_1's l2: 0.008733\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00112167\tvalid_1's l2: 0.008733\n[LightGBM] [Info] Total Bins 67366\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661442\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0207526\tvalid_1's l2: 0.0221337\n[200]\ttraining's l2: 0.0162642\tvalid_1's l2: 0.0182273\n[300]\ttraining's l2: 0.0132824\tvalid_1's l2: 0.0157637\n[400]\ttraining's l2: 0.0111983\tvalid_1's l2: 0.0141332\n[500]\ttraining's l2: 0.00965004\tvalid_1's l2: 0.0129682\n[600]\ttraining's l2: 0.00847631\tvalid_1's l2: 0.0121062\n[700]\ttraining's l2: 0.00756904\tvalid_1's l2: 0.0114895\n[800]\ttraining's l2: 0.0068279\tvalid_1's l2: 0.010978\n[900]\ttraining's l2: 0.00621254\tvalid_1's l2: 0.0105739\n[1000]\ttraining's l2: 0.00570931\tvalid_1's l2: 0.0102547\n[1100]\ttraining's l2: 0.00528794\tvalid_1's l2: 0.0100056\n[1200]\ttraining's l2: 0.00493733\tvalid_1's l2: 0.00980527\n[1300]\ttraining's l2: 0.0046232\tvalid_1's l2: 0.00963165\n[1400]\ttraining's l2: 0.00435859\tvalid_1's l2: 0.00950947\n[1500]\ttraining's l2: 0.00411744\tvalid_1's l2: 0.00939016\n[1600]\ttraining's l2: 0.00390168\tvalid_1's l2: 0.0092847\n[1700]\ttraining's l2: 0.00370383\tvalid_1's l2: 0.00919618\n[1800]\ttraining's l2: 0.00352691\tvalid_1's l2: 0.00913178\n[1900]\ttraining's l2: 0.00336593\tvalid_1's l2: 0.00906758\n[2000]\ttraining's l2: 0.00321518\tvalid_1's l2: 0.0090116\n[2100]\ttraining's l2: 0.00307272\tvalid_1's l2: 0.00896439\n[2200]\ttraining's l2: 0.00294324\tvalid_1's l2: 0.00891757\n[2300]\ttraining's l2: 0.00282396\tvalid_1's l2: 0.00888422\n[2400]\ttraining's l2: 0.002708\tvalid_1's l2: 0.00884793\n[2500]\ttraining's l2: 0.0026019\tvalid_1's l2: 0.00882059\n[2600]\ttraining's l2: 0.00250422\tvalid_1's l2: 0.00878855\n[2700]\ttraining's l2: 0.00241059\tvalid_1's l2: 0.00876421\n[2800]\ttraining's l2: 0.0023193\tvalid_1's l2: 0.00874188\n[2900]\ttraining's l2: 0.00223243\tvalid_1's l2: 0.00872194\n[3000]\ttraining's l2: 0.00215235\tvalid_1's l2: 0.0087039\n[3100]\ttraining's l2: 0.00207559\tvalid_1's l2: 0.00868309\n[3200]\ttraining's l2: 0.00200117\tvalid_1's l2: 0.00866711\n[3300]\ttraining's l2: 0.00193131\tvalid_1's l2: 0.00864917\n[3400]\ttraining's l2: 0.00186382\tvalid_1's l2: 0.00863156\n[3500]\ttraining's l2: 0.00179792\tvalid_1's l2: 0.00862039\n[3600]\ttraining's l2: 0.00173708\tvalid_1's l2: 0.0086085\n[3700]\ttraining's l2: 0.00167901\tvalid_1's l2: 0.00859671\n[3800]\ttraining's l2: 0.00162246\tvalid_1's l2: 0.00858602\n[3900]\ttraining's l2: 0.00156887\tvalid_1's l2: 0.00857249\n[4000]\ttraining's l2: 0.00151749\tvalid_1's l2: 0.0085593\n[4100]\ttraining's l2: 0.0014683\tvalid_1's l2: 0.00855182\n[4200]\ttraining's l2: 0.00142062\tvalid_1's l2: 0.00854491\n[4300]\ttraining's l2: 0.00137536\tvalid_1's l2: 0.00853537\n[4400]\ttraining's l2: 0.00133223\tvalid_1's l2: 0.00852986\n[4500]\ttraining's l2: 0.00129022\tvalid_1's l2: 0.0085235\n[4600]\ttraining's l2: 0.00124927\tvalid_1's l2: 0.00851798\n[4700]\ttraining's l2: 0.00121005\tvalid_1's l2: 0.00851189\n[4800]\ttraining's l2: 0.00117329\tvalid_1's l2: 0.0085082\n[4900]\ttraining's l2: 0.00113754\tvalid_1's l2: 0.00850101\n[5000]\ttraining's l2: 0.00110295\tvalid_1's l2: 0.0084945\nDid not meet early stopping. Best iteration is:\n[4999]\ttraining's l2: 0.00110329\tvalid_1's l2: 0.00849447\n[LightGBM] [Info] Total Bins 67389\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661377\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0208624\tvalid_1's l2: 0.021518\n[200]\ttraining's l2: 0.0163467\tvalid_1's l2: 0.0175594\n[300]\ttraining's l2: 0.0133555\tvalid_1's l2: 0.0150993\n[400]\ttraining's l2: 0.0112745\tvalid_1's l2: 0.0134819\n[500]\ttraining's l2: 0.00975151\tvalid_1's l2: 0.0123678\n[600]\ttraining's l2: 0.00857516\tvalid_1's l2: 0.0115396\n[700]\ttraining's l2: 0.00765856\tvalid_1's l2: 0.010914\n[800]\ttraining's l2: 0.0069159\tvalid_1's l2: 0.0104277\n[900]\ttraining's l2: 0.00629901\tvalid_1's l2: 0.0100207\n[1000]\ttraining's l2: 0.00579231\tvalid_1's l2: 0.0097063\n[1100]\ttraining's l2: 0.00536973\tvalid_1's l2: 0.00945568\n[1200]\ttraining's l2: 0.00500747\tvalid_1's l2: 0.00925818\n[1300]\ttraining's l2: 0.00470083\tvalid_1's l2: 0.00909219\n[1400]\ttraining's l2: 0.00442365\tvalid_1's l2: 0.00895103\n[1500]\ttraining's l2: 0.00417408\tvalid_1's l2: 0.00884098\n[1600]\ttraining's l2: 0.00395555\tvalid_1's l2: 0.00875239\n[1700]\ttraining's l2: 0.00376225\tvalid_1's l2: 0.00867497\n[1800]\ttraining's l2: 0.00358926\tvalid_1's l2: 0.0086198\n[1900]\ttraining's l2: 0.003423\tvalid_1's l2: 0.00856181\n[2000]\ttraining's l2: 0.00327153\tvalid_1's l2: 0.00851503\n[2100]\ttraining's l2: 0.00312855\tvalid_1's l2: 0.00846208\n[2200]\ttraining's l2: 0.00299694\tvalid_1's l2: 0.00842408\n[2300]\ttraining's l2: 0.00287529\tvalid_1's l2: 0.00838807\n[2400]\ttraining's l2: 0.00276329\tvalid_1's l2: 0.00835593\n[2500]\ttraining's l2: 0.00265725\tvalid_1's l2: 0.00832968\n[2600]\ttraining's l2: 0.00255635\tvalid_1's l2: 0.00830165\n[2700]\ttraining's l2: 0.00246137\tvalid_1's l2: 0.00827537\n[2800]\ttraining's l2: 0.00236934\tvalid_1's l2: 0.00824657\n[2900]\ttraining's l2: 0.00228237\tvalid_1's l2: 0.00822854\n[3000]\ttraining's l2: 0.00219967\tvalid_1's l2: 0.00820656\n[3100]\ttraining's l2: 0.00211872\tvalid_1's l2: 0.00818112\n[3200]\ttraining's l2: 0.00204259\tvalid_1's l2: 0.00816166\n[3300]\ttraining's l2: 0.00196938\tvalid_1's l2: 0.00814627\n[3400]\ttraining's l2: 0.00190143\tvalid_1's l2: 0.00813307\n[3500]\ttraining's l2: 0.00183769\tvalid_1's l2: 0.00812381\n[3600]\ttraining's l2: 0.0017744\tvalid_1's l2: 0.00811289\n[3700]\ttraining's l2: 0.00171575\tvalid_1's l2: 0.00810291\n[3800]\ttraining's l2: 0.00165903\tvalid_1's l2: 0.00809025\n[3900]\ttraining's l2: 0.00160547\tvalid_1's l2: 0.00808337\n[4000]\ttraining's l2: 0.00155297\tvalid_1's l2: 0.00807347\n[4100]\ttraining's l2: 0.00150249\tvalid_1's l2: 0.00806964\n[4200]\ttraining's l2: 0.00145369\tvalid_1's l2: 0.00805741\n[4300]\ttraining's l2: 0.00140757\tvalid_1's l2: 0.00804798\n[4400]\ttraining's l2: 0.00136384\tvalid_1's l2: 0.00804187\n[4500]\ttraining's l2: 0.00132121\tvalid_1's l2: 0.00803457\n[4600]\ttraining's l2: 0.0012813\tvalid_1's l2: 0.00802946\n[4700]\ttraining's l2: 0.00124089\tvalid_1's l2: 0.00802646\n[4800]\ttraining's l2: 0.00120287\tvalid_1's l2: 0.00802103\n[4900]\ttraining's l2: 0.00116661\tvalid_1's l2: 0.00801892\n[5000]\ttraining's l2: 0.00113128\tvalid_1's l2: 0.00801451\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00113128\tvalid_1's l2: 0.00801451\n[LightGBM] [Info] Total Bins 67340\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661045\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0205651\tvalid_1's l2: 0.0222248\n[200]\ttraining's l2: 0.0160415\tvalid_1's l2: 0.0183035\n[300]\ttraining's l2: 0.0130449\tvalid_1's l2: 0.0158044\n[400]\ttraining's l2: 0.0109861\tvalid_1's l2: 0.0141621\n[500]\ttraining's l2: 0.0095081\tvalid_1's l2: 0.0130426\n[600]\ttraining's l2: 0.00838014\tvalid_1's l2: 0.0122023\n[700]\ttraining's l2: 0.00750083\tvalid_1's l2: 0.0115676\n[800]\ttraining's l2: 0.0067853\tvalid_1's l2: 0.01107\n[900]\ttraining's l2: 0.00618248\tvalid_1's l2: 0.0106523\n[1000]\ttraining's l2: 0.00568999\tvalid_1's l2: 0.010313\n[1100]\ttraining's l2: 0.00527786\tvalid_1's l2: 0.0100444\n[1200]\ttraining's l2: 0.00492269\tvalid_1's l2: 0.00982218\n[1300]\ttraining's l2: 0.00462531\tvalid_1's l2: 0.00965403\n[1400]\ttraining's l2: 0.00435902\tvalid_1's l2: 0.00950491\n[1500]\ttraining's l2: 0.00412353\tvalid_1's l2: 0.009383\n[1600]\ttraining's l2: 0.00391035\tvalid_1's l2: 0.00927967\n[1700]\ttraining's l2: 0.00371744\tvalid_1's l2: 0.00919484\n[1800]\ttraining's l2: 0.00354429\tvalid_1's l2: 0.00912639\n[1900]\ttraining's l2: 0.0033874\tvalid_1's l2: 0.00906618\n[2000]\ttraining's l2: 0.0032381\tvalid_1's l2: 0.00901565\n[2100]\ttraining's l2: 0.00310074\tvalid_1's l2: 0.00896636\n[2200]\ttraining's l2: 0.00296808\tvalid_1's l2: 0.00892099\n[2300]\ttraining's l2: 0.00284744\tvalid_1's l2: 0.00888207\n[2400]\ttraining's l2: 0.00273338\tvalid_1's l2: 0.00884868\n[2500]\ttraining's l2: 0.00262614\tvalid_1's l2: 0.00881127\n[2600]\ttraining's l2: 0.0025249\tvalid_1's l2: 0.00877984\n[2700]\ttraining's l2: 0.00242858\tvalid_1's l2: 0.00875202\n[2800]\ttraining's l2: 0.00233735\tvalid_1's l2: 0.00872691\n[2900]\ttraining's l2: 0.00224831\tvalid_1's l2: 0.00870063\n[3000]\ttraining's l2: 0.00216485\tvalid_1's l2: 0.00868357\n[3100]\ttraining's l2: 0.00208611\tvalid_1's l2: 0.00866084\n[3200]\ttraining's l2: 0.00200997\tvalid_1's l2: 0.00864129\n[3300]\ttraining's l2: 0.00193881\tvalid_1's l2: 0.008628\n[3400]\ttraining's l2: 0.00187275\tvalid_1's l2: 0.00861678\n[3500]\ttraining's l2: 0.00180903\tvalid_1's l2: 0.00860653\n[3600]\ttraining's l2: 0.00174677\tvalid_1's l2: 0.00859041\n[3700]\ttraining's l2: 0.00168851\tvalid_1's l2: 0.00858265\n[3800]\ttraining's l2: 0.00163151\tvalid_1's l2: 0.00857346\n[3900]\ttraining's l2: 0.00157727\tvalid_1's l2: 0.00856409\n[4000]\ttraining's l2: 0.00152555\tvalid_1's l2: 0.00855311\n[4100]\ttraining's l2: 0.00147594\tvalid_1's l2: 0.00854505\n[4200]\ttraining's l2: 0.00142813\tvalid_1's l2: 0.00853546\n[4300]\ttraining's l2: 0.0013828\tvalid_1's l2: 0.00852679\n[4400]\ttraining's l2: 0.00133879\tvalid_1's l2: 0.00851699\n[4500]\ttraining's l2: 0.00129633\tvalid_1's l2: 0.00851056\n[4600]\ttraining's l2: 0.00125614\tvalid_1's l2: 0.00850353\n[4700]\ttraining's l2: 0.00121818\tvalid_1's l2: 0.00849908\n[4800]\ttraining's l2: 0.00118113\tvalid_1's l2: 0.00849165\n[4900]\ttraining's l2: 0.00114528\tvalid_1's l2: 0.00848516\n[5000]\ttraining's l2: 0.00111053\tvalid_1's l2: 0.00848076\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00111053\tvalid_1's l2: 0.00848076\n[LightGBM] [Info] Total Bins 67385\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661050\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0207216\tvalid_1's l2: 0.0216479\n[200]\ttraining's l2: 0.0162353\tvalid_1's l2: 0.0176313\n[300]\ttraining's l2: 0.01329\tvalid_1's l2: 0.0151004\n[400]\ttraining's l2: 0.0112366\tvalid_1's l2: 0.0134773\n[500]\ttraining's l2: 0.00973024\tvalid_1's l2: 0.0123601\n[600]\ttraining's l2: 0.00859854\tvalid_1's l2: 0.0115507\n[700]\ttraining's l2: 0.00768799\tvalid_1's l2: 0.0109276\n[800]\ttraining's l2: 0.00695135\tvalid_1's l2: 0.010444\n[900]\ttraining's l2: 0.00633559\tvalid_1's l2: 0.0100363\n[1000]\ttraining's l2: 0.00584007\tvalid_1's l2: 0.00972653\n[1100]\ttraining's l2: 0.00541558\tvalid_1's l2: 0.00947616\n[1200]\ttraining's l2: 0.00505074\tvalid_1's l2: 0.00926397\n[1300]\ttraining's l2: 0.00473358\tvalid_1's l2: 0.00909243\n[1400]\ttraining's l2: 0.00446208\tvalid_1's l2: 0.00895812\n[1500]\ttraining's l2: 0.00421489\tvalid_1's l2: 0.00884173\n[1600]\ttraining's l2: 0.00399045\tvalid_1's l2: 0.0087342\n[1700]\ttraining's l2: 0.00378866\tvalid_1's l2: 0.00864307\n[1800]\ttraining's l2: 0.00360962\tvalid_1's l2: 0.00857725\n[1900]\ttraining's l2: 0.0034433\tvalid_1's l2: 0.00852047\n[2000]\ttraining's l2: 0.00329203\tvalid_1's l2: 0.00846867\n[2100]\ttraining's l2: 0.00315057\tvalid_1's l2: 0.00842525\n[2200]\ttraining's l2: 0.00301569\tvalid_1's l2: 0.00837658\n[2300]\ttraining's l2: 0.00289474\tvalid_1's l2: 0.00834196\n[2400]\ttraining's l2: 0.00277721\tvalid_1's l2: 0.00830504\n[2500]\ttraining's l2: 0.00266888\tvalid_1's l2: 0.00827314\n[2600]\ttraining's l2: 0.00256588\tvalid_1's l2: 0.00824225\n[2700]\ttraining's l2: 0.00246971\tvalid_1's l2: 0.00821785\n[2800]\ttraining's l2: 0.0023778\tvalid_1's l2: 0.00819431\n[2900]\ttraining's l2: 0.00228504\tvalid_1's l2: 0.00816347\n[3000]\ttraining's l2: 0.00220143\tvalid_1's l2: 0.00814453\n[3100]\ttraining's l2: 0.0021198\tvalid_1's l2: 0.0081189\n[3200]\ttraining's l2: 0.00204247\tvalid_1's l2: 0.00810003\n[3300]\ttraining's l2: 0.00196888\tvalid_1's l2: 0.00808283\n[3400]\ttraining's l2: 0.00190136\tvalid_1's l2: 0.0080681\n[3500]\ttraining's l2: 0.00183537\tvalid_1's l2: 0.00805591\n[3600]\ttraining's l2: 0.00177276\tvalid_1's l2: 0.00803975\n[3700]\ttraining's l2: 0.00171232\tvalid_1's l2: 0.00802825\n[3800]\ttraining's l2: 0.00165586\tvalid_1's l2: 0.0080159\n[3900]\ttraining's l2: 0.00160197\tvalid_1's l2: 0.00800425\n[4000]\ttraining's l2: 0.00154789\tvalid_1's l2: 0.00799523\n[4100]\ttraining's l2: 0.00149749\tvalid_1's l2: 0.00798501\n[4200]\ttraining's l2: 0.00145073\tvalid_1's l2: 0.00797703\n[4300]\ttraining's l2: 0.00140368\tvalid_1's l2: 0.00796681\n[4400]\ttraining's l2: 0.00135995\tvalid_1's l2: 0.00795949\n[4500]\ttraining's l2: 0.00131793\tvalid_1's l2: 0.00795132\n[4600]\ttraining's l2: 0.00127697\tvalid_1's l2: 0.00794526\n[4700]\ttraining's l2: 0.00123787\tvalid_1's l2: 0.00793673\n[4800]\ttraining's l2: 0.00119943\tvalid_1's l2: 0.00792959\n[4900]\ttraining's l2: 0.00116281\tvalid_1's l2: 0.00792603\n[5000]\ttraining's l2: 0.00112696\tvalid_1's l2: 0.00791949\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00112696\tvalid_1's l2: 0.00791949\n[LightGBM] [Info] Total Bins 67346\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661659\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0206833\tvalid_1's l2: 0.0219458\n[200]\ttraining's l2: 0.0161515\tvalid_1's l2: 0.0180022\n[300]\ttraining's l2: 0.0131857\tvalid_1's l2: 0.0155758\n[400]\ttraining's l2: 0.0111183\tvalid_1's l2: 0.0140187\n[500]\ttraining's l2: 0.00960579\tvalid_1's l2: 0.0129544\n[600]\ttraining's l2: 0.00845024\tvalid_1's l2: 0.0121642\n[700]\ttraining's l2: 0.00750991\tvalid_1's l2: 0.0115375\n[800]\ttraining's l2: 0.00678\tvalid_1's l2: 0.0110753\n[900]\ttraining's l2: 0.00618218\tvalid_1's l2: 0.0106969\n[1000]\ttraining's l2: 0.00568846\tvalid_1's l2: 0.0104101\n[1100]\ttraining's l2: 0.00527899\tvalid_1's l2: 0.0101784\n[1200]\ttraining's l2: 0.0049284\tvalid_1's l2: 0.00999202\n[1300]\ttraining's l2: 0.00462207\tvalid_1's l2: 0.00984177\n[1400]\ttraining's l2: 0.00435456\tvalid_1's l2: 0.00971082\n[1500]\ttraining's l2: 0.00411719\tvalid_1's l2: 0.00959579\n[1600]\ttraining's l2: 0.00390799\tvalid_1's l2: 0.00950772\n[1700]\ttraining's l2: 0.0037142\tvalid_1's l2: 0.00941889\n[1800]\ttraining's l2: 0.00353941\tvalid_1's l2: 0.00934868\n[1900]\ttraining's l2: 0.00337679\tvalid_1's l2: 0.00929504\n[2000]\ttraining's l2: 0.0032263\tvalid_1's l2: 0.00924099\n[2100]\ttraining's l2: 0.00308977\tvalid_1's l2: 0.00918772\n[2200]\ttraining's l2: 0.00296124\tvalid_1's l2: 0.00914518\n[2300]\ttraining's l2: 0.00284101\tvalid_1's l2: 0.00910296\n[2400]\ttraining's l2: 0.00272918\tvalid_1's l2: 0.0090746\n[2500]\ttraining's l2: 0.00262347\tvalid_1's l2: 0.00904853\n[2600]\ttraining's l2: 0.00252116\tvalid_1's l2: 0.00902026\n[2700]\ttraining's l2: 0.00242297\tvalid_1's l2: 0.00899693\n[2800]\ttraining's l2: 0.00233111\tvalid_1's l2: 0.00897518\n[2900]\ttraining's l2: 0.0022426\tvalid_1's l2: 0.00895535\n[3000]\ttraining's l2: 0.00216026\tvalid_1's l2: 0.00893756\n[3100]\ttraining's l2: 0.00208288\tvalid_1's l2: 0.00892289\n[3200]\ttraining's l2: 0.0020084\tvalid_1's l2: 0.00890721\n[3300]\ttraining's l2: 0.00193635\tvalid_1's l2: 0.00888887\n[3400]\ttraining's l2: 0.00186894\tvalid_1's l2: 0.00887948\n[3500]\ttraining's l2: 0.00180298\tvalid_1's l2: 0.0088676\n[3600]\ttraining's l2: 0.00174005\tvalid_1's l2: 0.00885769\n[3700]\ttraining's l2: 0.00167964\tvalid_1's l2: 0.00884563\n[3800]\ttraining's l2: 0.00162405\tvalid_1's l2: 0.0088379\n[3900]\ttraining's l2: 0.00156965\tvalid_1's l2: 0.00882825\n[4000]\ttraining's l2: 0.00151771\tvalid_1's l2: 0.00882227\n[4100]\ttraining's l2: 0.0014677\tvalid_1's l2: 0.00881587\n[4200]\ttraining's l2: 0.00141951\tvalid_1's l2: 0.00880426\n[4300]\ttraining's l2: 0.00137424\tvalid_1's l2: 0.00879929\n[4400]\ttraining's l2: 0.00133055\tvalid_1's l2: 0.00879151\n[4500]\ttraining's l2: 0.00128888\tvalid_1's l2: 0.00878359\n[4600]\ttraining's l2: 0.00124909\tvalid_1's l2: 0.00877671\n[4700]\ttraining's l2: 0.00121145\tvalid_1's l2: 0.00877002\n[4800]\ttraining's l2: 0.00117424\tvalid_1's l2: 0.00876436\n[4900]\ttraining's l2: 0.0011388\tvalid_1's l2: 0.00876089\n[5000]\ttraining's l2: 0.00110421\tvalid_1's l2: 0.00875576\nDid not meet early stopping. Best iteration is:\n[4998]\ttraining's l2: 0.00110491\tvalid_1's l2: 0.00875573\n[LightGBM] [Info] Total Bins 67198\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661229\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0207132\tvalid_1's l2: 0.0215033\n[200]\ttraining's l2: 0.0161686\tvalid_1's l2: 0.0176271\n[300]\ttraining's l2: 0.0132172\tvalid_1's l2: 0.0152787\n[400]\ttraining's l2: 0.0111488\tvalid_1's l2: 0.0137262\n[500]\ttraining's l2: 0.00962891\tvalid_1's l2: 0.0126374\n[600]\ttraining's l2: 0.00847017\tvalid_1's l2: 0.0118536\n[700]\ttraining's l2: 0.00757093\tvalid_1's l2: 0.0112667\n[800]\ttraining's l2: 0.0068422\tvalid_1's l2: 0.0108083\n[900]\ttraining's l2: 0.00624071\tvalid_1's l2: 0.0104372\n[1000]\ttraining's l2: 0.00573977\tvalid_1's l2: 0.0101301\n[1100]\ttraining's l2: 0.00531563\tvalid_1's l2: 0.00988336\n[1200]\ttraining's l2: 0.0049541\tvalid_1's l2: 0.00969548\n[1300]\ttraining's l2: 0.00464116\tvalid_1's l2: 0.00953982\n[1400]\ttraining's l2: 0.00436538\tvalid_1's l2: 0.00942218\n[1500]\ttraining's l2: 0.00412554\tvalid_1's l2: 0.00931552\n[1600]\ttraining's l2: 0.003911\tvalid_1's l2: 0.00922738\n[1700]\ttraining's l2: 0.00371018\tvalid_1's l2: 0.00914734\n[1800]\ttraining's l2: 0.00352748\tvalid_1's l2: 0.00908477\n[1900]\ttraining's l2: 0.00336188\tvalid_1's l2: 0.00902485\n[2000]\ttraining's l2: 0.00321032\tvalid_1's l2: 0.00897721\n[2100]\ttraining's l2: 0.00307562\tvalid_1's l2: 0.0089396\n[2200]\ttraining's l2: 0.0029483\tvalid_1's l2: 0.00890704\n[2300]\ttraining's l2: 0.00282807\tvalid_1's l2: 0.00888317\n[2400]\ttraining's l2: 0.00271549\tvalid_1's l2: 0.00885154\n[2500]\ttraining's l2: 0.00260966\tvalid_1's l2: 0.00882271\n[2600]\ttraining's l2: 0.00251073\tvalid_1's l2: 0.00880366\n[2700]\ttraining's l2: 0.00241332\tvalid_1's l2: 0.00877913\n[2800]\ttraining's l2: 0.0023215\tvalid_1's l2: 0.00875616\n[2900]\ttraining's l2: 0.00223624\tvalid_1's l2: 0.00873626\n[3000]\ttraining's l2: 0.00215395\tvalid_1's l2: 0.00872288\n[3100]\ttraining's l2: 0.00207683\tvalid_1's l2: 0.00870854\n[3200]\ttraining's l2: 0.0020025\tvalid_1's l2: 0.00869387\n[3300]\ttraining's l2: 0.00193309\tvalid_1's l2: 0.00868108\n[3400]\ttraining's l2: 0.00186431\tvalid_1's l2: 0.00866695\n[3500]\ttraining's l2: 0.00180006\tvalid_1's l2: 0.00865606\n[3600]\ttraining's l2: 0.00173845\tvalid_1's l2: 0.00864362\n[3700]\ttraining's l2: 0.00168012\tvalid_1's l2: 0.00863414\n[3800]\ttraining's l2: 0.00162325\tvalid_1's l2: 0.00862363\n[3900]\ttraining's l2: 0.00156902\tvalid_1's l2: 0.00861523\n[4000]\ttraining's l2: 0.00151761\tvalid_1's l2: 0.00860981\n[4100]\ttraining's l2: 0.00146724\tvalid_1's l2: 0.00859922\n[4200]\ttraining's l2: 0.00142117\tvalid_1's l2: 0.00859099\n[4300]\ttraining's l2: 0.00137599\tvalid_1's l2: 0.00858452\n[4400]\ttraining's l2: 0.00133232\tvalid_1's l2: 0.00858145\n[4500]\ttraining's l2: 0.00128939\tvalid_1's l2: 0.00856975\n[4600]\ttraining's l2: 0.00124966\tvalid_1's l2: 0.00856775\n[4700]\ttraining's l2: 0.00121106\tvalid_1's l2: 0.00855875\n[4800]\ttraining's l2: 0.0011737\tvalid_1's l2: 0.00855362\n[4900]\ttraining's l2: 0.00113824\tvalid_1's l2: 0.00854692\n[5000]\ttraining's l2: 0.00110425\tvalid_1's l2: 0.00854501\nDid not meet early stopping. Best iteration is:\n[4959]\ttraining's l2: 0.0011181\tvalid_1's l2: 0.00854481\n[LightGBM] [Info] Total Bins 67378\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.659689\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0206818\tvalid_1's l2: 0.0213684\n[200]\ttraining's l2: 0.0160816\tvalid_1's l2: 0.0177072\n[300]\ttraining's l2: 0.0131176\tvalid_1's l2: 0.0154695\n[400]\ttraining's l2: 0.0110543\tvalid_1's l2: 0.0139707\n[500]\ttraining's l2: 0.00955151\tvalid_1's l2: 0.0129148\n[600]\ttraining's l2: 0.00841126\tvalid_1's l2: 0.0121365\n[700]\ttraining's l2: 0.00750863\tvalid_1's l2: 0.0115413\n[800]\ttraining's l2: 0.00677548\tvalid_1's l2: 0.0110476\n[900]\ttraining's l2: 0.00618121\tvalid_1's l2: 0.0106617\n[1000]\ttraining's l2: 0.00568908\tvalid_1's l2: 0.0103394\n[1100]\ttraining's l2: 0.00528495\tvalid_1's l2: 0.0100926\n[1200]\ttraining's l2: 0.00493102\tvalid_1's l2: 0.00988058\n[1300]\ttraining's l2: 0.00463267\tvalid_1's l2: 0.00971229\n[1400]\ttraining's l2: 0.00436183\tvalid_1's l2: 0.00957046\n[1500]\ttraining's l2: 0.00412423\tvalid_1's l2: 0.0094497\n[1600]\ttraining's l2: 0.00391217\tvalid_1's l2: 0.00935066\n[1700]\ttraining's l2: 0.00372029\tvalid_1's l2: 0.00926639\n[1800]\ttraining's l2: 0.00354433\tvalid_1's l2: 0.00919269\n[1900]\ttraining's l2: 0.00338015\tvalid_1's l2: 0.00913142\n[2000]\ttraining's l2: 0.00323223\tvalid_1's l2: 0.00907821\n[2100]\ttraining's l2: 0.00309272\tvalid_1's l2: 0.00902314\n[2200]\ttraining's l2: 0.00296416\tvalid_1's l2: 0.00897768\n[2300]\ttraining's l2: 0.00284245\tvalid_1's l2: 0.00893984\n[2400]\ttraining's l2: 0.00273016\tvalid_1's l2: 0.0089043\n[2500]\ttraining's l2: 0.00262347\tvalid_1's l2: 0.00887038\n[2600]\ttraining's l2: 0.00252467\tvalid_1's l2: 0.00884832\n[2700]\ttraining's l2: 0.00243037\tvalid_1's l2: 0.00882627\n[2800]\ttraining's l2: 0.00234202\tvalid_1's l2: 0.00879781\n[2900]\ttraining's l2: 0.00225337\tvalid_1's l2: 0.00877054\n[3000]\ttraining's l2: 0.00217154\tvalid_1's l2: 0.00875083\n[3100]\ttraining's l2: 0.00209316\tvalid_1's l2: 0.00872876\n[3200]\ttraining's l2: 0.00201848\tvalid_1's l2: 0.0087115\n[3300]\ttraining's l2: 0.00194739\tvalid_1's l2: 0.00869377\n[3400]\ttraining's l2: 0.00187971\tvalid_1's l2: 0.00867591\n[3500]\ttraining's l2: 0.00181311\tvalid_1's l2: 0.00866178\n[3600]\ttraining's l2: 0.00175081\tvalid_1's l2: 0.00865075\n[3700]\ttraining's l2: 0.00168988\tvalid_1's l2: 0.00863439\n[3800]\ttraining's l2: 0.00163373\tvalid_1's l2: 0.00862468\n[3900]\ttraining's l2: 0.00157834\tvalid_1's l2: 0.0086101\n[4000]\ttraining's l2: 0.00152625\tvalid_1's l2: 0.0086027\n[4100]\ttraining's l2: 0.00147688\tvalid_1's l2: 0.00859589\n[4200]\ttraining's l2: 0.00143005\tvalid_1's l2: 0.00858928\n[4300]\ttraining's l2: 0.00138364\tvalid_1's l2: 0.00857973\n[4400]\ttraining's l2: 0.00133838\tvalid_1's l2: 0.00857178\n[4500]\ttraining's l2: 0.00129616\tvalid_1's l2: 0.00856608\n[4600]\ttraining's l2: 0.00125611\tvalid_1's l2: 0.00856281\n[4700]\ttraining's l2: 0.00121704\tvalid_1's l2: 0.00855611\n[4800]\ttraining's l2: 0.00117944\tvalid_1's l2: 0.00855137\n[4900]\ttraining's l2: 0.00114254\tvalid_1's l2: 0.00854567\n[5000]\ttraining's l2: 0.00110816\tvalid_1's l2: 0.00854082\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00110816\tvalid_1's l2: 0.00854082\n[LightGBM] [Info] Total Bins 67366\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.661627\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0204633\tvalid_1's l2: 0.0230685\n[200]\ttraining's l2: 0.016063\tvalid_1's l2: 0.0189876\n[300]\ttraining's l2: 0.0131261\tvalid_1's l2: 0.0163261\n[400]\ttraining's l2: 0.0110844\tvalid_1's l2: 0.0145485\n[500]\ttraining's l2: 0.00959483\tvalid_1's l2: 0.013333\n[600]\ttraining's l2: 0.00849053\tvalid_1's l2: 0.0124776\n[700]\ttraining's l2: 0.00758941\tvalid_1's l2: 0.0118099\n[800]\ttraining's l2: 0.00685992\tvalid_1's l2: 0.0112891\n[900]\ttraining's l2: 0.00626225\tvalid_1's l2: 0.0108662\n[1000]\ttraining's l2: 0.0057634\tvalid_1's l2: 0.0105302\n[1100]\ttraining's l2: 0.00534277\tvalid_1's l2: 0.0102616\n[1200]\ttraining's l2: 0.00498827\tvalid_1's l2: 0.0100492\n[1300]\ttraining's l2: 0.00468544\tvalid_1's l2: 0.00987083\n[1400]\ttraining's l2: 0.00441035\tvalid_1's l2: 0.0097273\n[1500]\ttraining's l2: 0.00417397\tvalid_1's l2: 0.009615\n[1600]\ttraining's l2: 0.0039595\tvalid_1's l2: 0.00951189\n[1700]\ttraining's l2: 0.00376357\tvalid_1's l2: 0.00942297\n[1800]\ttraining's l2: 0.00358578\tvalid_1's l2: 0.00933949\n[1900]\ttraining's l2: 0.00342266\tvalid_1's l2: 0.00927558\n[2000]\ttraining's l2: 0.00326446\tvalid_1's l2: 0.00919978\n[2100]\ttraining's l2: 0.00312174\tvalid_1's l2: 0.00915145\n[2200]\ttraining's l2: 0.00299295\tvalid_1's l2: 0.00910712\n[2300]\ttraining's l2: 0.00286938\tvalid_1's l2: 0.00906149\n[2400]\ttraining's l2: 0.00275592\tvalid_1's l2: 0.00902277\n[2500]\ttraining's l2: 0.00264887\tvalid_1's l2: 0.00899243\n[2600]\ttraining's l2: 0.00254635\tvalid_1's l2: 0.0089606\n[2700]\ttraining's l2: 0.00244858\tvalid_1's l2: 0.00892842\n[2800]\ttraining's l2: 0.00235543\tvalid_1's l2: 0.00889704\n[2900]\ttraining's l2: 0.00226924\tvalid_1's l2: 0.00887258\n[3000]\ttraining's l2: 0.00218808\tvalid_1's l2: 0.00885417\n[3100]\ttraining's l2: 0.00210852\tvalid_1's l2: 0.00883047\n[3200]\ttraining's l2: 0.00203342\tvalid_1's l2: 0.00881143\n[3300]\ttraining's l2: 0.00196154\tvalid_1's l2: 0.00879177\n[3400]\ttraining's l2: 0.00189398\tvalid_1's l2: 0.00877224\n[3500]\ttraining's l2: 0.00183017\tvalid_1's l2: 0.00876126\n[3600]\ttraining's l2: 0.00176789\tvalid_1's l2: 0.00874925\n[3700]\ttraining's l2: 0.00170752\tvalid_1's l2: 0.00873809\n[3800]\ttraining's l2: 0.00164898\tvalid_1's l2: 0.00872516\n[3900]\ttraining's l2: 0.00159343\tvalid_1's l2: 0.00871147\n[4000]\ttraining's l2: 0.00153986\tvalid_1's l2: 0.00869686\n[4100]\ttraining's l2: 0.00148887\tvalid_1's l2: 0.0086863\n[4200]\ttraining's l2: 0.00144016\tvalid_1's l2: 0.00867617\n[4300]\ttraining's l2: 0.00139496\tvalid_1's l2: 0.0086713\n[4400]\ttraining's l2: 0.00135057\tvalid_1's l2: 0.00866417\n[4500]\ttraining's l2: 0.00130821\tvalid_1's l2: 0.00865958\n[4600]\ttraining's l2: 0.00126685\tvalid_1's l2: 0.00865445\n[4700]\ttraining's l2: 0.00122797\tvalid_1's l2: 0.0086468\n[4800]\ttraining's l2: 0.00119042\tvalid_1's l2: 0.00863938\n[4900]\ttraining's l2: 0.00115357\tvalid_1's l2: 0.00863202\n[5000]\ttraining's l2: 0.00111837\tvalid_1's l2: 0.00862697\nDid not meet early stopping. Best iteration is:\n[4996]\ttraining's l2: 0.00111986\tvalid_1's l2: 0.00862634\n[LightGBM] [Info] Total Bins 67373\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 3076\n[LightGBM] [Info] Start training from score 0.659132\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0210529\tvalid_1's l2: 0.02047\n[200]\ttraining's l2: 0.016415\tvalid_1's l2: 0.0171354\n[300]\ttraining's l2: 0.0133537\tvalid_1's l2: 0.0149871\n[400]\ttraining's l2: 0.011247\tvalid_1's l2: 0.0135839\n[500]\ttraining's l2: 0.00971136\tvalid_1's l2: 0.012603\n[600]\ttraining's l2: 0.00853975\tvalid_1's l2: 0.0118728\n[700]\ttraining's l2: 0.00761325\tvalid_1's l2: 0.0113079\n[800]\ttraining's l2: 0.00686963\tvalid_1's l2: 0.0108492\n[900]\ttraining's l2: 0.00627173\tvalid_1's l2: 0.0104697\n[1000]\ttraining's l2: 0.0057783\tvalid_1's l2: 0.0101818\n[1100]\ttraining's l2: 0.00536149\tvalid_1's l2: 0.00994994\n[1200]\ttraining's l2: 0.00500316\tvalid_1's l2: 0.00974995\n[1300]\ttraining's l2: 0.00469073\tvalid_1's l2: 0.00957827\n[1400]\ttraining's l2: 0.00441699\tvalid_1's l2: 0.00943474\n[1500]\ttraining's l2: 0.00417703\tvalid_1's l2: 0.00931684\n[1600]\ttraining's l2: 0.00395937\tvalid_1's l2: 0.00921577\n[1700]\ttraining's l2: 0.00375895\tvalid_1's l2: 0.00912403\n[1800]\ttraining's l2: 0.00358052\tvalid_1's l2: 0.00904467\n[1900]\ttraining's l2: 0.00342076\tvalid_1's l2: 0.00898733\n[2000]\ttraining's l2: 0.00326941\tvalid_1's l2: 0.00892668\n[2100]\ttraining's l2: 0.00313446\tvalid_1's l2: 0.00887876\n[2200]\ttraining's l2: 0.00300139\tvalid_1's l2: 0.00882732\n[2300]\ttraining's l2: 0.00287952\tvalid_1's l2: 0.00878853\n[2400]\ttraining's l2: 0.00276763\tvalid_1's l2: 0.00874955\n[2500]\ttraining's l2: 0.00266001\tvalid_1's l2: 0.00871923\n[2600]\ttraining's l2: 0.00255725\tvalid_1's l2: 0.00869053\n[2700]\ttraining's l2: 0.00246069\tvalid_1's l2: 0.00865751\n[2800]\ttraining's l2: 0.00236791\tvalid_1's l2: 0.00862918\n[2900]\ttraining's l2: 0.00228051\tvalid_1's l2: 0.0085979\n[3000]\ttraining's l2: 0.0021966\tvalid_1's l2: 0.00857275\n[3100]\ttraining's l2: 0.00211561\tvalid_1's l2: 0.00854785\n[3200]\ttraining's l2: 0.00204008\tvalid_1's l2: 0.00852831\n[3300]\ttraining's l2: 0.0019688\tvalid_1's l2: 0.00850806\n[3400]\ttraining's l2: 0.00190019\tvalid_1's l2: 0.00848699\n[3500]\ttraining's l2: 0.00183376\tvalid_1's l2: 0.00846713\n[3600]\ttraining's l2: 0.00177147\tvalid_1's l2: 0.00845817\n[3700]\ttraining's l2: 0.00171074\tvalid_1's l2: 0.00844128\n[3800]\ttraining's l2: 0.00165337\tvalid_1's l2: 0.00842655\n[3900]\ttraining's l2: 0.00159935\tvalid_1's l2: 0.00841261\n[4000]\ttraining's l2: 0.00154706\tvalid_1's l2: 0.0084019\n[4100]\ttraining's l2: 0.00149702\tvalid_1's l2: 0.00839006\n[4200]\ttraining's l2: 0.00144771\tvalid_1's l2: 0.00837573\n[4300]\ttraining's l2: 0.00140167\tvalid_1's l2: 0.00836815\n[4400]\ttraining's l2: 0.00135605\tvalid_1's l2: 0.00835827\n[4500]\ttraining's l2: 0.00131258\tvalid_1's l2: 0.00834956\n[4600]\ttraining's l2: 0.00127133\tvalid_1's l2: 0.00833928\n[4700]\ttraining's l2: 0.00123197\tvalid_1's l2: 0.00833118\n[4800]\ttraining's l2: 0.00119474\tvalid_1's l2: 0.00832491\n[4900]\ttraining's l2: 0.00115788\tvalid_1's l2: 0.00831752\n[5000]\ttraining's l2: 0.00112308\tvalid_1's l2: 0.00831001\nDid not meet early stopping. Best iteration is:\n[5000]\ttraining's l2: 0.00112308\tvalid_1's l2: 0.00831001\nLGB Model 1 CV MSE: 0.008518\nLGB Model 2 CV MSE: 0.008556\nXGB CV MSE: 0.008610\nCATBoost CV MSE: 0.010753\nEnsemble #1 (mean of LGBs + XGB + CAT) CV MSE: 0.008923\nEnsemble submission saved to submission_ensemble.csv\n","output_type":"stream"}],"execution_count":9}]}