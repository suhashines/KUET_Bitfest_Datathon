{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:17:24.086913Z","iopub.execute_input":"2024-12-26T18:17:24.087237Z","iopub.status.idle":"2024-12-26T18:17:29.488933Z","shell.execute_reply.started":"2024-12-26T18:17:24.087210Z","shell.execute_reply":"2024-12-26T18:17:29.487934Z"}},"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\nfrom sentence_transformers import SentenceTransformer\nfrom ast import literal_eval\nimport re\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:17:30.978842Z","iopub.execute_input":"2024-12-26T18:17:30.979173Z","iopub.status.idle":"2024-12-26T18:17:46.285684Z","shell.execute_reply.started":"2024-12-26T18:17:30.979146Z","shell.execute_reply":"2024-12-26T18:17:46.284993Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(lgbm.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:17:49.632075Z","iopub.execute_input":"2024-12-26T18:17:49.632852Z","iopub.status.idle":"2024-12-26T18:17:49.637569Z","shell.execute_reply.started":"2024-12-26T18:17:49.632816Z","shell.execute_reply":"2024-12-26T18:17:49.636436Z"}},"outputs":[{"name":"stdout","text":"4.5.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self):\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n    \n    def process_list(self, text):\n        try:\n            if pd.isna(text) or text == '':\n                return []\n            return literal_eval(text)\n        except:\n            return text.split(',')\n    \n    def get_embeddings(self, texts):\n        texts = [str(t) if not pd.isna(t) else '' for t in texts]\n        return self.model.encode(texts)\n\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.label_encoders = {}\n    \n    def extract_year(self, date_str):\n        if pd.isna(date_str):\n            return None\n        try:\n            return int(re.findall(r'\\d{4}', str(date_str))[0])\n        except:\n            return None\n    \n    def process_dates(self, df):\n        # Experience duration\n        df['experience_years'] = df.apply(\n            lambda x: self.extract_year(x['end_dates']) - self.extract_year(x['start_dates'])\n            if self.extract_year(x['end_dates']) and self.extract_year(x['start_dates'])\n            else 0, axis=1\n        )\n        return df\n    \n    def process_categorical(self, df, col):\n        if col not in self.label_encoders:\n            self.label_encoders[col] = LabelEncoder()\n            df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].fillna('MISSING'))\n        else:\n            df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].fillna('MISSING'))\n        return df\n    \n    def transform(self, df):\n        # Process dates\n        df = self.process_dates(df)\n        \n        # Process categorical\n        for col in ['degree_names', 'result_types', 'major_field_of_studies']:\n            df = self.process_categorical(df, col)\n        \n        # Get embeddings for text features\n        text_features = ['skills', 'career_objective', 'responsibilities']\n        embedding_cols = {}\n        \n        for feature in text_features:\n            embeddings = self.text_processor.get_embeddings(df[feature])\n            embedding_cols.update({\n                f'{feature}_emb_{i}': embeddings[:, i] \n                for i in range(embeddings.shape[1])\n            })\n        \n        # Concatenate all embeddings at once\n        embedding_df = pd.DataFrame(embedding_cols, index=df.index)\n        df = pd.concat([df, embedding_df], axis=1)\n        \n        # Skills matching score\n        df['skills_required'] = df['skills_required'].fillna('')\n        df['skills'] = df['skills'].fillna('')\n        required_skills = df['skills_required'].apply(self.text_processor.process_list)\n        candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n        df['skills_match_ratio'] = [\n            len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n            for req, cand in zip(required_skills, candidate_skills)\n        ]\n        \n        return df\n\ndef train_model():\n    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n    \n    fe = FeatureEngineer()\n    \n    print(\"Transforming train data...\")\n    train_df = fe.transform(train_df)\n    print(\"Transforming test data...\")\n    test_df = fe.transform(test_df)\n    \n    # Select features\n    text_cols = ['skills', 'career_objective', 'responsibilities']\n    emb_cols = [col for col in train_df.columns if 'emb_' in col]\n    cat_cols = [col for col in train_df.columns if 'encoded' in col]\n    num_cols = ['experience_years', 'skills_match_ratio']\n    \n    feature_cols = emb_cols + cat_cols + num_cols\n    \n    params = {\n        'objective': 'regression_l2',\n        'metric': 'l2',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.9,\n        'max_depth': 8,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1\n    }\n    \n    # Cross-validation\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = []\n    test_preds = np.zeros(len(test_df))\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n        print(f\"Training fold {fold + 1}\")\n        X_train = train_df.iloc[train_idx][feature_cols]\n        y_train = train_df.iloc[train_idx]['matched_score']\n        X_val = train_df.iloc[val_idx][feature_cols]\n        y_val = train_df.iloc[val_idx]['matched_score']\n        \n        train_data = lgbm.Dataset(X_train, label=y_train)\n        val_data = lgbm.Dataset(X_val, label=y_val)\n        \n        model = lgbm.train(\n            params,\n            train_data,\n            num_boost_round=1000,\n            valid_sets=[train_data, val_data],\n            # early_stopping_round=50,\n            callbacks=[\n                lgbm.early_stopping(stopping_rounds=50),\n                lgbm.log_evaluation(100)\n            ]\n            # verbose_eval=100\n            # **callbacks=[lgb.early_stopping_rounds(stopping_rounds=50), lgb.log_evaluation(100)]**\n        )\n        \n        val_preds = model.predict(X_val)\n        fold_score = mean_squared_error(y_val, val_preds)\n        cv_scores.append(fold_score)\n        \n        test_preds += model.predict(test_df[feature_cols]) / kf.n_splits\n    \n    print(f\"CV MSE: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n    \n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_preds\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:45:43.110304Z","iopub.execute_input":"2024-12-26T18:45:43.110634Z","iopub.status.idle":"2024-12-26T18:45:43.127300Z","shell.execute_reply.started":"2024-12-26T18:45:43.110610Z","shell.execute_reply":"2024-12-26T18:45:43.126426Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T18:45:47.189909Z","iopub.execute_input":"2024-12-26T18:45:47.190190Z","iopub.status.idle":"2024-12-26T18:48:28.429071Z","shell.execute_reply.started":"2024-12-26T18:45:47.190168Z","shell.execute_reply":"2024-12-26T18:48:28.428307Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Transforming train data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/239 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b391ed68f0c479d9f0095a8d84d7c66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/239 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3add4427a7ce43debb318303031f55db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/239 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ed3091ced04707850ccc4731a706c3"}},"metadata":{}},{"name":"stdout","text":"Transforming test data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"930ccec9ffc44ef39e73689e6c0162e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc2d4e8e64e64b57a650eb5c00e3c017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02a4812f2024795b5cd87da912d1f79"}},"metadata":{}},{"name":"stdout","text":"Training fold 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065770 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 187170\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1156\n[LightGBM] [Info] Start training from score 0.658422\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00562275\tvalid_1's l2: 0.00982499\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[200]\ttraining's l2: 0.00365696\tvalid_1's l2: 0.00893094\n[300]\ttraining's l2: 0.00259898\tvalid_1's l2: 0.0086548\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[400]\ttraining's l2: 0.00187446\tvalid_1's l2: 0.00853791\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[500]\ttraining's l2: 0.00139779\tvalid_1's l2: 0.00845645\n[600]\ttraining's l2: 0.0010443\tvalid_1's l2: 0.00837287\n[700]\ttraining's l2: 0.000783498\tvalid_1's l2: 0.00832066\nEarly stopping, best iteration is:\n[728]\ttraining's l2: 0.000726093\tvalid_1's l2: 0.00831204\nTraining fold 2\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064327 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 188046\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1156\n[LightGBM] [Info] Start training from score 0.661442\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00550839\tvalid_1's l2: 0.00958032\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[200]\ttraining's l2: 0.00355835\tvalid_1's l2: 0.0087011\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttraining's l2: 0.00257034\tvalid_1's l2: 0.00849336\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[400]\ttraining's l2: 0.00188937\tvalid_1's l2: 0.00835669\n[500]\ttraining's l2: 0.00139238\tvalid_1's l2: 0.00831952\nEarly stopping, best iteration is:\n[515]\ttraining's l2: 0.00133125\tvalid_1's l2: 0.00831188\nTraining fold 3\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067315 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 188058\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1156\n[LightGBM] [Info] Start training from score 0.661377\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\ttraining's l2: 0.00556952\tvalid_1's l2: 0.00899142\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[200]\ttraining's l2: 0.00364906\tvalid_1's l2: 0.00815896\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttraining's l2: 0.00262856\tvalid_1's l2: 0.00792492\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[400]\ttraining's l2: 0.00191645\tvalid_1's l2: 0.0077961\n[500]\ttraining's l2: 0.00142024\tvalid_1's l2: 0.00775297\nEarly stopping, best iteration is:\n[504]\ttraining's l2: 0.00140456\tvalid_1's l2: 0.00775092\nTraining fold 4\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 187321\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1156\n[LightGBM] [Info] Start training from score 0.661045\nTraining until validation scores don't improve for 50 rounds\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[100]\ttraining's l2: 0.00554932\tvalid_1's l2: 0.00997289\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[200]\ttraining's l2: 0.00359989\tvalid_1's l2: 0.00912935\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttraining's l2: 0.00260796\tvalid_1's l2: 0.00879163\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[400]\ttraining's l2: 0.00191787\tvalid_1's l2: 0.00865868\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[500]\ttraining's l2: 0.00144203\tvalid_1's l2: 0.00862666\n[600]\ttraining's l2: 0.00109111\tvalid_1's l2: 0.00860217\nEarly stopping, best iteration is:\n[584]\ttraining's l2: 0.00114204\tvalid_1's l2: 0.00859819\nTraining fold 5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071814 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 187138\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1156\n[LightGBM] [Info] Start training from score 0.661050\nTraining until validation scores don't improve for 50 rounds\n[100]\ttraining's l2: 0.00570454\tvalid_1's l2: 0.00907085\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[200]\ttraining's l2: 0.00371251\tvalid_1's l2: 0.00817871\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[300]\ttraining's l2: 0.00270021\tvalid_1's l2: 0.00788691\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[400]\ttraining's l2: 0.00195326\tvalid_1's l2: 0.00774536\n[500]\ttraining's l2: 0.00145298\tvalid_1's l2: 0.00766908\n[600]\ttraining's l2: 0.00109574\tvalid_1's l2: 0.00763309\n[700]\ttraining's l2: 0.000821755\tvalid_1's l2: 0.0075793\nEarly stopping, best iteration is:\n[674]\ttraining's l2: 0.000878795\tvalid_1's l2: 0.00757392\nCV MSE: 0.008109 ± 0.000384\nSubmission saved to submission.csv\n","output_type":"stream"}],"execution_count":18}]}