{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90798,"databundleVersionId":10606811,"sourceType":"competition"}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n# Load the train and test datasets\nnew_train_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/train.csv\")\nnew_test_data = pd.read_csv(\"/kaggle/input/bitfest-datathon-2025/test.csv\")\n\n# Check basic structure of the datasets\nprint(\"Train Dataset Overview:\\n\")\nprint(new_train_data.info())\nprint(\"\\nTest Dataset Overview:\\n\")\nprint(new_test_data.info())\n\nnew_train_data['responsibilities']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:28:13.703184Z","iopub.execute_input":"2024-12-27T06:28:13.703521Z","iopub.status.idle":"2024-12-27T06:28:14.282531Z","shell.execute_reply.started":"2024-12-27T06:28:13.703499Z","shell.execute_reply":"2024-12-27T06:28:14.281827Z"}},"outputs":[{"name":"stdout","text":"Train Dataset Overview:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7635 entries, 0 to 7634\nData columns (total 35 columns):\n #   Column                               Non-Null Count  Dtype  \n---  ------                               --------------  -----  \n 0   address                              633 non-null    object \n 1   career_objective                     3794 non-null   object \n 2   skills                               7592 non-null   object \n 3   educational_institution_name         7574 non-null   object \n 4   degree_names                         7574 non-null   object \n 5   passing_years                        7574 non-null   object \n 6   educational_results                  7574 non-null   object \n 7   result_types                         7574 non-null   object \n 8   major_field_of_studies               7574 non-null   object \n 9   professional_company_names           7568 non-null   object \n 10  company_urls                         7568 non-null   object \n 11  start_dates                          7568 non-null   object \n 12  end_dates                            7568 non-null   object \n 13  related_skils_in_job                 7568 non-null   object \n 14  positions                            7568 non-null   object \n 15  locations                            7568 non-null   object \n 16  responsibilities                     7635 non-null   object \n 17  extra_curricular_activity_types      2732 non-null   object \n 18  extra_curricular_organization_names  2732 non-null   object \n 19  extra_curricular_organization_links  2732 non-null   object \n 20  role_positions                       2732 non-null   object \n 21  languages                            569 non-null    object \n 22  proficiency_levels                   569 non-null    object \n 23  certification_providers              1587 non-null   object \n 24  certification_skills                 1587 non-null   object \n 25  online_links                         1587 non-null   object \n 26  issue_dates                          1587 non-null   object \n 27  expiry_dates                         1587 non-null   object \n 28  ﻿job_position_name                   7635 non-null   object \n 29  educationaL_requirements             7635 non-null   object \n 30  experiencere_requirement             6549 non-null   object \n 31  age_requirement                      4371 non-null   object \n 32  responsibilities.1                   7635 non-null   object \n 33  skills_required                      6264 non-null   object \n 34  matched_score                        7635 non-null   float64\ndtypes: float64(1), object(34)\nmemory usage: 2.0+ MB\nNone\n\nTest Dataset Overview:\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1909 entries, 0 to 1908\nData columns (total 35 columns):\n #   Column                               Non-Null Count  Dtype \n---  ------                               --------------  ----- \n 0   ID                                   1909 non-null   int64 \n 1   address                              151 non-null    object\n 2   career_objective                     946 non-null    object\n 3   skills                               1896 non-null   object\n 4   educational_institution_name         1886 non-null   object\n 5   degree_names                         1886 non-null   object\n 6   passing_years                        1886 non-null   object\n 7   educational_results                  1886 non-null   object\n 8   result_types                         1886 non-null   object\n 9   major_field_of_studies               1886 non-null   object\n 10  professional_company_names           1892 non-null   object\n 11  company_urls                         1892 non-null   object\n 12  start_dates                          1892 non-null   object\n 13  end_dates                            1892 non-null   object\n 14  related_skils_in_job                 1892 non-null   object\n 15  positions                            1892 non-null   object\n 16  locations                            1892 non-null   object\n 17  responsibilities                     1909 non-null   object\n 18  extra_curricular_activity_types      694 non-null    object\n 19  extra_curricular_organization_names  694 non-null    object\n 20  extra_curricular_organization_links  694 non-null    object\n 21  role_positions                       694 non-null    object\n 22  languages                            131 non-null    object\n 23  proficiency_levels                   131 non-null    object\n 24  certification_providers              421 non-null    object\n 25  certification_skills                 421 non-null    object\n 26  online_links                         421 non-null    object\n 27  issue_dates                          421 non-null    object\n 28  expiry_dates                         421 non-null    object\n 29  ﻿job_position_name                   1909 non-null   object\n 30  educationaL_requirements             1909 non-null   object\n 31  experiencere_requirement             1631 non-null   object\n 32  age_requirement                      1086 non-null   object\n 33  responsibilities.1                   1909 non-null   object\n 34  skills_required                      1579 non-null   object\ndtypes: int64(1), object(34)\nmemory usage: 522.1+ KB\nNone\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0       Technical Support\\nTroubleshooting\\nCollaborat...\n1       Machine Learning Leadership\\nCross-Functional ...\n2       Trade Marketing Executive\\nBrand Visibility, S...\n3       Apparel Sourcing\\nQuality Garment Sourcing\\nRe...\n4       iOS Lifecycle\\nRequirement Analysis\\nNative Fr...\n                              ...                        \n7630    15+ Years Banking Experience\\nAudit/Inspection...\n7631    Data Platform Design\\nData Pipeline Developmen...\n7632    Mikrotik Router Configuration\\nOLT Device Setu...\n7633    Machinery Maintenance\\nTroubleshooting\\nReport...\n7634    Machinery Maintenance\\nTroubleshooting\\nReport...\nName: responsibilities, Length: 7635, dtype: object"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nimport re\n\n# Step 1: Define column groups based on type and relevance\ntext_columns = ['address','career_objective','locations', 'extra_curricular_activity_types','extra_curricular_organization_links','online_links']\nnumerical_columns = ['matched_score']  # Additional numeric columns\ndate_columns = ['start_dates', 'end_dates', 'issue_dates', 'expiry_dates']\n# Define columns for KNN Imputation\nknn_columns = ['age_requirement','experiencere_requirement']  # Add other columns if needed\n# Get all column names in the DataFrame\nall_columns = set(new_train_data.columns)\n\n# Combine all defined column groups\ndefined_columns = set(text_columns + numerical_columns + date_columns + knn_columns)\n\n# Identify columns that are not in the defined groups\ncategorical_columns = list(all_columns - defined_columns)\n\n# Step 2: Preprocessing Function for Age Column\ndef preprocess_age_requirement(column):\n    # Extract numeric ranges and replace non-numeric with NaN\n    def extract_mean_age(val):\n        if isinstance(val, str):\n            # Find ranges like \"Age 25 to 35 years\" and compute the mean\n            match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', val)\n            if match:\n                return (int(match.group(1)) + int(match.group(2))) / 2\n            # Find single ages like \"Age 25 years\"\n            match = re.search(r'(\\d+)', val)\n            if match:\n                return int(match.group(1))\n        return None  # Return None for non-numeric values\n\n    return column.apply(extract_mean_age)\n\ndef preprocess_experience_requirement(column):\n    \"\"\"\n    Preprocess the experience_requirement column by extracting numeric ranges or single values\n    and replacing non-numeric entries with NaN.\n    \"\"\"\n    def extract_mean_experience(val):\n        if isinstance(val, str):\n            # Find ranges like \"3 to 5 years\" and compute the mean\n            range_match = re.search(r'(\\d+)\\s*to\\s*(\\d+)', val)\n            if range_match:\n                return (int(range_match.group(1)) + int(range_match.group(2))) / 2\n            \n            # Find \"At least X year(s)\" or similar patterns\n            at_least_match = re.search(r'At least (\\d+)', val)\n            if at_least_match:\n                return int(at_least_match.group(1))\n            \n            # Find single experience values like \"1 year\" or \"2 year(s)\"\n            single_match = re.search(r'(\\d+)', val)\n            if single_match:\n                return int(single_match.group(1))\n        \n        # Return None for non-numeric or unprocessable entries\n        return None\n\n    # Apply the extraction logic to the entire column\n    return column.apply(extract_mean_experience)\n\n# Step 3: Preprocess Age Requirement\nnew_train_data['age_requirement'] = preprocess_age_requirement(new_train_data['age_requirement'])\nnew_test_data['age_requirement'] = preprocess_age_requirement(new_test_data['age_requirement'])\nnew_train_data['experiencere_requirement'] = preprocess_experience_requirement(new_train_data['experiencere_requirement'])\nnew_test_data['experiencere_requirement'] = preprocess_experience_requirement(new_test_data['experiencere_requirement'])\n\n# Step 4: Impute Categorical Columns\nfor col in categorical_columns:\n    mode_value = new_train_data[col].mode()[0] if not new_train_data[col].mode().empty else \"Not Specified\"\n    new_train_data[col].fillna(mode_value, inplace=True)\n    new_test_data[col].fillna(mode_value, inplace=True)\n\n# Step 5: Impute Text Columns with Placeholder\nfor col in text_columns:\n    new_train_data[col].fillna(\"No Information\", inplace=True)\n    new_test_data[col].fillna(\"No Information\", inplace=True)\n\n\n\nfor col in numerical_columns:\n    if col in new_train_data.columns:  # Check if column exists in train data\n        median_value = new_train_data[col].median()\n        new_train_data[col].fillna(median_value, inplace=True)\n    if col in new_test_data.columns:  # Check if column exists in test data\n        median_value = new_train_data[col].median()  # Use train data's median for consistency\n        new_test_data[col].fillna(median_value, inplace=True)\n    else:\n        print(f\"'{col}' not found in test data.\")\n\n# Step 7: Handle Date Columns (Fill with placeholder or special handling)\nfor col in date_columns:\n    new_train_data[col].fillna(\"Unknown Date\", inplace=True)\n    new_test_data[col].fillna(\"Unknown Date\", inplace=True)\n\n# Ensure the selected columns are numeric\nfor col in knn_columns:\n    new_train_data[col] = pd.to_numeric(new_train_data[col], errors='coerce')\n    new_test_data[col] = pd.to_numeric(new_test_data[col], errors='coerce')\n\n# Check for all-NaN columns and fill temporarily\nfor col in knn_columns:\n    if new_train_data[col].isnull().all():\n        new_train_data[col].fillna(0, inplace=True)  # Replace with a temporary value\n    if new_test_data[col].isnull().all():\n        new_test_data[col].fillna(0, inplace=True)  # Replace with a temporary value\n\n# Initialize the KNN Imputer\nimputer = KNNImputer(n_neighbors=5)\n\n# Apply KNN Imputation on Train Data\nknn_train_data = pd.DataFrame(\n    imputer.fit_transform(new_train_data[knn_columns]),\n    columns=knn_columns,\n    index=new_train_data.index\n)\nnew_train_data[knn_columns] = knn_train_data\n\n# Apply KNN Imputation on Test Data\nknn_test_data = pd.DataFrame(\n    imputer.transform(new_test_data[knn_columns]),\n    columns=knn_columns,\n    index=new_test_data.index\n)\nnew_test_data[knn_columns] = knn_test_data\n\nprint(\"KNN Imputation applied successfully!\")\n\n\n# Step 9: Create Indicator Columns for Missing Data\nfor col in new_train_data.columns:\n    if new_train_data[col].isnull().any():\n        new_train_data[f'{col}_missing'] = new_train_data[col].isnull().astype(int)\n        new_test_data[f'{col}_missing'] = new_test_data[col].isnull().astype(int)\n\n# Print completion message\nprint(\"Missing values handled successfully with a refined strategy.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:28:19.541065Z","iopub.execute_input":"2024-12-27T06:28:19.541421Z","iopub.status.idle":"2024-12-27T06:28:21.888045Z","shell.execute_reply.started":"2024-12-27T06:28:19.541393Z","shell.execute_reply":"2024-12-27T06:28:21.887234Z"}},"outputs":[{"name":"stdout","text":"'matched_score' not found in test data.\nKNN Imputation applied successfully!\nMissing values handled successfully with a refined strategy.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\nimport re\nfrom ast import literal_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:55:47.534576Z","iopub.execute_input":"2024-12-27T06:55:47.534903Z","iopub.status.idle":"2024-12-27T06:55:47.539425Z","shell.execute_reply.started":"2024-12-27T06:55:47.534880Z","shell.execute_reply":"2024-12-27T06:55:47.538360Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# import sklearn\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# print(sklearn.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T05:50:41.074117Z","iopub.execute_input":"2024-12-27T05:50:41.074532Z","iopub.status.idle":"2024-12-27T05:50:41.080852Z","shell.execute_reply.started":"2024-12-27T05:50:41.074505Z","shell.execute_reply":"2024-12-27T05:50:41.079718Z"}},"outputs":[{"name":"stdout","text":"1.2.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class TextProcessor:\n    def __init__(self, max_features=200):\n        self.tfidf_models = {}\n        self.count_models = {}\n        self.svd_models = {}\n        self.max_features = max_features\n    \n    def clean_text(self, text):\n        if pd.isna(text):\n            return ''\n        text = str(text).lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    \n    def process_list(self, text):\n        if pd.isna(text) or text == '':\n            return []\n        try:\n            items = literal_eval(text)\n            return [self.clean_text(item) for item in items]\n        except:\n            return [self.clean_text(item) for item in str(text).split(',')]\n\n    def fit_transform_text(self, texts, feature_name):\n        processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n        # TF-IDF features\n        self.tfidf_models[feature_name] = TfidfVectorizer(\n            max_features=self.max_features,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        tfidf_matrix = self.tfidf_models[feature_name].fit_transform(processed_texts)\n        \n        # Count features\n        self.count_models[feature_name] = CountVectorizer(\n            max_features=self.max_features//2,\n            ngram_range=(1, 2),\n            stop_words='english'\n        )\n        count_matrix = self.count_models[feature_name].fit_transform(processed_texts)\n        \n        # Reduce dimensionality\n        self.svd_models[feature_name] = TruncatedSVD(n_components=50)\n        svd_matrix = self.svd_models[feature_name].fit_transform(tfidf_matrix)\n        \n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix\n        ])\n\n    def transform_text(self, texts, feature_name):\n        processed_texts = [' '.join(self.process_list(text)) if isinstance(text, str) else '' for text in texts]\n        \n        tfidf_matrix = self.tfidf_models[feature_name].transform(processed_texts)\n        count_matrix = self.count_models[feature_name].transform(processed_texts)\n        svd_matrix = self.svd_models[feature_name].transform(tfidf_matrix)\n        \n        return np.hstack([\n            tfidf_matrix.toarray(),\n            count_matrix.toarray(),\n            svd_matrix\n        ])\n\nclass FeatureEngineer:\n    def __init__(self):\n        self.text_processor = TextProcessor()\n        self.label_encoders = {}\n        self.scaler = StandardScaler()\n        \n    def extract_years_experience(self, row):\n        try:\n            start_years = [int(y) for y in re.findall(r'\\d{4}', str(row['start_dates']))]\n            end_years = [int(y) for y in re.findall(r'\\d{4}', str(row['end_dates']))]\n            if not end_years:\n                end_years = [2024]  # Current year for ongoing positions\n            return sum(e - s for s, e in zip(start_years, end_years))\n        except:\n            return 0\n    \n    def extract_education_level(self, degree):\n        if pd.isna(degree):\n            return 0\n        degree = str(degree).lower()\n        if 'phd' in degree or 'doctorate' in degree:\n            return 4\n        elif 'master' in degree:\n            return 3\n        elif 'bachelor' in degree or 'bsc' in degree or 'ba' in degree:\n            return 2\n        elif 'diploma' in degree or 'certificate' in degree:\n            return 1\n        return 0\n\n    def transform(self, df, is_train=True):\n        feature_dict = {}\n        \n        # Experience features\n        feature_dict['total_experience'] = df.apply(self.extract_years_experience, axis=1)\n        feature_dict['education_level'] = df['degree_names'].apply(self.extract_education_level)\n        feature_dict['num_companies'] = df['professional_company_names'].str.count(',').fillna(0) + 1\n        feature_dict['num_skills'] = df['skills'].str.count(',').fillna(0) + 1\n        feature_dict['has_certification'] = (~df['certification_skills'].isna()).astype(int)\n        \n        # Process text features\n        text_features = ['skills', 'career_objective', 'responsibilities', 'educational_institution_name']\n        all_text_features = {}\n        \n        for feature in text_features:\n            if is_train:\n                text_matrix = self.text_processor.fit_transform_text(df[feature], feature)\n            else:\n                text_matrix = self.text_processor.transform_text(df[feature], feature)\n            \n            for i in range(text_matrix.shape[1]):\n                all_text_features[f'{feature}_text_{i}'] = text_matrix[:, i]\n        \n        # Skills matching scores\n        df['skills_required'] = df['skills_required'].fillna('')\n        df['skills'] = df['skills'].fillna('')\n        required_skills = df['skills_required'].apply(self.text_processor.process_list)\n        candidate_skills = df['skills'].apply(self.text_processor.process_list)\n        \n        feature_dict['skills_match_ratio'] = [\n            len(set(req).intersection(set(cand))) / len(set(req)) if len(set(req)) > 0 else 0\n            for req, cand in zip(required_skills, candidate_skills)\n        ]\n        \n        # Convert features to DataFrame\n        feature_df = pd.DataFrame(feature_dict, index=df.index)\n        text_feature_df = pd.DataFrame(all_text_features, index=df.index)\n        \n        # Scale numerical features\n        if is_train:\n            feature_df = pd.DataFrame(\n                self.scaler.fit_transform(feature_df),\n                columns=feature_df.columns,\n                index=feature_df.index\n            )\n        else:\n            feature_df = pd.DataFrame(\n                self.scaler.transform(feature_df),\n                columns=feature_df.columns,\n                index=feature_df.index\n            )\n        \n        return pd.concat([feature_df, text_feature_df], axis=1)\n\ndef train_model():\n    train_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/train.csv')\n    test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n    fe = FeatureEngineer()\n\n    print(\"Transforming train data...\")\n    train_features = fe.transform(train_df, is_train=True)\n    print(\"Transforming test data...\")\n    test_features = fe.transform(test_df, is_train=False)\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = []\n    test_preds = np.zeros(len(test_df))\n\n    params = {\n        'objective': 'regression_l2',\n        'metric': 'l2',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 5,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'min_child_samples': 20\n    }\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features)):\n        print(f\"Training fold {fold + 1}\")\n        X_train = train_features.iloc[train_idx]\n        y_train = train_df.iloc[train_idx]['matched_score']\n        X_val = train_features.iloc[val_idx]\n        y_val = train_df.iloc[val_idx]['matched_score']\n\n        train_data = lgbm.Dataset(X_train, label=y_train)\n        val_data = lgbm.Dataset(X_val, label=y_val)\n        \n        model = lgbm.train(\n            params,\n            train_data,\n            num_boost_round=2000,\n            valid_sets=[train_data, val_data],\n            callbacks=[\n                lgbm.early_stopping(stopping_rounds=100),\n                lgbm.log_evaluation(100)\n            ]\n        )\n\n        val_preds = model.predict(X_val)\n        fold_score = mean_squared_error(y_val, val_preds)\n        cv_scores.append(fold_score)\n        \n        test_preds += model.predict(test_features) / kf.n_splits\n\n    print(f\"CV MSE: {np.mean(cv_scores):.6f} ± {np.std(cv_scores):.6f}\")\n\n    submission = pd.DataFrame({\n        'ID': test_df['ID'],\n        'matched_score': test_preds\n    })\n    submission.to_csv('submission.csv', index=False)\n    print(\"Submission saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:56:37.927038Z","iopub.execute_input":"2024-12-27T06:56:37.927341Z","iopub.status.idle":"2024-12-27T06:56:37.947909Z","shell.execute_reply.started":"2024-12-27T06:56:37.927321Z","shell.execute_reply":"2024-12-27T06:56:37.946900Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\n# test_df = pd.read_csv('/kaggle/input/bitfest-datathon-2025/test.csv')\n\n# # Initialize feature engineer\n# fe = FeatureEngineer()\n\n# print(\"Transforming test data...\")\n# test_df = fe.transform(test_df, is_train=False)\n# print(test_df[feature_cols].shape)\n# print(type(test_df[feature_cols]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T06:56:41.213567Z","iopub.execute_input":"2024-12-27T06:56:41.213921Z","iopub.status.idle":"2024-12-27T06:59:45.852247Z","shell.execute_reply.started":"2024-12-27T06:56:41.213894Z","shell.execute_reply":"2024-12-27T06:59:45.851604Z"}},"outputs":[{"name":"stdout","text":"Transforming train data...\nTransforming test data...\nTraining fold 1\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055748 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44568\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1387\n[LightGBM] [Info] Start training from score 0.658422\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0144699\tvalid_1's l2: 0.0157499\n[200]\ttraining's l2: 0.0103138\tvalid_1's l2: 0.0128671\n[300]\ttraining's l2: 0.0082846\tvalid_1's l2: 0.0115175\n[400]\ttraining's l2: 0.00706778\tvalid_1's l2: 0.0107158\n[500]\ttraining's l2: 0.00630446\tvalid_1's l2: 0.0102996\n[600]\ttraining's l2: 0.00575072\tvalid_1's l2: 0.0100355\n[700]\ttraining's l2: 0.0053028\tvalid_1's l2: 0.00984222\n[800]\ttraining's l2: 0.00487916\tvalid_1's l2: 0.00966644\n[900]\ttraining's l2: 0.00452918\tvalid_1's l2: 0.0095147\n[1000]\ttraining's l2: 0.00422926\tvalid_1's l2: 0.00942945\n[1100]\ttraining's l2: 0.00395968\tvalid_1's l2: 0.00935812\n[1200]\ttraining's l2: 0.00372425\tvalid_1's l2: 0.00930792\n[1300]\ttraining's l2: 0.00348199\tvalid_1's l2: 0.00923845\n[1400]\ttraining's l2: 0.00327485\tvalid_1's l2: 0.00918742\n[1500]\ttraining's l2: 0.00307591\tvalid_1's l2: 0.00915064\n[1600]\ttraining's l2: 0.00289564\tvalid_1's l2: 0.00909713\n[1700]\ttraining's l2: 0.0027259\tvalid_1's l2: 0.00905602\n[1800]\ttraining's l2: 0.00257285\tvalid_1's l2: 0.0090346\n[1900]\ttraining's l2: 0.00243093\tvalid_1's l2: 0.00902001\n[2000]\ttraining's l2: 0.00229455\tvalid_1's l2: 0.00898758\nDid not meet early stopping. Best iteration is:\n[2000]\ttraining's l2: 0.00229455\tvalid_1's l2: 0.00898758\nTraining fold 2\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.048102 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44573\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1385\n[LightGBM] [Info] Start training from score 0.661442\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0145708\tvalid_1's l2: 0.0158039\n[200]\ttraining's l2: 0.0103534\tvalid_1's l2: 0.0123716\n[300]\ttraining's l2: 0.00830443\tvalid_1's l2: 0.0108598\n[400]\ttraining's l2: 0.00714621\tvalid_1's l2: 0.0101265\n[500]\ttraining's l2: 0.00632364\tvalid_1's l2: 0.00967267\n[600]\ttraining's l2: 0.00574607\tvalid_1's l2: 0.00940608\n[700]\ttraining's l2: 0.00528824\tvalid_1's l2: 0.00921991\n[800]\ttraining's l2: 0.00488794\tvalid_1's l2: 0.00905037\n[900]\ttraining's l2: 0.00454449\tvalid_1's l2: 0.00893922\n[1000]\ttraining's l2: 0.0042411\tvalid_1's l2: 0.00885793\n[1100]\ttraining's l2: 0.00395709\tvalid_1's l2: 0.00878589\n[1200]\ttraining's l2: 0.00370299\tvalid_1's l2: 0.00873881\n[1300]\ttraining's l2: 0.0034797\tvalid_1's l2: 0.00869377\n[1400]\ttraining's l2: 0.00326394\tvalid_1's l2: 0.00863826\n[1500]\ttraining's l2: 0.00307006\tvalid_1's l2: 0.00860262\n[1600]\ttraining's l2: 0.00289125\tvalid_1's l2: 0.00856485\n[1700]\ttraining's l2: 0.00273266\tvalid_1's l2: 0.00852646\n[1800]\ttraining's l2: 0.0025787\tvalid_1's l2: 0.00850476\n[1900]\ttraining's l2: 0.00243727\tvalid_1's l2: 0.00846972\n[2000]\ttraining's l2: 0.00230767\tvalid_1's l2: 0.00843541\nDid not meet early stopping. Best iteration is:\n[1998]\ttraining's l2: 0.00231022\tvalid_1's l2: 0.0084346\nTraining fold 3\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051457 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44618\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1397\n[LightGBM] [Info] Start training from score 0.661377\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0145839\tvalid_1's l2: 0.0155294\n[200]\ttraining's l2: 0.0104704\tvalid_1's l2: 0.0121319\n[300]\ttraining's l2: 0.00837616\tvalid_1's l2: 0.0104822\n[400]\ttraining's l2: 0.00719927\tvalid_1's l2: 0.00959628\n[500]\ttraining's l2: 0.00642324\tvalid_1's l2: 0.00916721\n[600]\ttraining's l2: 0.00583225\tvalid_1's l2: 0.00886174\n[700]\ttraining's l2: 0.00538008\tvalid_1's l2: 0.00867373\n[800]\ttraining's l2: 0.00497998\tvalid_1's l2: 0.00854526\n[900]\ttraining's l2: 0.00464092\tvalid_1's l2: 0.00842526\n[1000]\ttraining's l2: 0.00432423\tvalid_1's l2: 0.00835717\n[1100]\ttraining's l2: 0.00404438\tvalid_1's l2: 0.00829929\n[1200]\ttraining's l2: 0.00379379\tvalid_1's l2: 0.0082573\n[1300]\ttraining's l2: 0.00357121\tvalid_1's l2: 0.00822454\n[1400]\ttraining's l2: 0.00334873\tvalid_1's l2: 0.00818687\n[1500]\ttraining's l2: 0.00314771\tvalid_1's l2: 0.00814944\n[1600]\ttraining's l2: 0.00296486\tvalid_1's l2: 0.00812548\n[1700]\ttraining's l2: 0.00279741\tvalid_1's l2: 0.00810655\n[1800]\ttraining's l2: 0.00264026\tvalid_1's l2: 0.00807389\n[1900]\ttraining's l2: 0.0024904\tvalid_1's l2: 0.00804355\n[2000]\ttraining's l2: 0.00235219\tvalid_1's l2: 0.0080191\nDid not meet early stopping. Best iteration is:\n[1997]\ttraining's l2: 0.00235588\tvalid_1's l2: 0.00801838\nTraining fold 4\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053354 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44548\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1387\n[LightGBM] [Info] Start training from score 0.661045\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0143739\tvalid_1's l2: 0.0162239\n[200]\ttraining's l2: 0.0103302\tvalid_1's l2: 0.012788\n[300]\ttraining's l2: 0.00828943\tvalid_1's l2: 0.0111209\n[400]\ttraining's l2: 0.0071048\tvalid_1's l2: 0.0102584\n[500]\ttraining's l2: 0.006297\tvalid_1's l2: 0.00977018\n[600]\ttraining's l2: 0.00572728\tvalid_1's l2: 0.00948429\n[700]\ttraining's l2: 0.00525874\tvalid_1's l2: 0.00929414\n[800]\ttraining's l2: 0.004889\tvalid_1's l2: 0.00917559\n[900]\ttraining's l2: 0.00456111\tvalid_1's l2: 0.00907042\n[1000]\ttraining's l2: 0.00426615\tvalid_1's l2: 0.00899396\n[1100]\ttraining's l2: 0.0039893\tvalid_1's l2: 0.00893235\n[1200]\ttraining's l2: 0.00374356\tvalid_1's l2: 0.00888957\n[1300]\ttraining's l2: 0.00350544\tvalid_1's l2: 0.00883647\n[1400]\ttraining's l2: 0.0032891\tvalid_1's l2: 0.00879154\n[1500]\ttraining's l2: 0.00309524\tvalid_1's l2: 0.00877089\n[1600]\ttraining's l2: 0.00291401\tvalid_1's l2: 0.00872612\n[1700]\ttraining's l2: 0.00273449\tvalid_1's l2: 0.00869629\n[1800]\ttraining's l2: 0.00258065\tvalid_1's l2: 0.00865995\n[1900]\ttraining's l2: 0.00244022\tvalid_1's l2: 0.00863602\n[2000]\ttraining's l2: 0.00231121\tvalid_1's l2: 0.00861979\nDid not meet early stopping. Best iteration is:\n[1982]\ttraining's l2: 0.00233373\tvalid_1's l2: 0.00861892\nTraining fold 5\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052765 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 44618\n[LightGBM] [Info] Number of data points in the train set: 6108, number of used features: 1401\n[LightGBM] [Info] Start training from score 0.661050\nTraining until validation scores don't improve for 100 rounds\n[100]\ttraining's l2: 0.0145294\tvalid_1's l2: 0.0154821\n[200]\ttraining's l2: 0.010418\tvalid_1's l2: 0.0120647\n[300]\ttraining's l2: 0.0083345\tvalid_1's l2: 0.0104277\n[400]\ttraining's l2: 0.00715067\tvalid_1's l2: 0.00962169\n[500]\ttraining's l2: 0.00638932\tvalid_1's l2: 0.00918922\n[600]\ttraining's l2: 0.00580772\tvalid_1's l2: 0.00897896\n[700]\ttraining's l2: 0.00533069\tvalid_1's l2: 0.00880533\n[800]\ttraining's l2: 0.00493253\tvalid_1's l2: 0.0086587\n[900]\ttraining's l2: 0.0045753\tvalid_1's l2: 0.00852767\n[1000]\ttraining's l2: 0.00426957\tvalid_1's l2: 0.00846811\n[1100]\ttraining's l2: 0.00398866\tvalid_1's l2: 0.00837303\n[1200]\ttraining's l2: 0.0037515\tvalid_1's l2: 0.00832611\n[1300]\ttraining's l2: 0.00351536\tvalid_1's l2: 0.00827642\n[1400]\ttraining's l2: 0.00330078\tvalid_1's l2: 0.00822452\n[1500]\ttraining's l2: 0.00311016\tvalid_1's l2: 0.00817908\n[1600]\ttraining's l2: 0.00293166\tvalid_1's l2: 0.00814829\n[1700]\ttraining's l2: 0.00276186\tvalid_1's l2: 0.00811351\n[1800]\ttraining's l2: 0.00261263\tvalid_1's l2: 0.00807876\n[1900]\ttraining's l2: 0.00246689\tvalid_1's l2: 0.00804884\n[2000]\ttraining's l2: 0.00233529\tvalid_1's l2: 0.00804739\nDid not meet early stopping. Best iteration is:\n[1971]\ttraining's l2: 0.00237023\tvalid_1's l2: 0.00804078\nCV MSE: 0.008420 ± 0.000365\nSubmission saved to submission.csv\n","output_type":"stream"}],"execution_count":30}]}